<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 PDP and Causal Interpretation | Limitations of Interpretable Machine Learning Methods</title>
  <meta name="description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 PDP and Causal Interpretation | Limitations of Interpretable Machine Learning Methods" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 PDP and Causal Interpretation | Limitations of Interpretable Machine Learning Methods" />
  
  <meta name="twitter:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  <meta name="twitter:image" content="images/cover.png" />



<meta name="date" content="2020-01-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pdp-correlated.html"/>
<link rel="next" href="ale.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Limitations of ML Interpretability</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statistical-modeling-the-two-approaches"><i class="fa fa-check"></i><b>1.1</b> Statistical Modeling: The Two Approaches</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#importance-of-interpretability"><i class="fa fa-check"></i><b>1.2</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretable-machine-learning"><i class="fa fa-check"></i><b>1.3</b> Interpretable Machine Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.4</b> Outline of the booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>2</b> Introduction to Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="2.1" data-path="pdp.html"><a href="pdp.html#partial-dependence-plots-pdp"><i class="fa fa-check"></i><b>2.1</b> Partial Dependence Plots (PDP)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="pdp.html"><a href="pdp.html#advantages-and-limitations-of-partial-dependence-plots"><i class="fa fa-check"></i><b>2.1.1</b> Advantages and Limitations of Partial Dependence Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pdp.html"><a href="pdp.html#individual-conditional-expectation-curves"><i class="fa fa-check"></i><b>2.2</b> Individual Conditional Expectation Curves</a><ul>
<li class="chapter" data-level="2.2.1" data-path="pdp.html"><a href="pdp.html#centered-ice-plot"><i class="fa fa-check"></i><b>2.2.1</b> Centered ICE Plot</a></li>
<li class="chapter" data-level="2.2.2" data-path="pdp.html"><a href="pdp.html#derivative-ice-plot"><i class="fa fa-check"></i><b>2.2.2</b> Derivative ICE Plot</a></li>
<li class="chapter" data-level="2.2.3" data-path="pdp.html"><a href="pdp.html#advantages-and-limitations-of-ice-plots"><i class="fa fa-check"></i><b>2.2.3</b> Advantages and Limitations of ICE Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="pdp-correlated.html"><a href="pdp-correlated.html"><i class="fa fa-check"></i><b>3</b> PDP and Correlated Features</a><ul>
<li class="chapter" data-level="3.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ProblemDescription"><i class="fa fa-check"></i><b>3.1</b> Problem Description</a><ul>
<li class="chapter" data-level="3.1.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#what-is-the-issue-with-dependent-features"><i class="fa fa-check"></i><b>3.1.1</b> What is the issue with dependent features?</a></li>
<li class="chapter" data-level="3.1.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#what-is-the-issue-with-extrapolation"><i class="fa fa-check"></i><b>3.1.2</b> What is the issue with extrapolation?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#RealData"><i class="fa fa-check"></i><b>3.2</b> Dependent Features: Bike Sharing Dataset</a><ul>
<li class="chapter" data-level="3.2.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#dependency-between-numerical-features"><i class="fa fa-check"></i><b>3.2.1</b> Dependency between Numerical Features</a></li>
<li class="chapter" data-level="3.2.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#dependency-between-categorical-features"><i class="fa fa-check"></i><b>3.2.2</b> Dependency between Categorical Features</a></li>
<li class="chapter" data-level="3.2.3" data-path="pdp-correlated.html"><a href="pdp-correlated.html#NumCat"><i class="fa fa-check"></i><b>3.2.3</b> Dependency between Numerical and Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="pdp-correlated.html"><a href="pdp-correlated.html#SimulatedData"><i class="fa fa-check"></i><b>3.3</b> Dependent Features: Simulated Data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-settings-numerical-features"><i class="fa fa-check"></i><b>3.3.1</b> Simulation Settings: Numerical Features</a></li>
<li class="chapter" data-level="3.3.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-of-setting-1-linear-dependence"><i class="fa fa-check"></i><b>3.3.2</b> Simulation of Setting 1: Linear Dependence</a></li>
<li class="chapter" data-level="3.3.3" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-of-setting-2-nonlinear-dependence"><i class="fa fa-check"></i><b>3.3.3</b> Simulation of Setting 2: Nonlinear Dependence</a></li>
<li class="chapter" data-level="3.3.4" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-of-setting-3-missing-informative-feature-x_3"><i class="fa fa-check"></i><b>3.3.4</b> Simulation of Setting 3: Missing informative feature <span class="math inline">\(x_3\)</span></a></li>
<li class="chapter" data-level="3.3.5" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-settings-categorical-features"><i class="fa fa-check"></i><b>3.3.5</b> Simulation Settings: Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ExtrapolationProblem"><i class="fa fa-check"></i><b>3.4</b> Extrapolation Problem: Simulation</a><ul>
<li class="chapter" data-level="3.4.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ExtrapolationProblemEstablished"><i class="fa fa-check"></i><b>3.4.1</b> Simulation based on established learners</a></li>
<li class="chapter" data-level="3.4.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ExtrapolationProblemPrediction"><i class="fa fa-check"></i><b>3.4.2</b> Simulation based on own prediction function</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="pdp-correlated.html"><a href="pdp-correlated.html#summary"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="pdp-causal.html"><a href="pdp-causal.html"><i class="fa fa-check"></i><b>4</b> PDP and Causal Interpretation</a><ul>
<li class="chapter" data-level="4.1" data-path="pdp-causal.html"><a href="pdp-causal.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="pdp-causal.html"><a href="pdp-causal.html#motivation"><i class="fa fa-check"></i><b>4.2</b> Motivation</a></li>
<li class="chapter" data-level="4.3" data-path="pdp-causal.html"><a href="pdp-causal.html#causal-interpretability-interventions-and-directed-acyclical-graphs"><i class="fa fa-check"></i><b>4.3</b> Causal Interpretability: Interventions and Directed Acyclical Graphs</a></li>
<li class="chapter" data-level="4.4" data-path="pdp-causal.html"><a href="pdp-causal.html#scenarios"><i class="fa fa-check"></i><b>4.4</b> Scenarios</a></li>
<li class="chapter" data-level="4.5" data-path="pdp-causal.html"><a href="pdp-causal.html#conclusion"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5</b> Introduction to Accumulated Local Effects (ALE)</a><ul>
<li class="chapter" data-level="5.1" data-path="ale.html"><a href="ale.html#motivation-1"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ale.html"><a href="ale.html#ale-intro-formula"><i class="fa fa-check"></i><b>5.2</b> The Theoretical Formula</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ale.html"><a href="ale.html#centering"><i class="fa fa-check"></i><b>5.2.1</b> Centering</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html#estimation-formula"><i class="fa fa-check"></i><b>5.3</b> Estimation Formula</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#implementation-formula"><i class="fa fa-check"></i><b>5.3.1</b> Implementation Formula</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ale.html"><a href="ale.html#ale-intro-interpret"><i class="fa fa-check"></i><b>5.4</b> Intuition and Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ale-pdp.html"><a href="ale-pdp.html"><i class="fa fa-check"></i><b>6</b> Comparison of ALE and PDP</a><ul>
<li class="chapter" data-level="6.1" data-path="ale-pdp.html"><a href="ale-pdp.html#comparison-one-feature"><i class="fa fa-check"></i><b>6.1</b> Comparison one feature</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ale-pdp.html"><a href="ale-pdp.html#example-1-multiplicative-prediction-function"><i class="fa fa-check"></i><b>6.1.1</b> Example 1: Multiplicative prediction function</a></li>
<li class="chapter" data-level="6.1.2" data-path="ale-pdp.html"><a href="ale-pdp.html#example-2-additive-prediction-function"><i class="fa fa-check"></i><b>6.1.2</b> Example 2: Additive prediction function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ale-pdp.html"><a href="ale-pdp.html#comparison-two-features"><i class="fa fa-check"></i><b>6.2</b> Comparison two features</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ale-pdp.html"><a href="ale-pdp.html#the-2d-ale"><i class="fa fa-check"></i><b>6.2.1</b> The 2D ALE</a></li>
<li class="chapter" data-level="6.2.2" data-path="ale-pdp.html"><a href="ale-pdp.html#d-ale-vs-2d-pdp"><i class="fa fa-check"></i><b>6.2.2</b> 2D ALE vs 2D PDP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ale-pdp.html"><a href="ale-pdp.html#runtime-comparison"><i class="fa fa-check"></i><b>6.3</b> Runtime comparison</a><ul>
<li class="chapter" data-level="6.3.1" data-path="ale-pdp.html"><a href="ale-pdp.html#one-numerical-feature-of-interest"><i class="fa fa-check"></i><b>6.3.1</b> One numerical feature of interest</a></li>
<li class="chapter" data-level="6.3.2" data-path="ale-pdp.html"><a href="ale-pdp.html#two-numerical-features-of-interest"><i class="fa fa-check"></i><b>6.3.2</b> Two numerical features of interest</a></li>
<li class="chapter" data-level="6.3.3" data-path="ale-pdp.html"><a href="ale-pdp.html#one-categorial-feature-of-interest"><i class="fa fa-check"></i><b>6.3.3</b> One categorial feature of interest</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ale-pdp.html"><a href="ale-pdp.html#comparison-for-unevenly-distributed-data---example-4-munich-rents"><i class="fa fa-check"></i><b>6.4</b> Comparison for unevenly distributed data - Example 4: Munich rents</a></li>
<li class="chapter" data-level="6.5" data-path="ale-pdp.html"><a href="ale-pdp.html#appendix"><i class="fa fa-check"></i><b>6.5</b> Appendix</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ale-pdp.html"><a href="ale-pdp.html#ale-2d-example-calculation"><i class="fa fa-check"></i><b>6.5.1</b> Calculation of theoretical 2D ALE example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ale-misc.html"><a href="ale-misc.html"><i class="fa fa-check"></i><b>7</b> ALE Intervals, Piece-Wise Constant Models and Categorical Features</a><ul>
<li class="chapter" data-level="7.1" data-path="ale-misc.html"><a href="ale-misc.html#how-to-choose-the-number-andor-length-of-the-intervals"><i class="fa fa-check"></i><b>7.1</b> How to choose the number and/or length of the intervals</a><ul>
<li class="chapter" data-level="7.1.1" data-path="ale-misc.html"><a href="ale-misc.html#state-of-the-art"><i class="fa fa-check"></i><b>7.1.1</b> State of the art</a></li>
<li class="chapter" data-level="7.1.2" data-path="ale-misc.html"><a href="ale-misc.html#ale-approximations"><i class="fa fa-check"></i><b>7.1.2</b> ALE Approximations</a></li>
<li class="chapter" data-level="7.1.3" data-path="ale-misc.html"><a href="ale-misc.html#example-1-additive-feature-effects"><i class="fa fa-check"></i><b>7.1.3</b> Example 1: additive feature effects</a></li>
<li class="chapter" data-level="7.1.4" data-path="ale-misc.html"><a href="ale-misc.html#example-2-multiplicative-feature-effects"><i class="fa fa-check"></i><b>7.1.4</b> Example 2: multiplicative feature effects</a></li>
<li class="chapter" data-level="7.1.5" data-path="ale-misc.html"><a href="ale-misc.html#example-3-unbalanced-datasets-and-shaky-prediction-functions"><i class="fa fa-check"></i><b>7.1.5</b> Example 3: Unbalanced datasets and shaky prediction functions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ale-misc.html"><a href="ale-misc.html#problems-with-piece-wise-constant-models"><i class="fa fa-check"></i><b>7.2</b> Problems with piece-wise constant models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ale-misc.html"><a href="ale-misc.html#example-4-simple-step-function"><i class="fa fa-check"></i><b>7.2.1</b> Example 4: Simple step function</a></li>
<li class="chapter" data-level="7.2.2" data-path="ale-misc.html"><a href="ale-misc.html#example-5-two-dimensional-step-functions-and-unluckily-distributed-data"><i class="fa fa-check"></i><b>7.2.2</b> Example 5: Two-dimensional step functions and unluckily distributed data</a></li>
<li class="chapter" data-level="7.2.3" data-path="ale-misc.html"><a href="ale-misc.html#outlook"><i class="fa fa-check"></i><b>7.2.3</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ale-misc.html"><a href="ale-misc.html#categorical-features"><i class="fa fa-check"></i><b>7.3</b> Categorical Features</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ale-misc.html"><a href="ale-misc.html#ordering-the-features"><i class="fa fa-check"></i><b>7.3.1</b> Ordering the features</a></li>
<li class="chapter" data-level="7.3.2" data-path="ale-misc.html"><a href="ale-misc.html#estimation-of-the-ale"><i class="fa fa-check"></i><b>7.3.2</b> Estimation of the ALE</a></li>
<li class="chapter" data-level="7.3.3" data-path="ale-misc.html"><a href="ale-misc.html#example-of-ale-with-categorical-feature"><i class="fa fa-check"></i><b>7.3.3</b> Example of ALE with categorical feature</a></li>
<li class="chapter" data-level="7.3.4" data-path="ale-misc.html"><a href="ale-misc.html#interpretation"><i class="fa fa-check"></i><b>7.3.4</b> Interpretation</a></li>
<li class="chapter" data-level="7.3.5" data-path="ale-misc.html"><a href="ale-misc.html#changes-of-the-ale-due-to-different-orders"><i class="fa fa-check"></i><b>7.3.5</b> Changes of the ALE due to different orders</a></li>
<li class="chapter" data-level="7.3.6" data-path="ale-misc.html"><a href="ale-misc.html#conclusion-1"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pfi.html"><a href="pfi.html"><i class="fa fa-check"></i><b>8</b> Introduction to Feature Importance</a><ul>
<li class="chapter" data-level="8.1" data-path="pfi.html"><a href="pfi.html#permutation-feature-importance-pfi"><i class="fa fa-check"></i><b>8.1</b> Permutation Feature Importance (PFI)</a></li>
<li class="chapter" data-level="8.2" data-path="pfi.html"><a href="pfi.html#leave-one-covariate-out-loco"><i class="fa fa-check"></i><b>8.2</b> Leave-One-Covariate-Out (LOCO)</a></li>
<li class="chapter" data-level="8.3" data-path="pfi.html"><a href="pfi.html#interpretability-of-feature-importance-and-its-limitations"><i class="fa fa-check"></i><b>8.3</b> Interpretability of Feature Importance and its Limitations</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pfi-correlated.html"><a href="pfi-correlated.html"><i class="fa fa-check"></i><b>9</b> PFI, LOCO and Correlated Features</a><ul>
<li class="chapter" data-level="9.1" data-path="pfi-correlated.html"><a href="pfi-correlated.html#effect-on-feature-importance-by-adding-correlated-features"><i class="fa fa-check"></i><b>9.1</b> Effect on Feature Importance by Adding Correlated Features</a><ul>
<li class="chapter" data-level="9.1.1" data-path="pfi-correlated.html"><a href="pfi-correlated.html#simulation"><i class="fa fa-check"></i><b>9.1.1</b> Simulation</a></li>
<li class="chapter" data-level="9.1.2" data-path="pfi-correlated.html"><a href="pfi-correlated.html#real-data"><i class="fa fa-check"></i><b>9.1.2</b> Real Data</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="pfi-correlated.html"><a href="pfi-correlated.html#alternative-measures-dealing-with-correlated-features"><i class="fa fa-check"></i><b>9.2</b> Alternative Measures Dealing with Correlated Features</a></li>
<li class="chapter" data-level="9.3" data-path="pfi-correlated.html"><a href="pfi-correlated.html#summary-1"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
<li class="chapter" data-level="9.4" data-path="pfi-correlated.html"><a href="pfi-correlated.html#note-to-the-reader"><i class="fa fa-check"></i><b>9.4</b> Note to the reader</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pfi-partial.html"><a href="pfi-partial.html"><i class="fa fa-check"></i><b>10</b> Partial and Individual Permutation Feature Importance</a><ul>
<li class="chapter" data-level="10.1" data-path="pfi-partial.html"><a href="pfi-partial.html#ch2"><i class="fa fa-check"></i><b>10.1</b> Preliminaries on Partial and Individual Conditional Importance</a></li>
<li class="chapter" data-level="10.2" data-path="pfi-partial.html"><a href="pfi-partial.html#ch3"><i class="fa fa-check"></i><b>10.2</b> Simulations: A cookbook for using with PI and ICI</a><ul>
<li class="chapter" data-level="10.2.1" data-path="pfi-partial.html"><a href="pfi-partial.html#ch31"><i class="fa fa-check"></i><b>10.2.1</b> Detect Interactions</a></li>
<li class="chapter" data-level="10.2.2" data-path="pfi-partial.html"><a href="pfi-partial.html#ch32"><i class="fa fa-check"></i><b>10.2.2</b> Explain Interactions</a></li>
<li class="chapter" data-level="10.2.3" data-path="pfi-partial.html"><a href="pfi-partial.html#ch323"><i class="fa fa-check"></i><b>10.2.3</b> Stress Methods in a Non-Linear Relationship Setting</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="pfi-partial.html"><a href="pfi-partial.html#ch4"><i class="fa fa-check"></i><b>10.3</b> Real Data Application: Boston Housing</a></li>
<li class="chapter" data-level="10.4" data-path="pfi-partial.html"><a href="pfi-partial.html#ch5"><i class="fa fa-check"></i><b>10.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pfi-data.html"><a href="pfi-data.html"><i class="fa fa-check"></i><b>11</b> PFI: Training vs. Test Data</a><ul>
<li class="chapter" data-level="11.1" data-path="pfi-data.html"><a href="pfi-data.html#introduction-to-test-vs.training-data"><i class="fa fa-check"></i><b>11.1</b> Introduction to Test vs. Training Data</a></li>
<li class="chapter" data-level="11.2" data-path="pfi-data.html"><a href="pfi-data.html#theoretical-discussion-for-test-and-training-data"><i class="fa fa-check"></i><b>11.2</b> Theoretical Discussion for Test and Training Data</a></li>
<li class="chapter" data-level="11.3" data-path="pfi-data.html"><a href="pfi-data.html#reaction-to-model-behavior"><i class="fa fa-check"></i><b>11.3</b> Reaction to model behavior</a><ul>
<li class="chapter" data-level="11.3.1" data-path="pfi-data.html"><a href="pfi-data.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>11.3.1</b> Gradient Boosting Machines</a></li>
<li class="chapter" data-level="11.3.2" data-path="pfi-data.html"><a href="pfi-data.html#data-sets-used-for-calculations"><i class="fa fa-check"></i><b>11.3.2</b> Data sets used for calculations</a></li>
<li class="chapter" data-level="11.3.3" data-path="pfi-data.html"><a href="pfi-data.html#results"><i class="fa fa-check"></i><b>11.3.3</b> Results</a></li>
<li class="chapter" data-level="11.3.4" data-path="pfi-data.html"><a href="pfi-data.html#interpretation-of-the-results"><i class="fa fa-check"></i><b>11.3.4</b> Interpretation of the results</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="pfi-data.html"><a href="pfi-data.html#summary-2"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>12</b> Introduction to Local Interpretable Model-Agnostic Explanations (LIME)</a><ul>
<li class="chapter" data-level="12.1" data-path="lime.html"><a href="lime.html#local-surrogate-models-and-lime"><i class="fa fa-check"></i><b>12.1</b> Local Surrogate Models and LIME</a></li>
<li class="chapter" data-level="12.2" data-path="lime.html"><a href="lime.html#how-lime-works-in-detail"><i class="fa fa-check"></i><b>12.2</b> How LIME works in detail</a><ul>
<li class="chapter" data-level="12.2.1" data-path="lime.html"><a href="lime.html#neighbourhood"><i class="fa fa-check"></i><b>12.2.1</b> Neighbourhood</a></li>
<li class="chapter" data-level="12.2.2" data-path="lime.html"><a href="lime.html#what-makes-a-good-explainer"><i class="fa fa-check"></i><b>12.2.2</b> What makes a good explainer?</a></li>
<li class="chapter" data-level="12.2.3" data-path="lime.html"><a href="lime.html#sampling-and-perturbation"><i class="fa fa-check"></i><b>12.2.3</b> Sampling and perturbation</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="lime.html"><a href="lime.html#example"><i class="fa fa-check"></i><b>12.3</b> Example</a></li>
<li class="chapter" data-level="12.4" data-path="lime.html"><a href="lime.html#outlook-1"><i class="fa fa-check"></i><b>12.4</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lime-neighbor.html"><a href="lime-neighbor.html"><i class="fa fa-check"></i><b>13</b> LIME and Neighborhood</a><ul>
<li class="chapter" data-level="13.1" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id2"><i class="fa fa-check"></i><b>13.1</b> The Neighborhood in LIME in more detail</a></li>
<li class="chapter" data-level="13.2" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id3"><i class="fa fa-check"></i><b>13.2</b> The problem in a one-dimensional setting</a></li>
<li class="chapter" data-level="13.3" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id4"><i class="fa fa-check"></i><b>13.3</b> The problem in more complex settings</a><ul>
<li class="chapter" data-level="13.3.1" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id41"><i class="fa fa-check"></i><b>13.3.1</b> Simulated data</a></li>
<li class="chapter" data-level="13.3.2" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id42"><i class="fa fa-check"></i><b>13.3.2</b> Real data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id5"><i class="fa fa-check"></i><b>13.4</b> Discussion and outlook</a></li>
<li class="chapter" data-level="13.5" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id6"><i class="fa fa-check"></i><b>13.5</b> Note to the reader</a><ul>
<li class="chapter" data-level="13.5.1" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id61"><i class="fa fa-check"></i><b>13.5.1</b> Packages used</a></li>
<li class="chapter" data-level="13.5.2" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id62"><i class="fa fa-check"></i><b>13.5.2</b> How we used the lime R package and why</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lime-sample.html"><a href="lime-sample.html"><i class="fa fa-check"></i><b>14</b> LIME and Sampling</a><ul>
<li class="chapter" data-level="14.1" data-path="lime-sample.html"><a href="lime-sample.html#understanding-sampling-in-lime"><i class="fa fa-check"></i><b>14.1</b> Understanding sampling in LIME</a><ul>
<li class="chapter" data-level="14.1.1" data-path="lime-sample.html"><a href="lime-sample.html#formula"><i class="fa fa-check"></i><b>14.1.1</b> Formula</a></li>
<li class="chapter" data-level="14.1.2" data-path="lime-sample.html"><a href="lime-sample.html#sampling-strategies"><i class="fa fa-check"></i><b>14.1.2</b> Sampling strategies</a></li>
<li class="chapter" data-level="14.1.3" data-path="lime-sample.html"><a href="lime-sample.html#visualization-of-a-basic-example"><i class="fa fa-check"></i><b>14.1.3</b> Visualization of a basic example</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="lime-sample.html"><a href="lime-sample.html#sketching-problems-of-sampling"><i class="fa fa-check"></i><b>14.2</b> Sketching Problems of Sampling</a></li>
<li class="chapter" data-level="14.3" data-path="lime-sample.html"><a href="lime-sample.html#real-world-problems-with-lime"><i class="fa fa-check"></i><b>14.3</b> Real World Problems with LIME</a><ul>
<li class="chapter" data-level="14.3.1" data-path="lime-sample.html"><a href="lime-sample.html#boston-housing-data"><i class="fa fa-check"></i><b>14.3.1</b> Boston Housing Data</a></li>
<li class="chapter" data-level="14.3.2" data-path="lime-sample.html"><a href="lime-sample.html#rental-bikes-data"><i class="fa fa-check"></i><b>14.3.2</b> Rental Bikes Data</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lime-sample.html"><a href="lime-sample.html#experiments-regarding-sampling-stability"><i class="fa fa-check"></i><b>14.4</b> Experiments regarding Sampling stability</a><ul>
<li class="chapter" data-level="14.4.1" data-path="lime-sample.html"><a href="lime-sample.html#influence-of-feature-dimension"><i class="fa fa-check"></i><b>14.4.1</b> Influence of feature dimension</a></li>
<li class="chapter" data-level="14.4.2" data-path="lime-sample.html"><a href="lime-sample.html#influence-of-sample-size"><i class="fa fa-check"></i><b>14.4.2</b> Influence of sample size</a></li>
<li class="chapter" data-level="14.4.3" data-path="lime-sample.html"><a href="lime-sample.html#influence-of-black-box"><i class="fa fa-check"></i><b>14.4.3</b> Influence of black-box</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="lime-sample.html"><a href="lime-sample.html#outlook-2"><i class="fa fa-check"></i><b>14.5</b> Outlook</a></li>
<li class="chapter" data-level="14.6" data-path="lime-sample.html"><a href="lime-sample.html#conclusion-2"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Limitations of Interpretable Machine Learning Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pdp-causal" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> PDP and Causal Interpretation</h1>
<p><em>Author: Thommy Dassen</em></p>
<p><em>Supervisor: Gunnar König</em></p>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>Machine learning methods excel at learning associations between features and the target. The supervised machine learning model estimates <span class="math inline">\(Y\)</span> using the information provided in the feature set <span class="math inline">\(X\)</span>. Partial dependence plots (PDPs) allow us to inspect the learned model. We can analyze how the model prediction changes given changes in the features.</p>
<p>The unexperienced user may be tempted to transfer this insight about the model into an insight in the real world. If our model’s prediction changes given a change on some feature <span class="math inline">\(X_j\)</span>, can we change the target variable <span class="math inline">\(Y\)</span> in the real world by performing an intervention on <span class="math inline">\(X_j\)</span>?</p>
<p>Of course, this is not generally true. Without further assumptions we cannot interpret PDPs causally (like we cannot interpret coefficients of linear models causally). Curiously, certain assumption sets still allow for a causal interpretation.</p>
<p>In this Chapter we will elaborate on specific assumptions sets that allow causal interpretation and will empirically evaluate the resulting plots. We therefore assume certain data generating mechanisms, also referred to as structural equation models (SEMs), and use Judea Pearl’s do-calculas framework to compute the true distributions under an intervention, allowing to e.g. compute the average causal effect. Graphs are used to visualize the underlying dependence structure. We will see scenarios in which the PDP gives the same result as an intervention and scenarios in which the limitations of the PDP as a tool for causal interpretation become clear.</p>
<p>To interpret causally means to interpret one state (the effect) to be the result of another (the cause), with the cause being (partly) responsible for the effect, and the effect being partially dependent on the cause. <span class="citation">(Zhao and Hastie <a href="#ref-zhaohastie">2018</a>)</span> formulated three elements that are needed to ensure that the PDP coincides with the intervention effect:<br />
1. A good predictive model which closely approximates the real relationship.<br />
2. Domain knowledge to ensure the causal structure makes sense and the backdoor criterion, explained below, is met.<br />
3. A visualization tool like a PDP (or an Individual Conditional Expectation plot)</p>
<p>The first condition is an important one, because there is a big difference between being able to causally interpret an effect for the <em>model</em> and using it as a causal interpretation for the real world. The second condition will make clear when PDPs are the same, and when they are different from interventions on the data. In this chapter we will systematically analyze a number of scenarios in order to see under which conditions PDPs can be causally interpreted or not.</p>
</div>
<div id="motivation" class="section level2">
<h2><span class="header-section-number">4.2</span> Motivation</h2>
<p>Before we have a look at various scenarios and settings involving interventions, let’s look at an exemplary problem. Let’s say we have a dataset containing data on the amount of ice-cream consumption per capita and the temperature outside. The temperature is causal for the ice-cream consupmtion: People eat more ice-cream when temperatures are high than when temperatures are low. Therefore, both quantities are dependent as well. A statistical model learns to use the information in one of the variables to predict the other, as is evident in the plots below.</p>
<div class="figure" style="text-align: left"><span id="fig:Figure1"></span>
<img src="images/ice_cream.jpeg" alt="The PDP on the left shows temperature causing ice-cream consumption. The PDP on the right shows ice-cream consumption causing temperatures. Which one is correct?" width="100%" />
<p class="caption">
FIGURE 4.1: The PDP on the left shows temperature causing ice-cream consumption. The PDP on the right shows ice-cream consumption causing temperatures. Which one is correct?
</p>
</div>
<p>While the dependence is learned correctly, it would be wrong to interpret the second plot causally. When more ice cream is consumed, it is likely that the temperature is higher. However, consuming more ice cream will not change the amount of ice cream that is being eaten.</p>
<p>This example illustrates the gap between predictive models and causal models. Without further assumptions we cannot interpret effects of changes in features on the model prediction as effects that would be present in the real world. In this simple scenario it is quite clear in which cases a causal interpretation is clear. However, in more complicated scenarios a more elaborate formalization of assumptions under which a causal interpretation is possible are helpful. We will analyze this theoretically in the next section and will empirically evaluate the theoretical findings.</p>
<p>It may be noted that other problems can occur when using PDPs for model interpretation. E.g. as shown by <span class="citation">(Scholbeck <a href="#ref-scholbeck">2018</a>)</span>, assume we have data that is distributed as follows:</p>
<p><span class="math display">\[ Y \leftarrow  X_{1}^2 - 15X_{1}X_2 + \epsilon \]</span> <span class="math display">\[ X_1 \sim \mathcal{U}(-1, 1), \ \ \ \ \ X_2 \sim \mathcal{U}(-1, 1), \ \ \ \ \ \epsilon \sim \mathcal{N}(0,0.1), \ \ \ \ \ N \leftarrow 1000    \]</span> Training a Random Fordest on this data leads to Figure <a href="pdp-causal.html#fig:Figure2">4.2</a> below. Looking at the PDP, one would assume that <span class="math inline">\(X_1\)</span> has virtually no impact on <span class="math inline">\(Y\)</span>. The ICE curves, however, show that the averaging effect of the PDP completely obfuscates the true effect, which is highly positive for some observations while being highly negative for others. In this example too it would be misguided to simply interpret the PDP causally and state that <span class="math inline">\(X_1\)</span> does not have any impact on <span class="math inline">\(Y\)</span> whatsoever. This may capture the average effect correctly, but evidently not true on the individual level.</p>
<div class="figure" style="text-align: left"><span id="fig:Figure2"></span>
<img src="images/avg_vs_individual.jpeg" alt="The average effect of the PDP (yellow line) hides the heterogeneity of the individual effects " width="100%" />
<p class="caption">
FIGURE 4.2: The average effect of the PDP (yellow line) hides the heterogeneity of the individual effects
</p>
</div>
</div>
<div id="causal-interpretability-interventions-and-directed-acyclical-graphs" class="section level2">
<h2><span class="header-section-number">4.3</span> Causal Interpretability: Interventions and Directed Acyclical Graphs</h2>
<p>For the rest of the Chapter we will assume we know the structure of the mechanisms are underlying our dataset to derive assumptions under which a causal interpretation is possible. With the help of Pearl’s do-calculus <span class="citation">(Pearl <a href="#ref-pearl1993">1993</a>)</span> we can compute the true causal effect of interventions. Pearls do-calculus relies on an underlying Structural Equation Model, the structure of which can be visualized with causal graphs. In the structural equation model this is equivalent to removing all incoming dependencies for the intervened variable and fixing a specified value. As an effect, the distributions of variables that are caused by the intervened variable may change as well. For more details refer to <span class="citation">(Pearl <a href="#ref-pearl1993">1993</a>)</span>. It is important to recognize that an intervention is different from conditioning on a variable. Therefore the difference between a variable <span class="math inline">\(X\)</span> taking a value <span class="math inline">\(x\)</span> naturally and having a fixed value <span class="math inline">\(X=x\)</span> is reflected in the notation. The latter is denoted by <span class="math inline">\(do(X=x)\)</span>. As such, <span class="math inline">\(P(Y=y|X=x)\)</span> is the probability that <span class="math inline">\(Y=y\)</span> conditional on <span class="math inline">\(X=x\)</span>. <span class="math inline">\(P(Y=y|do(X=x))\)</span> is then the population distribution of <span class="math inline">\(Y\)</span> if the value of <span class="math inline">\(X\)</span> was fixed at <span class="math inline">\(x\)</span> for the entire population.</p>
<p>In order to avoid complex scenarios (dynamical models, equilibrium computation) we restrict ourselves to causal structures that can be visualized with Direct Acyclical Graphs. A DAG is a representation of relationships between variables in graphical form. Each variable is represented as a <em>node</em> and the lines between these nodes, or <em>edges</em>, show the direction of the causal relationship through arrowheads. In addition to being directed, these graphs are per definition acyclical. This means that a relationship <span class="math inline">\(X \rightarrow Y \rightarrow Z \rightarrow X\)</span> can not be represented as a DAG. Several examples of DAGs follow in the rest of the chapter, as each scenario starts with one. With regards to interventions, in a graphical sense this simply means removing edges from direct parents to the variable.</p>
<p>In order to know when a causal interpretation makes sense, more is needed than only a representation of a DAG and knowledge of how to do an intervention. An important formula introduced by <span class="citation">(Pearl <a href="#ref-pearl1993">1993</a>)</span> adresses exactly this problem: The back-door adjustment formula. This formula stipulates that the causal effect of <span class="math inline">\(X_S\)</span> on <span class="math inline">\(Y\)</span> can be identified if the causal relationship between the variables can be visualized in a graph and <span class="math inline">\(X_C\)</span>, the complementary set to <span class="math inline">\(X_S\)</span>, adheres to what he called the back-door criterion. The back-door adjustment formula is:</p>
<p><span class="math display">\[P(Y|do(X_S = x_S)) = \int P(Y |X_S = x_S, X_C = x_C) dP(x_C)\]</span> As <span class="citation">(Zhao and Hastie <a href="#ref-zhaohastie">2018</a>)</span> pointed out, this formula is basically the same as the formula for the partial dependence of <span class="math inline">\(g\)</span> on a subset of variables <span class="math inline">\(X_S\)</span> given output <span class="math inline">\(g(x)\)</span>:</p>
<p><span class="math display">\[ g_S(x_S) = \mathbb E_{x_C}[g(x_S, X_C)] = \int g(x_S, x_C)dP(x_C)  \]</span> If we take the expectation of Pearl’s adjustment formula we get: <span class="math display">\[ E[Y |do(X_S = x_S)] = \int E[Y |X_S = x_S, X_C = x_C] dP(x_C) \]</span> These last two formulas are the same, if <span class="math inline">\(C\)</span> is the complement of <span class="math inline">\(S\)</span>.</p>
<p><span class="citation">(Pearl <a href="#ref-pearl1993">1993</a>)</span> defined a back-door criterion that needs to be fulfilled in order for the adjustment formula to be valid. It states that:</p>
<ol style="list-style-type: decimal">
<li><p>No node in <span class="math inline">\(X_C\)</span> can be a descendant of <span class="math inline">\(X_S\)</span> in the DAG <span class="math inline">\(G\)</span>.</p></li>
<li><p>Every “back-door” path between <span class="math inline">\(X_S\)</span> and <span class="math inline">\(Y\)</span> has to be blocked by <span class="math inline">\(X_C\)</span>.</p></li>
</ol>
</div>
<div id="scenarios" class="section level2">
<h2><span class="header-section-number">4.4</span> Scenarios</h2>
<p>After these two introductory problems of PDPs, the rest of this chapter will look at PDPs through the causal framework of <span class="citation">(Pearl <a href="#ref-pearl1993">1993</a>)</span>. This means we will look at various causal scenarios visualized through DAGs and compare the PDPs created under this structure with the actual effect of interventions.</p>
<p>In each scenario nine settings will be simulated for PDP creation, consisting of three standard deviations for the error term (0.1, 0.3 and 0.5) and three magnitudes of observations (100, 1000, 10000). Furthermore, each setting for the PDP was simulated across twenty runs. Each of the nine plots will therefore show twenty PDPs in order to give a solid view of the relationship the PDPs capture for each setting. In addition to the plots of the PDPs, which will be the first three columns in each figure, the actual effect under intervention will be shown in a fourth column as a single yellow line. The true intervention effects (column 4) were all simulated with a thousand observations. Initial tests resulted in a large increase in computation time with a higher number of observations, but with results that hardly differed from those obtained with one thousand observations. For reasons of computational efficiency, we only use out of the box random forest models. In future work, other model classes and hyperparameter settings should be considered, e.g. by using approaches from automatic machine learning.</p>
<p>The process for obtaining the intervention curve was as follows: Let <span class="math inline">\(X\)</span> be the predictor variable of interest with possible values <span class="math inline">\(x_{1}, x_{2}, x_{n}\)</span> and <span class="math inline">\(Y\)</span> the response variable of interest. for each unique <span class="math inline">\(i \in \{1,2,\dots,n\}\)</span> do<br />
(1) make a copy of the data set<br />
(2) replace the original values of <span class="math inline">\(X\)</span> with the value <span class="math inline">\(X_{(i)}\)</span> of <span class="math inline">\(X\)</span> under intervention<br />
(3) recompute all variables dependent on <span class="math inline">\(X\)</span> using the replacement values as input. This includes <span class="math inline">\(Y\)</span>, but potentially also other features that rely on <span class="math inline">\(X\)</span> for their value. Note that only <span class="math inline">\(X\)</span> is replaced with <span class="math inline">\(X_i\)</span> in the existing equations. Both the equations and error terms remain the same as before.<br />
(4) Compute the average <span class="math inline">\(Y_i\)</span> in dataset <span class="math inline">\(i\)</span> given <span class="math inline">\(X_i\)</span>.<br />
(5) (<span class="math inline">\(X_i\)</span>, <span class="math inline">\(Y_i\)</span>) are a single point on the intervention curve.</p>
<p><strong>Scenario 1: </strong></p>
<div class="figure"><span id="fig:Figure3"></span>
<img src="book_files/figure-html/Figure3-1.svg" alt="Chain DAG where X has a direct impact on Y, but is dependent on Z" width="336" />
<p class="caption">
FIGURE 4.3: Chain DAG where X has a direct impact on Y, but is dependent on Z
</p>
</div>
<p>In the first scenario, we have a chain DAG, seen in Figure <a href="pdp-causal.html#fig:Figure3">4.3</a>. Our variable <span class="math inline">\(X\)</span> is impacted by <span class="math inline">\(Z\)</span> and has a direct effect on <span class="math inline">\(Y\)</span>. <span class="math inline">\(Z\)</span>, however, does not. <span class="math inline">\(X_C\)</span> consists of <span class="math inline">\(Z\)</span>, which is not a descendant of <span class="math inline">\(X\)</span>. There is also no backdoor path between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The backdoor criterion is met. In this scenario the expectation is thus that the PDPs should be overall equal to the true intervention. The initial simulation settings for this scenario are as follows:</p>
<p><span class="math display">\[ Y \leftarrow X + \epsilon_Y  \]</span> <span class="math display">\[ \epsilon_X,\epsilon_Y ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_Z \sim \mathcal{U}(-1,1),\ \ \ \ \ Z \leftarrow \epsilon_Z, \ \ \ \ \ X \leftarrow Z + \epsilon_X, \ \ \ \ \ N \leftarrow 100 \]</span></p>
<p>As will be done in all scenarios, both standard deviaton for <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(N\)</span> were varied across 3 levels leading to 9 settings.</p>
<div class="figure" style="text-align: left"><span id="fig:Figure4"></span>
<img src="images/scenario1_all.jpg" alt="Comparison for scenario 1 of PDPs under various settings with the (yellow) intervention curve on the right" width="100%" />
<p class="caption">
FIGURE 4.4: Comparison for scenario 1 of PDPs under various settings with the (yellow) intervention curve on the right
</p>
</div>
<p>Overall the PDPs match the intervention curves fairly well. Outside of the extreme regions of <span class="math inline">\(X\)</span>, where some curvature is present, the linear quality of the intervention curve is evident in the PDPs. Furthermore, the scale of <span class="math inline">\(\hat{Y}\)</span> is comparable to the scale of <span class="math inline">\(Y_{intervention}\)</span> in most settings.</p>
<p><strong>Scenario 2: Chain DAG</strong></p>
<div class="figure"><span id="fig:Figure5"></span>
<img src="book_files/figure-html/Figure5-1.svg" alt="Chain DAG where X has no a direct impact on Y, but only indirectly through Z" width="336" />
<p class="caption">
FIGURE 4.5: Chain DAG where X has no a direct impact on Y, but only indirectly through Z
</p>
</div>
<p>In this scenario the DAG again looks like a chain. <span class="math inline">\(X\)</span> has an effect on <span class="math inline">\(Y\)</span> through <span class="math inline">\(Z\)</span>, but no direct relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> exists. Note that since <span class="math inline">\(Z\)</span> is a descendant of <span class="math inline">\(X\)</span>, the PDP and intervention curve should not coincide. The initial simulation settings for this scenario are as follows:</p>
<p><span class="math display">\[ Y \leftarrow Z + \epsilon_Y  \]</span> <span class="math display">\[ \epsilon_Y,\epsilon_Z ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_X \sim \mathcal{U}(-1,1), \ \ \ \ \ X \leftarrow \epsilon_X, \ \ \ \ \ Z \leftarrow X + \epsilon_Z, \ \ \ \ \ N \leftarrow 100 \]</span></p>
<div class="figure" style="text-align: left"><span id="fig:Figure6"></span>
<img src="images/scenario2_all.jpg" alt="Comparison for scenario 2 of PDPs under various settings with the (yellow) intervention curve on the right" width="100%" />
<p class="caption">
FIGURE 4.6: Comparison for scenario 2 of PDPs under various settings with the (yellow) intervention curve on the right
</p>
</div>
<p>As can be seen from Figure <a href="pdp-causal.html#fig:Figure6">4.6</a>, the PDP plots do not match the intervention plots well in several cases. The effect strength is a lot smaller and in fact, four out of nine settings show a negative slope for the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in comparison to the overall positive slope for the true intervention. The first row performs best, as expected due to the relatively small error that is used. Interesting is also that in the second and third row, where the error has been increased, the PDP slope goes from positive to negative between the first and second column. PDP accuracy thus suffers in situations where the observation count is low. In theory, we would expect the slope to be zero. It remains to be investigated why we still see a trend, and why this trend is negative in some cases.</p>
<p><strong>Scenario 3</strong></p>
<p>After two chains, we now have a scenario where <span class="math inline">\(X\)</span> has an influence on both <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> directly, as well as <span class="math inline">\(Z\)</span> having an impact on <span class="math inline">\(Y\)</span>, as seen in Figure <a href="pdp-causal.html#fig:Figure7">4.7</a>. X confounds <span class="math inline">\(Z \rightarrow Y\)</span> here. An example of a confounding variable in real life might be for instance the relationship between the <em>level of physical activity</em> and <em>weight gain</em>, which is confounded by <em>age</em>. <em>Age</em> affects both <em>weight gain</em> and the <em>level of physical activity</em> (on average), making it similar to the X in our scenario here. This scenario is similar to the previous one with only an edge between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> having been added. As can be seen from the DAG in <a href="pdp-causal.html#fig:Figure7">4.7</a>, <span class="math inline">\(Z\)</span> is still a descendant of <span class="math inline">\(X\)</span>. As the backdoor criterion is not met, we cannot expect a causal interpretation to be valid.</p>
<div class="figure"><span id="fig:Figure7"></span>
<img src="book_files/figure-html/Figure7-1.svg" alt="X is a confounding variable impacting both Z and Y" width="336" />
<p class="caption">
FIGURE 4.7: X is a confounding variable impacting both Z and Y
</p>
</div>
<p>The similarity to the previous scenario can also be noted in the simulation settings, where the only difference is the addition of <span class="math inline">\(X\)</span> to the equation for <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[ Y \leftarrow Z + X + \epsilon_Y  \]</span> <span class="math display">\[ \epsilon_Y,\epsilon_Z ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_X \sim \mathcal{U}(-1,1), \ \ \ \ \ X \leftarrow \epsilon_X, \ \ \ \ \ Z \leftarrow X + \epsilon_Z, \ \ \ \ \ N \leftarrow 100 \]</span></p>
<div class="figure" style="text-align: left"><span id="fig:Figure8"></span>
<img src="images/scenario3_all.jpg" alt="Comparison for scenario 3 of PDPs under various settings with the (yellow) intervention curve on the right" width="100%" />
<p class="caption">
FIGURE 4.8: Comparison for scenario 3 of PDPs under various settings with the (yellow) intervention curve on the right
</p>
</div>
<p>The expectation was that the PDP would not show the same results as the true intervention. On first glance in Figure <a href="pdp-causal.html#fig:Figure8">4.8</a>, the PDPs do not seem to be as inaccurate as they were in scenario 2. An overall upward trend seen in the true intervention on the right is also captured by the PDPs in all settings. However, big differences do exist. First of all, the scale of <span class="math inline">\(\hat{Y}\)</span> is off in every setting. This issue gets worse both when the standard deviation of the error increases and when the number of observations is increased. The worst setting is the bottom right, where <span class="math inline">\(N = 10.000\)</span> and the standard deviation of the error is 0.5. The range of <span class="math inline">\(\hat{Y}\)</span> is very small compared to the true intervention next to it and the slope is not steep enough. In fact, the correct slope can only be seen in a very few points: In the top row plot 2 and 3 around <span class="math inline">\(X=0\)</span> and on the second row plot 3, also around <span class="math inline">\(X=0\)</span>. Overall the result is not as poor as in scenario 2, but a causal interpretation of these plots would lead to a severe underestimation of the impact <span class="math inline">\(X\)</span> has on <span class="math inline">\(Y\)</span>.</p>
<p><strong>Scenario 4</strong></p>
<p>Scenario 4 consists of a direct effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. <span class="math inline">\(Z\)</span> meanwhile is unrelated to both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It is however included in the simulation and included in the model that is run to create the PDPs. In a non-simulated setting, <span class="math inline">\(Z\)</span> can be seen as a variable that we assume might be related to <span class="math inline">\(Y\)</span> and therefore include, but in actuality has nothing to do with the causal process and should not be included. We will see now how the PDP deals with this kind of variable in the mix.</p>
<div class="figure" style="text-align: left"><span id="fig:Figure9"></span>
<img src="images/scenario4.jpeg" alt="X directly impacts Y. Z is included in our model, but has no relationship to X or Y" width="100%" />
<p class="caption">
FIGURE 4.9: X directly impacts Y. Z is included in our model, but has no relationship to X or Y
</p>
</div>
For the simulation the initial settings look as follows, again increasing both the standard deviation of the error and the magnitude of observations from this starting point. <span class="math display">\[ Y \leftarrow X + \epsilon_Y  \]</span> <span class="math display">\[ \epsilon_Y ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_X, \epsilon_Z \sim \mathcal{U}(-1,1), \ \ \ \ \ X \leftarrow \epsilon_X,\ \ \ \ \ Z \leftarrow \epsilon_Z, \ \ \ \ \ N \leftarrow 100 \]</span>
<div class="figure" style="text-align: left"><span id="fig:Figure10causal"></span>
<img src="images/scenario4_all.jpg" alt="Comparison for scenario 4 of PDPs under various settings with the (yellow) intervention curve on the right" width="100%" />
<p class="caption">
FIGURE 4.10: Comparison for scenario 4 of PDPs under various settings with the (yellow) intervention curve on the right
</p>
</div>
<p>The PDPs in Figure <a href="pdp-causal.html#fig:Figure10causal">4.10</a> are able to capture the intervention curve well. Only in the cases where <span class="math inline">\(N=100\)</span> is there slight curvature at the extreme ends of <span class="math inline">\(X\)</span>. With this low number of observations the model is less accurate. In all other settings a consistent slope is present from <span class="math inline">\(X = -1\)</span> to <span class="math inline">\(X = 1\)</span>, with the scale of <span class="math inline">\(\hat{Y}\)</span> matching that of <span class="math inline">\(Y_{intervention}\)</span>.</p>
<p><strong>Scenario 5</strong></p>
<p>In this scenario Z is a confounding variable. This is similar to scenario 3 where <span class="math inline">\(X\)</span> was the confounder. Thinking back to the example of <em>age</em> being a confounding variable for <em>level of physical activity</em> and <em>weight gain</em>, in the previous example our <span class="math inline">\(X\)</span> was comparable to the confounder <em>age</em>. In this scenario, we can keep the same example, but say our variable <span class="math inline">\(X\)</span> is now comparable to the variable <em>level of physical activity</em> that is being confounded. Since <span class="math inline">\(X\)</span> has no descendants and there is no backdoor path, the expectation here is that the PDPs will be similar to the intervention curve.</p>
<div class="figure"><span id="fig:Figure11causal"></span>
<img src="book_files/figure-html/Figure11causal-1.svg" alt="Z is a confounding variable impacting both X and Y" width="336" />
<p class="caption">
FIGURE 4.11: Z is a confounding variable impacting both X and Y
</p>
</div>
<p>The following simulation settings were used:</p>
<p><span class="math display">\[ Y \leftarrow Z + X + \epsilon_Y  \]</span> <span class="math display">\[ \epsilon_Y,\epsilon_X ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_Z \sim \mathcal{U}(-1,1), \ \ \ \ \ Z \leftarrow \epsilon_Z, \ \ \ \ \ X \leftarrow Z + \epsilon_X, \ \ \ \ \ N \leftarrow 100 \]</span></p>
<div class="figure" style="text-align: left"><span id="fig:Figure12causal"></span>
<img src="images/scenario5_all.jpg" alt="Comparison for scenario 5 of PDPs under various settings with the (yellow) intervention curve on the right" width="100%" />
<p class="caption">
FIGURE 4.12: Comparison for scenario 5 of PDPs under various settings with the (yellow) intervention curve on the right
</p>
</div>
<p>We can see in Figure <a href="pdp-causal.html#fig:Figure12causal">4.12</a> that aside from the extreme regions of <span class="math inline">\(X\)</span> where the slope is flatter, the PDPs are fairly similar to the Intervention Curves on the right. We can see that as the standard deviations of the errors increase, so does the range of <span class="math inline">\(Y_intervention\)</span> from (-1, 1) to (-2,2). This same trend can be observed in the PDPs. As could be expected, the PDPs with the highest number of observations are are most accurate and have the lowest standard deviation of errors.</p>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">4.5</span> Conclusion</h2>
<p>Causal interpretability of a PDP is dependent on several things: (1) The backdoor criterion being met. We have seen that if the backdoor criterion is met, meaning no variable in the complement set <span class="math inline">\(C\)</span> is a descendant of our variable of interest, the PDP should be the same as the intervention curve. We say should, because in practice it will also depend on: (2) The model fit. Even when the backdoor criterion is met, the PDP might not fully capture the exact same relationship as the intervention curve. Especially in extreme regions, where data is potentially sparse, the PDP can be deceptive. Same in scenarios with a higher(er) error and low number of observations.</p>
<p>Point (2) can be estimated with the standard goodness of fit measures that are pervasive in the statistics literature. It may be noted that models may show a good fit, but do not learn the patterns that we would expect them to learn from a theoretical point of view. Point (1) is even more difficult to verify, especially based on only the data. Here a certain amount of domain knowledge is needed to ensure the assumption is met. Still, it is a hard assumption to verify which could limit the confidence people have in a causal interpretation of a PDP. In the two variable case we may already have difficulty finding a causal interepretation. As we saw with the ice-cream example, the direction of the dependency already makes a large difference. In many cases, the necessary domain knowledge may be hard to attain.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-pearl1993">
<p>Pearl, Judea. 1993. “Comment: Graphical Models, Causality and Intervention.” <em>Statistical Science</em> 8 (3): 266–69.</p>
</div>
<div id="ref-scholbeck">
<p>Scholbeck, Christian. 2018. “Interpretierbares Machine-Learning. Post-Hoc Modellagnostische Verfahren Zur Bestimmung von Prädiktoreffekten in Supervised-Learning-Modellen.” Ludwig-Maximilians-Universität München.</p>
</div>
<div id="ref-zhaohastie">
<p>Zhao, Qingyuan, and Trevor Hastie. 2018. <em>Causal Interpretations of Black-Box Models</em>. <a href="http://web.stanford.edu/~hastie/Papers/pdp_zhao_final.pdf" class="uri">http://web.stanford.edu/~hastie/Papers/pdp_zhao_final.pdf</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pdp-correlated.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ale.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/iml_methods_limitations/edit/master/01-3-pdp-causal.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
