---
output:
  html_document: default
  pdf_document: default
  keep_tex: true
---
# PDP and Correlated Features

*Author: Veronika Kronseder*

## Problem Description

As outlined in chapter 2, PDPs and ICE plots are meaningful graphical tools to visualize the impact of individual feature variables. This is particularly true for black box algorithms, where the mechanism of each feature and its influence on the generated predictions may be difficult to retrace.\citep{Goldstein2013}  

The reliability of the produced curves, however, strongly builds on the independence assumption of the features. Furthermore, results can be misleading in areas with no or little observations, where the curve is drawn as a result of extrapolation. In this chapter, we want to illustrate and discuss the issue of depencencies between different types of variables, missing values and the associated implications on marginalized feature effects with a particular focus on PDPs.  

### What is the issue with dependent features?

When looking at PDPs, one should bear in mind that by definition the partial dependence function does not reflect the isolated effect of $x_S$ while the features in $x_C$ are ignored. This approach would correspond to the conditional expectation $\tilde{f}_S(x_S) = \mathbb{E}_{x_C}[f(x_S, x_C)|x_S]$, which is only congruent to the partial dependence function $f_{x_S}(x_S) = \mathbb{E}_{x_C}[f(x_S, x_C)]$ in case of $x_S$ and $x_C$ being independent.\citep{hastie2013elements}  

Although unlikely in many practical applications, the independence of feature variables is one of the major assumptions to produce meaningful PDPs. Its violation would mean that, when calculating averages of the features in $x_C$, the estimated partial dependence function $\hat{f}_{x_S}(x_S)$ takes unrealistic data points into consideration.\citep{molnar2019}  

Figure \@ref(fig:Figure01) illustrates the problem by contrasting simulated data with independent features $x_1$ and $x_2$ on the left with an example where the two features have a strong linear dependency, and thus are highly correlated, on the right.  

```{r Figure01, echo=FALSE, out.width='100%', fig.cap="Simulated data for independent (left) and strongly correlated (right) features $x_1$ and $x_2$. The marginal distribution of $x_2$ is displayed on the right side of each plot."}
knitr::include_graphics('images/VK_PDP_1_Data_ind_dep.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

When computing the PDP for feature $x_1$, we take $x_2$ into account by calculating its mean (here: $\overline{x}_2 \approx 0$) and keeping the value in $\hat{f}_{x_S}(x_1)$ constant at any point on the x-axis. This makes sense in the independent case, where observations are randomly scattered. However, when looking at the correlated features in the right part of figure \@ref(fig:Figure01), the average of $x_2$ is not a realistic value in combination with $x_1$-values in the very left and the very right part of the feature distribution.  

### What is the issue with extrapolation?
Generally speaking, extrapolation means leaving the distribution of observed data. On the one hand, this can affect the predictions, namely in the event of the prediction function doing 'weird stuff' in unobserved areas. In chapter ?? we will see an example where this instant leads to a failure of the Partial Dependence Plots.\citep{molnar2019}  

On the other hand, PDPs are also directly exposed to extrapolation problems due to the fact that the estimated partial dependence function $\hat{f}_{x_S}$ is evaluated at each observed $x^{(i)}_{S}$, giving a set of N ordered pairs: $\{(x^{(i)}_{S}, \hat{f}_{x^{(i)}_{S}})\}_{i=1}^N$. The resulting coordinates are plottet against each other and joined by lines. Not only outside the margins of observed values, but also in areas with a larger distance between neighboured $x_S$ values, the indicated relationship with the target variable might be inappropriate and volatile in case of outliers.\citep{Goldstein2013}

In figure \@ref(fig:Figure02), a part of the previously simulated observations has been deleted from both the independent and the correlated example to visualize a data situation which might have an impact on the PDP in terms of extrapolation. An example is given in chapter 3.??. The shift in observed areas can also be noticed from the marginal distribution of $x_2$.  

```{r Figure02, echo=FALSE, out.width='100%', fig.cap="Manipulated simulated data for independent (left) and strongly correlated (right) features $x_1$ and $x_2$. Observations with $x_1$ and $x_2$ [0,1.5] have been deleted to artificially produce an extrapolation problem. The marginal distribution of $x_2$, which is displayed on the right side of each plot, is obviously more affected in the correlated case."}
knitr::include_graphics('images/VK_PDP_2_Data_ind_dep_gap.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

The extrapolation problem in PDPs is strongly linked to the aforementioned independence assumption. Independent features are a prerequisite for the computation of meaningful extrapolation results, therefore one could say that both problems go hand in hand. In the following chapters, the failure of PDPs in case of a violation of the independence assumption shall be discussed by means of real data examples (chapter 3.2) and based on simulated cases (chapter 3.4). 


```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mvtnorm)
library(matrixcalc)
library(ggplot2) #is.positive.semi.definite
library(ggpubr) #ggdensity
library(cowplot) #plot_grid
library(mlr)
library(iml)

simulate_data <- function(obs, correlation){
  sigma <- diag(1, nrow = 2)
  sigma[1,2] <- sigma[2,1] <- correlation
  data <- as.data.frame(rmvnorm(n = obs, 
                                mean = rep(0, times = 2), 
                                sigma = sigma))
  colnames(data) <- c("X1", "X2")
  std <- abs(mean(sin(data$X1) + data$X2))*0.1
  data$Y <- sin(data$X1) + data$X2 + rnorm(n = obs, mean = 0, sd = std)
  invisible(data)
}

set.seed(123)
uncorrelated <- simulate_data(obs=1000, correlation = 0)
correlated <- simulate_data(obs=1000, correlation = 0.99)

# Data Manipulation: Produce area with no observations
uncorrelated_gap <- uncorrelated
for (i in 1:nrow(uncorrelated_gap)){
  uncorrelated_gap$X1[i] <- ifelse(uncorrelated_gap$X2[i] > 0 & uncorrelated_gap$X2[i] <= 1.5 & uncorrelated_gap$X1[i] > 0 & uncorrelated_gap$X1[i] <= 1.5, NA, uncorrelated_gap$X1[i])
}
for (i in 1:nrow(uncorrelated_gap)){uncorrelated_gap$X2[i] <- ifelse(is.na(uncorrelated_gap$X1[i]), NA, uncorrelated_gap$X2[i])}
for (i in 1:nrow(uncorrelated_gap)){uncorrelated_gap$Y[i] <- ifelse(is.na(uncorrelated_gap$X1[i]), NA, uncorrelated_gap$Y[i])} 

correlated_gap <- correlated
for (i in 1:nrow(correlated_gap)){
  correlated_gap$X1[i] <- ifelse(correlated_gap$X2[i] > 0 & correlated_gap$X2[i] <= 1.5 & correlated_gap$X1[i] > 0 & correlated_gap$X1[i] <= 1.5, NA, correlated_gap$X1[i])
}
for (i in 1:nrow(correlated_gap)){correlated_gap$X2[i] <- ifelse(is.na(correlated_gap$X1[i]), NA, correlated_gap$X2[i])}
for (i in 1:nrow(correlated_gap)){correlated_gap$Y[i] <- ifelse(is.na(correlated_gap$X1[i]), NA, correlated_gap$Y[i])} 


# Visualization of simulated data
data_plot <- function(data_uncor, data_cor){
  plot_uncorr <- 
    ggplot(data = data_uncor, aes(x = X1, y = X2)) + 
    geom_point(alpha = 0.5) + 
    labs(title = "Independent Features") + 
    theme_bw()+
    theme(plot.title = element_text(size=20, hjust = 0))+
    ylim(-3,3)
  density_uncorr <- 
    ggdensity(data_uncor$X2)+ 
    geom_density() + 
    rotate() + 
    clean_theme()+ 
    rremove("legend") +
    xlim(-3,3)
  
  plot_corr <- 
    ggplot(data = data_cor, aes(x = X1, y = X2)) + 
    geom_point(alpha = 0.5) + 
    labs(title = "Correlated Features") + 
    theme_bw()+
    theme(plot.title = element_text(size=20, hjust = 0))+
    ylim(-3,3)
  density_corr <- 
    ggdensity(data_cor$X2)+ 
    geom_density() + 
    rotate() + 
    clean_theme()+ 
    rremove("legend") +
    xlim(-3,3)
  
  p1 <- plot_grid(plot_uncorr, density_uncorr, ncol = 2, align = "hv", rel_widths = c(4, 1), rel_heights = c(1, 4))
  p2 <- plot_grid(plot_corr, density_corr, ncol = 2, align = "hv", rel_widths = c(4, 1), rel_heights = c(1, 4))
  
  ggarrange(p1, p2, ncol=2, align = "hv")
}

data_plot(uncorrelated, correlated)
data_plot(uncorrelated_gap, correlated_gap)
```


## Dependent Features: Bike Sharing Dataset
In order to investigate the impact of dependent features, we are now looking at the Bike-Sharing dataset from the rental company 'Capital-Bikeshare', which is available for download via the UCI Machine Learning Repository. Besides the daily count of rental bikes between the year 2011 and 2012 in Washington D.C., the dataset contains the corresponding weather and seasonal information.\citep{Fanaee-T}  

For our purposes, the dataset was restricted to the following variables:  

* $y$: cnt (count of total rental bikes including both casual and registered)  
* $x_1$: season: Season (1:springer, 2:summer, 3:fall, 4:winter)  
* $x_2$: yr: Year (0: 2011, 1:2012)  
* $x_3$: mnth: Month (1 to 12)  
* $x_4$: holiday: weather day is holiday or not  
* $x_5$: workingday: If day is neither weekend nor holiday is 1, otherwise is 0.  
* $x_6$: weathersit:  
    + 1: Clear, Few clouds, Partly cloudy, Partly cloudy  
    + 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist  
    + 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds  
    + 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog  
* $x_7$: temp: Normalized temperature in Celsius.  
* $x_8$: atemp: Normalized feeling temperature in Celsius.   
* $x_9$: hum: Normalized humidity.   
* $x_{10}$: windspeed: Normalized wind speed.  

For all machine learning models based on the Bike-Sharing dataset, 'cnt' is used as target variable, while the remaining information serves as feature variables. Six out of these ten features are categorical ($x_1$ to $x_6$), while the rest is measured on a numerical scale ($x_7$ to $x_{10}$). Since the appearance of a PDP depends on the class of the feature(s) of interest, we are looking at three different scenarios of dependency: 

1. Dependency between numerical features
2. Dependency between categorical features
3. Dependency between numerical and categorical features

At the same time, for each of those scenarios, three different learning algorithms shall be compared: 

* Linear Model (LM)
* Random Forest (RF)
* Support Vector Machines (SVM)

### Dependency between Numerical Features
The linear dependency between two numerical features can be measured by the Pearson correlation coefficient.\citep{fahrmeir2016statistik} Figure \@ref(fig:Figure03) shows the correlation matrix of all numerical features used in our analysis. It is striking, but certainly not surprising, that 'temp' and 'atemp' are strongly correlated, not to say almost perfectly collinear.  

```{r Figure03, echo=FALSE, out.width='75%', fig.cap="Matrix of Pearson correlation coefficients between all numerical variables extracted from the bike-sharing dataset.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_3_Num_Correlation_Matrix.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Due to their strong correlation, 'temp' ($x_7$) and 'atemp' ($x_8$) perfectly qualify for our analysis of the impact of dependent features on PDPs. In order to compare the partial dependence curve with and without the influence of dependent features, we compute PDPs based on the following models [^1]: 

\begin{equation}
 y \sim x_1 + x_2 + x_4 + x_5 + x_6 + \mathbf{x_7}  + x_9 + x_{10} (\#eq:1) 
\end{equation}

\begin{equation}
 y \sim x_1 + x_2 + x_4 + x_5 + x_6 + \mathbf{x_8}  + x_9 + x_{10} (\#eq:2)
\end{equation}

\begin{equation}
 y \sim x_1 + x_2 + x_4 + x_5 + x_6 \mathbf{+ x_7 + x_8} + x_9 + x_{10} (\#eq:3)
\end{equation}

[^1]: The representation of the different models with the feature variables connected via '+' shall, in this context, not be read as a (linear) regression model where all coefficients are equal to 1, but rather as a combination of applicable feature variables to explain $y$. The (non-)linear effect of each variable is modelled individually, depending on the observed values and the learner. 

While model \@ref(eq:1) and \@ref(eq:2) only take one of the two substituting variables into account, \@ref(eq:3) considers both 'temp' and 'atemp' in one and the same model. Figures \@ref(fig:Figure04), \@ref(fig:Figure05) and \@ref(fig:Figure06) compare the associated PDPs for the different learning algorithms. Note that 'season' ($x_1$) and 'mnth' ($x_3$) are not taken into account in combination with $x_7$ and/or $x_8$, since there are meaningful associations between those variables, too, as we will show in chapter 3.??. The exclusion from the models at this stage in order to illustrate the isolated effect of the dependence between two numerical variables.  

```{r Figure04, echo=FALSE, out.width='80%', fig.cap="PDPs based on Linear Regression learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_4_Correlated_numerical_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure05, echo=FALSE, out.width='80%', fig.cap="PDPs based on Support Vector Machines learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_5_Correlated_numerical_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure06, echo=FALSE, out.width='80%', fig.cap="PDPs based on Random Forest learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_6_Correlated_numerical_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

In all cases, we can see that the features' effect on the prediction is basically the same for $x_7$ and $x_8$, if only one of the dependent variables is used for modelling (see PDPs in top left and top right corners). If both 'temp' and 'atemp' are relevant for the prediction of $y$, each feature's impact is smoothened and neither the PDP for $x_7$ nor the one for $x_8$ seems to properly reflect the true effect of the temperature on the count of bike rentals. 

### Dependency between Categorical Features
In order to measure the association between two categorical features, we calculate the corrected contingency coefficient, which is based on the $\chi^2$-statistic. Other than the Pearson correlation coefficient, the corrected contingency coefficient is a measure of association $\in [0,1]$ which can only indicate the strength but not the direction of the variables' relationship.\citep{fahrmeir2016statistik} For the categorical features in the Bike-Sharing dataset, we observe the values stated in figure \@ref(fig:Figure07).

```{r Figure07, echo=FALSE, out.width='75%', fig.cap="Matrix of corrected contingency coefficients between all categorical variables extracted from the bike-sharing dataset.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_7_Cat_Correlation_Matrix.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

The only combination of categorical features with an exceptionally high corrected contingency coefficient, is 'season' ($x_1$) and 'mnth' ($x_3$). Also from a content-related point of view, this finding is no surprise, since both variables measure the time of the year. For the computation of the respective PDPs, we use the following models: 

\begin{equation} 
y \sim \mathbf{x_1} + x_2 + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:4)
\end{equation}
\begin{equation}
y \sim x_2 +\mathbf{x_3} + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:5)
\end{equation}
\begin{equation}
y \sim \mathbf{x_1} + x_2 + \mathbf{x_3} + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:6)
\end{equation}

The approach is equivalent to the numeric case, with model \@ref(eq:4) containing only 'season' and \@ref(eq:5) only 'mnth', while both dependent features are part of model \@ref(eq:6). The impact on the PDPs for categorical features are shown in figures \@ref(fig:Figure08), \@ref(fig:Figure09) and \@ref(fig:Figure10).  

```{r Figure08, echo=FALSE, out.width='80%', fig.cap="PDPs based on Linear Regression learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_8_Correlated_categorical_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure09, echo=FALSE, out.width='80%', fig.cap="PDPs based on Support Vector Machines learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_9_Correlated_categorical_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure10, echo=FALSE, out.width='80%', fig.cap="PDPs based on Random Forest learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_10_Correlated_categorical_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Again, in all PDPs based on the different learning algorithms, the results between models with and without dependent features are diverging. The predicted number of bike rentals between the seasons/months shows a stronger variation when modelled without feature dependencies.

### Dependency between Numerical and Categorical Features
Our third dependency scenario seeks to provide an example for a strong correlation between a numerical and a categorical feature. For this constellation, neither the Pearson correlation nor the contingency coefficient are applicable as such, since both methods are limited to their respective classes of variables.  

We can, however, fit a linear model to explain the numeric variable through the categorical feature. By doing so, we produce another numerical variable, the fitted values. In a next step, we can calculate the Pearson correlation coefficient between the observed and the fitted values of the numerical feature. The resulting measure of association lies within the interval $[0,1]$ and is equivalent to the square root of the linear model's variance explained ($R^2$).  QUELLE einfÃ¼gen !!

When applying this procedure to the categorical feature 'season' ($x_1$) and the numerical feature 'temp' ($x_7$), we find that with a value of 0.83, there seems to be a reasonable association between the two features. The PDPs are derived through the following models:  

\begin{equation}
y \sim \mathbf{x_1} + x_2 + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:7)
\end{equation}
\begin{equation}
y \sim x_1 + x_2 + x_4 + x_5 + x_6 + \mathbf{x_7}+ x_9 + x_{10} (\#eq:8)
\end{equation}
\begin{equation}
y \sim \mathbf{x_1} + x_2  + x_4 + x_5 + x_6 +\mathbf{x_7}+ x_9 + x_{10} (\#eq:9)
\end{equation}

Figure \@ref(fig:Figure11), \@ref(fig:Figure12) and \@ref(fig:Figure13) present the Partial Dependence Plots for the three underlying machine learning algorithms (LM, SVM and RF) defined for the purpose of our analysis.  

```{r Figure11, echo=FALSE, out.width='80%', fig.cap="PDPs based on Linear Regression learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_11_Correlated_cat_num_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure12, echo=FALSE, out.width='80%', fig.cap="PDPs based on Support Vector Machines learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_12_Correlated_cat_num_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure13, echo=FALSE, out.width='80%', fig.cap="PDPs based on Random Forest learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_13_Correlated_cat_num_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Compared to the first two scenarios, we observe a more moderate difference between the PDPs when comparing model \@ref(eq:7) and \@ref(eq:8) with just one of the dependent features to the full model \@ref(eq:9). The weaker association between the two variables, in contrast to scenario 1 and 2, could be an explanation for this observation. It is, however, evident that the dependency structure between two feature variables, irrespective of their class, does impact the Partial Dependence Plot.  

```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(ggpubr)
library(ggcorrplot)
library(mlr)
library(iml)
library(e1071) #svm
library(mgcv) #gam

bike_day <- read.csv("day.csv")
bike_day$season <- as.factor(bike_day$season)
bike_day$yr <- as.factor(bike_day$yr)
bike_day$mnth <- as.factor(bike_day$mnth)
bike_day$holiday <- as.factor(bike_day$holiday)
bike_day$workingday <- as.factor(bike_day$workingday)
bike_day$weathersit <- as.factor(bike_day$weathersit)

# Selection of variables for Prediction
bike <- bike_day[, c(4,6,8,9,10,11,12,13,16)]   # w/o season & mnth
bike2 <- bike_day[, c(3,4,5,6,8,9,12,13,16)]    # w/o temp & atemp
bike3 <- bike_day[, c(3,4,6,8,9,10,12,13,16)]   # w/o mnth & atemp


#### CORRELATED NUMERICAL FEATURES ####
# Correlation Matrix
cor_data <- cor(bike[, 5:9])
cor_mat <- 
  ggcorrplot(cor_data,lab = TRUE, 
             method = "square", 
             outline.col = "white", 
             tl.cex = 12, 
             show.legend=F, 
             tl.col="black") + 
  theme_bw(base_size=12) + 
  theme(legend.position="none", 
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5, face="bold", size=15), 
        axis.text = element_text(colour = "black", face="bold", size=12),
        axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(title = "Correlation Matrix of numerical features")

cor_mat

# ML models for numerical features
numerical <- function(learner){
  ## 1.) Both temp & atemp ## 
  task <- makeRegrTask(data = bike, target = "cnt")
  set.seed(1)
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike)
  eff <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "atemp", method = "pdp", grid.size = 50)
  
  p11 <- plot(eff)+ylim(2500,6000)+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with both temp & atemp")+theme_bw()
  p12 <- plot(eff2)+ylim(2500,6000)+labs(title = " ", subtitle = " ")+theme_bw()
  
  ## 2.) Only temp ## 
  task <- makeRegrTask(data = bike[, -6], target = "cnt")
  set.seed(1)
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike[, -6])
  eff <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "hum", method = "pdp", grid.size = 50)
  
  p2 <- plot(eff)+ylim(2500,6000)+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with temp only")+theme_bw()
  
  ## 3.) Only atemp ## 
  task <- makeRegrTask(data = bike[, -5], target = "cnt")
  set.seed(1)
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike[, -5])
  eff <- FeatureEffect$new(mod, feature = "atemp", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "hum", method = "pdp", grid.size = 50)
  
  p3 <- plot(eff)+ylim(2500,6000)+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with atemp only")+theme_bw()
  
  ggarrange(p2, p3, p11, p12, ncol=2, nrow=2)
}

set.seed(123)
numerical(learner = makeLearner("regr.lm"))
numerical(learner = makeLearner("regr.svm"))
numerical(learner = makeLearner("regr.randomForest"))

#### CORRELATED CATEGORICAL FEATURES ####
# Corrected Contingency Coefficient
cont_coeff <- function(data, feature1, feature2){
  a <- as.factor(feature1)
  b <- as.factor(feature2)
  
  chisq <- chisq.test(a, b, correct = F)
  K <- sqrt(chisq$statistic/(chisq$statistic+nrow(data)))
  M <- min(length(levels(a)), length(levels(b)))
  K_korr <- K/sqrt((M-1)/M)
  K_korr
}

cont_mat <- data.frame()
for (i in 1:6){ 
  for (j in 1:6){
    cont_mat[i,j] <- round(cont_coeff(bike2, bike2[,i], bike2[,j]),2)
    colnames(cont_mat)[i] <- names(bike2)[i]
    rownames(cont_mat)[i] <- names(bike2)[i]
  }
  }

plot_cont_mat <- ggcorrplot(as.matrix(cont_mat),lab = TRUE, 
           method = "square", 
           outline.col = "white", 
           tl.cex = 12, 
           show.legend=F, 
           tl.col="black") + 
  labs(title = "Corrected Contingency Coefficients of Categorical Features")+
  theme_bw(base_size=12) + 
  theme(legend.position="none", 
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5, face="bold",size=15), 
        axis.text = element_text(colour = "black", face="bold", size=12),
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_cont_mat

# ML models for categorical features
categorical <- function(learner){
  ## 1.) Both season & mnth ## 
  task <- makeRegrTask(data = bike2, target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike2)
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "mnth", method = "pdp", grid.size = 50)
  
  p11 <- plot(eff)+ylim(c(0,6000))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with both season & mnth")+theme_bw()
  p12 <- plot(eff2)+ylim(c(0,6000))+labs(title = " ", subtitle = " ")+theme_bw()
  
  ## 2.) Only season ## 
  task <- makeRegrTask(data = bike2[,-3], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike2[, -3])
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  
  p2 <- plot(eff)+ylim(c(0,6000))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with season only")+theme_bw()
  
  ## 3.) Only mnth ## 
  task <- makeRegrTask(data = bike2[,-1], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike2[,-1])
  eff <- FeatureEffect$new(mod, feature = "mnth", method = "pdp", grid.size = 50)
  
  p3 <- plot(eff)+ylim(c(0,6000))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with mnth only")+theme_bw()
  
  ggarrange(p2, p3, p11, p12, ncol=2, nrow=2)
}

set.sed(123)
categorical(learner = makeLearner("regr.lm"))
categorical(learner = makeLearner("regr.svm"))
categorical(learner = makeLearner("regr.randomForest"))

#### CORRELATED CATEGORICAL & NUMERICAL FEATURES ####
mod <- lm(temp~season, data=bike3)
cor(bike3$temp, mod$fitted.values)


### ML models ###
catnum <- function(learner){
  ## 1.) Both season & temp ## 
  task <- makeRegrTask(data = bike3, target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike3)
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  
  p11 <- plot(eff)+ylim(c(0,6500))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with both season & temp")+theme_bw()
  p12 <- plot(eff2)+ylim(c(0,6500))+labs(title = " ", subtitle = " ")+theme_bw()
  
  ## 2.) Only season ## 
  task <- makeRegrTask(data = bike3[,-6], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike3[, -6])
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  
  p2 <- plot(eff)+ylim(c(0,6500))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with season only")+theme_bw()
  
  ## 3.) Only temp ## 
  task <- makeRegrTask(data = bike3[,-1], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike3[,-1])
  eff <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  
  p3 <- plot(eff)+ylim(c(0,6500))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with temp only")+theme_bw()
  
  ggarrange(p2, p3, p11, p12, ncol=2, nrow=2)
}

set.seed(123)
catnum(learner = makeLearner("regr.lm"))
catnum(learner = makeLearner("regr.svm"))
catnum(learner = makeLearner("regr.randomForest"))
```


## Dependent Features: Simulated Data
A major disadvantage of the analysis of PDPs on the basis of real data examples is, that we cannot exclude other factors to play a role. As an example, underlying interactions could have an impact on the PDP and hide the true effect of a feature on the predicted target variable. In order to illustrate the isolated impact of dependent variables in the feature space, we have simulated data in different settings, which we will discuss in this chapter.  
For a start, the different settings of simulations used for our investigation shall be introduced. Just like in chapter 2, we are seperately looking at different classes of variables and different machine learning algorithms (LM, RF and SVM).  

## Simulation Settings: Numerical Features
PDPs for independent, correlated and dependent numerical features have been computed for each of the following data generating processes (DGP), which describe the true impact of the features on $y$:

\begin{enumerate}
    \item Linear Dependence: $y = x_1 + x_2 + x_3 + \varepsilon$
    \item Nonlinear Dependence in $x_1$: $y = \sin{( \, 3*x_1 ) \,} + x_2 + x_3 + \varepsilon$
    \item Missing informative feature $x_3$: $y = x_1 + x_2 + x_3 + \varepsilon$ with $x_3$ relevant for the DGP but unconsidered in the machine learning model
\end{enumerate}  

In the independent case, the feature variables $x_1$, $x_2$ and $x_3$ have been drawn from a gaussian distribution with $\mu = 0$, $\sigma^2 = 1$ and a correlation coefficient of $\rho_{ij} = 0$ $\forall$ $i \ne j$, $i,j \in \{1,2,3\}$.  
The correlated case is based on the same parameters for $\mu$ and  $\sigma^2$, but a correlation coefficient of $\rho_{12} = \rho_{21} = 0.90$, i.e. a relatively strong correlation between $x_1$ and $x_2$, and $\rho_{ij} = 0$ otherwise.  
The dependent case describes the event of perfect multicollinearity, where $x_2$ is a duplicate of $x_1$, based on the data generated in the independent case.  
The dependent variable $y$ results from the respective DGP with an error term $\varepsilon \sim N(0, \sigma^2_\varepsilon)$ and $\sigma^2_\varepsilon$ depending on the feature values.  

The simulation of the data itself constitutes one source of variation in the PDPs. For this reason, the data simulation has been repeated 20 times for each analysis and the resulting PDP curves are shown as gray lines in the plots below. The thicker, black line represents the average partial dependence curve over these 20 simulations and the error bars indicate their variation. Additionally, a red line represents the true effect of the feature for which the PDP is computed. In all cases, the simulations are based on a number of 500 observations and grid size 50.  

Since in the dependent case, $x_2$ is simply a duplicate of $x_1$, the DGP could also be written as $y = 2*x_1 + x_3 + \varepsilon$ in setting 1 and 3 and $y = \sin{( \, 3*x_1 ) \,} + x_1 + x_3 + \varepsilon$ in setting 2. For the purpose of this analysis, we are looking at each of the three features' PDPs separately. However, in order to illustrate the aforementioned, the common effect of $x_1$ and $x_2$ on the prediction is added to the plots as dashed blue line.  

### Simulation of Setting 1: Linear Dependence
#### PDPs for Linear Model
FIGURE 14 (LM independent): The Partial Dependence Plots for each feature based on the Linear Model adequately reflect the linear dependence structure. The effect is equivalent in each PDP, since all features have the same impact and are independent from each other.  
FIGURE 15 (LM correlated): Even with a relatively strong correlation of features $x_1$ and $x_2$, the PDPs adequately reflect the linear dependence structure when predictions are computed from the Linear Model.  
FIGURE 16 (LM dependent): In the event of perfect multicollinearity, the PDP for one of the dependent features ($x_2$) fails, while the PDPs for both other features adequately reflect the linear dependence structure.  

#### PDPs for Random Forest
FIGURE 17 (RF independent): The Partial Dependence Plots for each feature based on Random Forest cannot adequately reflect the linear dependence structure, particularly at the margins of the feature's distribution. Again, there is no visual differentiation between the different features due to independence.  
FIGURE 18 (RF correlated): Compared to the independent case, the computation of PDPs with a Random Forest learner does not produce significantly worse results when two features are correlated and the relationship between all variables and $y$ is linear.  
FIGURE 19 (RF dependent): The comparison of the PDPs with perfect multicollinearity to the correlated case reveals a slightly increased variation in the individual PDP curves. Other than in the Linear Model, the learner is not able to reveal the true common effect of $x_1$ and $x_2$.  

#### PDPs for Support Vector Machines
FIGURE 20 (SVM independent): Support Vector Machines as learning algorithms are able to reproduce the respective feature's effect on the prediction fairly adequate in case of independence. The accuracy decreases in the margins of the feature's distribution.  
FIGURE 21 (SVM correlated): With two correlated features, the prediction interval of both becomes smaller, while the learner produces the same 'shape' of effect in both cases.  
FIGURE 22 (SVM dependent): The observation depicted in the correlated case is even more evident if two features are identical. None of the PDPs for the dependent features reveals the true common effect of $x_1$ and $x_2$.  



### Simulation of Setting 2: Nonlinear Dependence
In simulation setting 2 we are looking at a DGP with a nonlinear relationship of $x_1$ and $y$ and a linear impact of $x_2$ and $x_3$. Due to the nonlinearity in one of the features, it is clear that the LM would not deliver accurate results. For this reason, we will restrict our analysis to RF and SVM.

#### PDPs for Random Forest
FIGURE 23 (RF independent):  From the PDP of feature $x_1$ it is evident that Random Forest as a learner can retrace the nonlinear effect of the variable quite well, except for the margin areas of the feature distribution. The PDPs for feature $x_2$ and $x_3$ are equivalent to those in simulation setting 1.  
FIGURE 24 (RF correlated):  With a simulated correlation between features $x_1$ and $x_2$ and a nonlinear relationship of $x_1$ and $y$, the ability of the respecitve PDPs to illustrate the feature's effect degrades. Both the nonlinear effect of $x_1$ and the linear effect of $x_2$ are distorted in the PDPs.  
FIGURE 25 (RF dependent):  In the event of perfect multicollinearity, the PDPs for the involved feature variables fail even more than previously. In contrast to the correlated case, we can observe that both curves take on a similar 'shape', which very roughly approximates the common effect.  

#### PDPs for Support Vector Machines
When using Support Vector Machines as machine learning algorithm, the interpretation of the PDPs based on Random Forest is equivalently applicable. FIGURES 26, 27 and 28 illustrate the partial dependence curves for setting 2 with SVM.

FIGURE 26 (SVM independent)  
FIGURE 27 (SVM correlated)  
FIGURE 28 (SVM dependent)  


### Simulation of Setting 3: Missing informative feature $x_3$
In simulation setting 3, we assume that there are 3 variables with an impact on the data generating process of $y$. In the training process of the machine learning model in this scenario, only two of those are considered. Consequently, when looking at the PDPs, we only compare the independent, the correlated and the dependent case for $x_1$ and $x_2$ respectively.  

#### PDPs for Linear Model
FIGURE 29 (LM independent): Compared the PDPs the Linear Model produced in setting 1, the variation in individual PDPs is slightly higher with missing information from $x_3$. Overall, the learner can adequately reflect the linear feature effects of $x_1$ and $x_2$.  
FIGURE 30 (LM correlated):  Other than in setting 1, the fact that the information of $x_3$ is missing in the model seems to increase the variablility between the individual PDPs. On average, we still obtain the true linear effect of the correlated features, but there are some individual curves which do indicate a steeper or more moderate slope.  
FIGURE 31 (LM dependent): The PDPs drawn on basis of the Linear Model and dependent features indicate that for both individual features, the PDP consistently provides false effects on the predicted outcome. While both effects are actually linear with a slope of 1, the PDP for $x_1$ shows a steeper increase and $x_2$ fails completely. Nonetheless, the PDP for $x_1$ does reflect the common effect of both variables together.

#### PDPs for Random Forest
FIGURE 32 (RF independent): Compared to setting 1, where all relevant feature variables were taken into account for the trainig of the model, the variation in PDP curves in setting 3 is larger. Between features $x_1$ and $x_2$, which are independent, there is no systematic difference traceable.  
FIGURE 33 (RF correlated): Other than an increased variability between the individual PDP curves and a slightly tighter prediction interval, with correlated features and Random Forest as learner, there is no apparent deviation to the PDPs of independent features.  
FIGURE 34 (RF dependent): In accordance with the observations made in setting 1, the interval of predicted values for dependent features become even smaller while the PDP curves further deviate from the true effect. Neither the individual effect of each feature, nor their common effect are illustrated adequately.  

#### PDPs for Support Vector Machines
FIGURE 35 (SVM independent): Similar to learning based on Random Forest, SVM with missing feature variable $x_3$ produces a higher variability between the simulated PDP curves. The margin areas, where the PDPs cannot adequately reflect the linear dependence, are broader than in setting 1.  
FIGURE 36 (SVM correlated): In the event of the two remaining features $x_1$ and $x_2$ being strongly correlated, the issue of larger variability between the individual simulations aggravates and the ability to reveal the linear effect ceases.  
FIGURE 37 (SVM dependent): In the event of a perfect multicollinearity of $x_1$ and $x_2$, the variablity of the individual PDP curves becomes smaller, but at the same time the models' ability to uncover the true linear effect vanishes. The interval of predicted values is remarkably smaller than in the independent case.  

## Simulation Settings: Categorical Features
In this chapter we want to investigate the impact of dependencies between two categorical and between a categorical and a numerical feature. For this purpose, we simulate data with a number of 1000 randomly drawn observations and three feature variables, where:  
\begin{itemize}
    \item $x_1$ categorical variable $\in \{0,1\}$,
    \item $x_2$ categorical variable $\in \{A,B,C\}$,
    \item $x_3$ numerical variable with $x_3 \sim N(\mu, \sigma^2)$.
\end{itemize}
All features are characterized by their linear relationship with the target variable: $y=x_1+x_2+x_3+\varepsilon$. \\
Again, in order to isolate the individual effects of two dependent features on their respective PDPs, we define three different simulation settings:
\begin{enumerate}
    \item Independent Case: In this setting, the feature variables are drawn independently from each other, i.e. the observations are randomly sampled with the following parameters:
    \begin{itemize}
        \item $x_1: P(x_1=1)=P(x_1=0)=0.5$
        \item $x_2: P(x_2=A)=0.475, P(x_2=B)=0.175, P(x_2=C)=0.35)$
        \item $x_3: P(x_3 \sim N(1,1))=0.5, P(x_3 \sim N(20,2))=0.5$
    \end{itemize}
The association between $x_1$ and $x_2$ can be measured by the corrected contingency coefficient, which is rather low with a value of 0.10. In accordance with the approach in chapter 2.?, we calculate the association between $x_1$ and $x_3$ by means of the variance-explained measure. With a value of 0.01 we take the independence assumption as confirmed.

    \item Dependence between two categorical features: In this setting, $x_1$ and $x_2$ are depending on each each other, i.e. the observations are randomly sampled with the following parameters:
    \begin{itemize}
        \item $x_1: P(x_1=1)=P(x_1=0)=0.5$
        \item $x_2: \begin{cases} P(x_2=A)=0.90, P(x_2=B)=0.10, P(x_2=C)=0), \text{if } x_1=0, \\ P(x_2=A)=0.05, P(x_2=B)=0.25, P(x_2=C)=0.70), \text{if } x_1=1 \end{cases}$
        \item $x_3: P(x_3 \sim N(1,1))=0.5, P(x_3 \sim N(20,2))=0.5$
    \end{itemize}
The corrected contingency coefficient of 0.94 comfirms a strong dependence between features $x_1$ and $x_2$. 

    \item Dependence between categorical and numerical features: In this setting, $x_1$ and $x_3$ are depending on each each other, i.e. the observations are randomly sampled with the following parameters:
    \begin{itemize}
        \item $x_1: P(x_1=1)=P(x_1=0)=0.5$
        \item $x_2: P(x_2=A)=0.475, P(x_2=B)=0.175, P(x_2=C)=0.35)$
        \item $x_3: \begin{cases} x_3 \sim N(1,1), \text{if }  x_1=0 \\ x_3 \sim N(20,2), \text{if } x_1=1 \end{cases}$
    \end{itemize}
With a value of 0.986, the variance-explained measure indicates a substantial degree of dependence between $x_1$ and $x_3$. 
\end{enumerate}

#### PDPs for Linear Model
FIGURE 38  
FIGURE 38 shows the PDPs for all feature variables and all simulation settings based on the Linear Model. Apparently, the Linear Model is robust against our simulated dependencies, since the PDPs for the individual settings do not differ significantly. 

#### PDPs for Random Forest
FIGURE 39  
FIGURE 39 shows the PDPs for all feature variables and all simulation settings based on Random Forest. Here, the learner seems to be impacted much stronger by our simulated dependencies, since the PDPs for dependent features indicate feature effects which differ from those in the independent case.

#### PDPs for Support Vector Machines
FIGURE 40  
FIGURE 40 shows the PDPs for all feature variables and all simulation settings based on Support Vector Machines. Apparently, the Model is robust against our simulated dependencies, since the PDPs for the individual settings do not differ significantly, just like in the LM. 


## Extrapolation Problem: Simulation
### Simulation based on established learners
As announced in chapter ??, in addition to the issue with dependent features, we want to investigate the extrapolation problem and its implications for the computation of Partial Dependence Plots. For this purpose, the dataset introduced in chapter ?? was simulated, once with $X_1$ and $X_2$ being independent, and once with both features strongly correlated. Remember that in a next step, the observed data was manipulated by cutting out all observations with $X_1, X_2 \in [0,1.5]$, and thus artificially producing an area with no observations. \\
In this chapter, we are looking at the PDPs resulting from these modifications. FIGURE 41 compares the PDP curves derived for both features based on the complete, uncorrelated dataset to its manipulated version with missing values. Apparently, with the more complex, nonlinear effect of $X_1$, there is already a deviation from the partial dependence curves to the true feature effect due to the extrapolation problem.  

FIGURE 41 (PDP Extrapolation Problem uncorrelated): The figure shows PDPs for feature $X_1$ and $X_2$, based on the complete, uncorrelated datasset in the upper part and based on same data with missing observations in the lower part of the plot. The red curve represents the true effect of the respective feature, while the PDPs derived from the Machine learning models are represented by curves drawn in black (Random Forest) and blue (SVM).  

In FIGURE 42 we do the same comparison, but this time based on the dataset with strongly correlated features. It is easy to recognize that in this constellation, the PDPs come up with estimated effects which are far from the true effects on the prediction, irrespective of the learning algorithm.  

FIGURE 42 (PDP Extrapolation Problem correlated): The figure shows PDPs for feature $X_1$ and $X_2$, based on the complete, correlated datasset in the upper part and based on same data with missing observations in the lower part of the plot. The red curve represents the true effect of the respective feature, while the PDPs derived from the Machine learning models are represented by curves drawn in black (Random Forest) and blue (SVM).  

### Simulation based on own prediction function
Another example where PDPs are prone to fail is in the event of the prediction function doing 'weird' stuff in areas outside the feature distribution. In order to illustrate this extreme case, a dataset of two dependent features has been simulated and an individual prediction function was defined.  

As an illustrative example we want to assume that it is our intention to predict the size of a potato ($\hat{y}$) by means of the share of maximum amount of soil ($x_1$) and the share of maximum amount of water ($x_2$) available during the process of growing the plant. The feature variables are dependent in the sense that when using a larger amount of soil, the farmer would also use a larger amount of water, i.e. $x_1$ and $x_2$ are positively correlated. Typically, the more ressources the farmer invests, the larger the crops. The corresponding model is basically a simple linear regression which adds up the two components. However, in the event of improper planting, meaning the usage of a too large amount of water in proportion to the soil (and vice versa), the plant would die and the result would be a potato of size 0. Luckily, all farmers in our dataset know how to grow potatoes, therefore there are no such zero cases in the underlying observations. FIGURE 42 illustrates the distribution of our observations and the prediction function.  
FIGURE 42  
Based on the linear regression model, the prediction function and particularly in view of the observed data, one would expect to uncover the linear effect of both feature variables when looking at the corresponding PDPs. As we can see in figure 43, this is not necessarily the case. Particularly the PDP for feature $X_1$ fails due in the areas where the prediction function does 'weird' stuff compared to what has been observed.  
FIGURE 43  

## Summary
Our analysis of Partial Dependence Plots in the context of dependent features and missing values has revealed that both a violation of the underlying independence assumption and the presence of areas with no observations can have a significant impact on the marginalized feature effects. As a consequence, there is a risk of misinterpretation of the effect of features in $x_S$. However, we have also seen cases where the PDP (or the underlying machine learning algorithm) proved to be relatively robust against the dependence of features. Besides the independence assumtion, there are also other parameters playing a role for the accuracy of PDPs, like the grid size, the number of observations, the learning algorithm, variance in the data, complexity of the data generating process, etc.  
In practical applications it is recommended to analyse the variables used in the model, both by means of correlation and/or association measures and from a content-based perspective. Furthermore, data scientists can apply further methods, such as ALE plots, which will be discussed in chapter ??. 

