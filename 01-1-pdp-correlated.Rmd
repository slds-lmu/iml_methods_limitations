---
output:
  html_document: default
  pdf_document: default
  keep_tex: true
---
# PDP and Correlated Features

*Author: Veronika Kronseder*

## Problem Description

As outlined in chapter 2, PDPs and ICE plots are meaningful graphical tools to visualize the impact of individual feature variables. This is particularly true for black box algorithms, where the mechanism of each feature and its influence on the generated predictions may be difficult to retrace.\citep{Goldstein2013}  

The reliability of the produced curves, however, strongly builds on the independence assumption of the features. Furthermore, results can be misleading in areas with no or little observations, where the curve is drawn as a result of extrapolation. In this chapter, we want to illustrate and discuss the issue of depencencies between different types of variables, missing values and the associated implications on marginalized feature effects with a particular focus on PDPs.  

### What is the issue with dependent features?

When looking at PDPs, one should bear in mind that by definition the partial dependence function does not reflect the isolated effect of $x_S$ while the features in $x_C$ are ignored. This approach would correspond to the conditional expectation $\tilde{f}_S(x_S) = \mathbb{E}_{x_C}[f(x_S, x_C)|x_S]$, which is only congruent to the partial dependence function $f_{x_S}(x_S) = \mathbb{E}_{x_C}[f(x_S, x_C)]$ in case of $x_S$ and $x_C$ being independent.\citep{hastie2013elements}  

Although unlikely in many practical applications, the independence of feature variables is one of the major assumptions to produce meaningful PDPs. Its violation would mean that, when calculating averages of the features in $x_C$, the estimated partial dependence function $\hat{f}_{x_S}(x_S)$ takes unrealistic data points into consideration.\citep{molnar2019}  

Figure \@ref(fig:Figure01) illustrates the problem by contrasting simulated data with independent features $x_1$ and $x_2$ on the left with an example where the two features have a strong linear dependency, and thus are highly correlated, on the right.  

```{r Figure01, echo=FALSE, out.width='100%', fig.cap="Simulated data for independent (left) and strongly correlated (right) features $x_1$ and $x_2$. The marginal distribution of $x_2$ is displayed on the right side of each plot."}
knitr::include_graphics('images/VK_PDP_1_Data_ind_dep.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

When computing the PDP for feature $x_1$, we take $x_2$ into account by calculating its mean (here: $\overline{x}_2 \approx 0$) and keeping the value in $\hat{f}_{x_S}(x_1)$ constant at any point on the x-axis. This makes sense in the independent case, where observations are randomly scattered. However, when looking at the correlated features in the right part of figure \@ref(fig:Figure01), the average of $x_2$ is not a realistic value in combination with $x_1$-values in the very left and the very right part of the feature distribution.  

### What is the issue with extrapolation?
Generally speaking, extrapolation means leaving the distribution of observed data. On the one hand, this can affect the predictions, namely in the event of the prediction function doing 'weird stuff' in unobserved areas. In chapter ?? we will see an example where this instant leads to a failure of the Partial Dependence Plots.\citep{molnar2019}  

On the other hand, PDPs are also directly exposed to extrapolation problems due to the fact that the estimated partial dependence function $\hat{f}_{x_S}$ is evaluated at each observed $x^{(i)}_{S}$, giving a set of N ordered pairs: $\{(x^{(i)}_{S}, \hat{f}_{x^{(i)}_{S}})\}_{i=1}^N$. The resulting coordinates are plottet against each other and joined by lines. Not only outside the margins of observed values, but also in areas with a larger distance between neighboured $x_S$ values, the indicated relationship with the target variable might be inappropriate and volatile in case of outliers.\citep{Goldstein2013}

In figure \@ref(fig:Figure02), a part of the previously simulated observations has been deleted from both the independent and the correlated example to visualize a data situation which might have an impact on the PDP in terms of extrapolation. An example is given in chapter 3.??. The shift in observed areas can also be noticed from the marginal distribution of $x_2$.  

```{r Figure02, echo=FALSE, out.width='100%', fig.cap="Manipulated simulated data for independent (left) and strongly correlated (right) features $x_1$ and $x_2$. Observations with $x_1$ and $x_2$ [0,1.5] have been deleted to artificially produce an extrapolation problem. The marginal distribution of $x_2$, which is displayed on the right side of each plot, is obviously more affected in the correlated case."}
knitr::include_graphics('images/VK_PDP_2_Data_ind_dep_gap.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

The extrapolation problem in PDPs is strongly linked to the aforementioned independence assumption. Independent features are a prerequisite for the computation of meaningful extrapolation results, therefore one could say that both problems go hand in hand. In the following chapters, the failure of PDPs in case of a violation of the independence assumption shall be discussed by means of real data examples (chapter 3.2) and based on simulated cases (chapter 3.4). 


```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mvtnorm)
library(matrixcalc)
library(ggplot2) #is.positive.semi.definite
library(ggpubr) #ggdensity
library(cowplot) #plot_grid
library(mlr)
library(iml)

simulate_data <- function(obs, correlation){
  sigma <- diag(1, nrow = 2)
  sigma[1,2] <- sigma[2,1] <- correlation
  data <- as.data.frame(rmvnorm(n = obs, 
                                mean = rep(0, times = 2), 
                                sigma = sigma))
  colnames(data) <- c("X1", "X2")
  std <- abs(mean(sin(data$X1) + data$X2))*0.1
  data$Y <- sin(data$X1) + data$X2 + rnorm(n = obs, mean = 0, sd = std)
  invisible(data)
}

set.seed(123)
uncorrelated <- simulate_data(obs=1000, correlation = 0)
correlated <- simulate_data(obs=1000, correlation = 0.99)

# Data Manipulation: Produce area with no observations
uncorrelated_gap <- uncorrelated
for (i in 1:nrow(uncorrelated_gap)){
  uncorrelated_gap$X1[i] <- ifelse(uncorrelated_gap$X2[i] > 0 & uncorrelated_gap$X2[i] <= 1.5 & uncorrelated_gap$X1[i] > 0 & uncorrelated_gap$X1[i] <= 1.5, NA, uncorrelated_gap$X1[i])
}
for (i in 1:nrow(uncorrelated_gap)){uncorrelated_gap$X2[i] <- ifelse(is.na(uncorrelated_gap$X1[i]), NA, uncorrelated_gap$X2[i])}
for (i in 1:nrow(uncorrelated_gap)){uncorrelated_gap$Y[i] <- ifelse(is.na(uncorrelated_gap$X1[i]), NA, uncorrelated_gap$Y[i])} 

correlated_gap <- correlated
for (i in 1:nrow(correlated_gap)){
  correlated_gap$X1[i] <- ifelse(correlated_gap$X2[i] > 0 & correlated_gap$X2[i] <= 1.5 & correlated_gap$X1[i] > 0 & correlated_gap$X1[i] <= 1.5, NA, correlated_gap$X1[i])
}
for (i in 1:nrow(correlated_gap)){correlated_gap$X2[i] <- ifelse(is.na(correlated_gap$X1[i]), NA, correlated_gap$X2[i])}
for (i in 1:nrow(correlated_gap)){correlated_gap$Y[i] <- ifelse(is.na(correlated_gap$X1[i]), NA, correlated_gap$Y[i])} 


# Visualization of simulated data
data_plot <- function(data_uncor, data_cor){
  plot_uncorr <- 
    ggplot(data = data_uncor, aes(x = X1, y = X2)) + 
    geom_point(alpha = 0.5) + 
    labs(title = "Independent Features") + 
    theme_bw()+
    theme(plot.title = element_text(size=20, hjust = 0))+
    ylim(-3,3)
  density_uncorr <- 
    ggdensity(data_uncor$X2)+ 
    geom_density() + 
    rotate() + 
    clean_theme()+ 
    rremove("legend") +
    xlim(-3,3)
  
  plot_corr <- 
    ggplot(data = data_cor, aes(x = X1, y = X2)) + 
    geom_point(alpha = 0.5) + 
    labs(title = "Correlated Features") + 
    theme_bw()+
    theme(plot.title = element_text(size=20, hjust = 0))+
    ylim(-3,3)
  density_corr <- 
    ggdensity(data_cor$X2)+ 
    geom_density() + 
    rotate() + 
    clean_theme()+ 
    rremove("legend") +
    xlim(-3,3)
  
  p1 <- plot_grid(plot_uncorr, density_uncorr, ncol = 2, align = "hv", rel_widths = c(4, 1), rel_heights = c(1, 4))
  p2 <- plot_grid(plot_corr, density_corr, ncol = 2, align = "hv", rel_widths = c(4, 1), rel_heights = c(1, 4))
  
  ggarrange(p1, p2, ncol=2, align = "hv")
}

data_plot(uncorrelated, correlated)
data_plot(uncorrelated_gap, correlated_gap)
```


## Dependent Features: Bike Sharing Dataset
In order to investigate the impact of dependent features, we are now looking at the Bike-Sharing dataset from the rental company 'Capital-Bikeshare', which is available for download via the UCI Machine Learning Repository. Besides the daily count of rental bikes between the year 2011 and 2012 in Washington D.C., the dataset contains the corresponding weather and seasonal information.\citep{Fanaee-T}  

For our purposes, the dataset was restricted to the following variables:  

* $y$: cnt (count of total rental bikes including both casual and registered)  
* $x_1$: season: Season (1:springer, 2:summer, 3:fall, 4:winter)  
* $x_2$: yr: Year (0: 2011, 1:2012)  
* $x_3$: mnth: Month (1 to 12)  
* $x_4$: holiday: weather day is holiday or not  
* $x_5$: workingday: If day is neither weekend nor holiday is 1, otherwise is 0.  
* $x_6$: weathersit:  
    + 1: Clear, Few clouds, Partly cloudy, Partly cloudy  
    + 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist  
    + 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds  
    + 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog  
* $x_7$: temp: Normalized temperature in Celsius.  
* $x_8$: atemp: Normalized feeling temperature in Celsius.   
* $x_9$: hum: Normalized humidity.   
* $x_{10}$: windspeed: Normalized wind speed.  

For all machine learning models based on the Bike-Sharing dataset, 'cnt' is used as target variable, while the remaining information serves as feature variables. Six out of these ten features are categorical ($x_1$ to $x_6$), while the rest is measured on a numerical scale ($x_7$ to $x_{10}$). Since the appearance of a PDP depends on the class of the feature(s) of interest, we are looking at three different scenarios of dependency: 

1. Dependency between numerical features
2. Dependency between categorical features
3. Dependency between numerical and categorical features

At the same time, for each of those scenarios, three different learning algorithms shall be compared: 

* Linear Model (LM)
* Random Forest (RF)
* Support Vector Machines (SVM)

### Dependency between Numerical Features
The linear dependency between two numerical features can be measured by the Pearson correlation coefficient.\citep{fahrmeir2016statistik} Figure \@ref(fig:Figure03) shows the correlation matrix of all numerical features used in our analysis. It is striking, but certainly not surprising, that 'temp' and 'atemp' are strongly correlated, not to say almost perfectly collinear.  

```{r Figure03, echo=FALSE, out.width='75%', fig.cap="Matrix of Pearson correlation coefficients between all numerical variables extracted from the bike-sharing dataset.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_3_Num_Correlation_Matrix.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Due to their strong correlation, 'temp' ($x_7$) and 'atemp' ($x_8$) perfectly qualify for our analysis of the impact of dependent features on PDPs. In order to compare the partial dependence curve with and without the influence of dependent features, we compute PDPs based on the following models [^1]: 

\begin{equation}
 y \sim x_1 + x_2 + x_4 + x_5 + x_6 + \mathbf{x_7}  + x_9 + x_{10} (\#eq:1) 
\end{equation}

\begin{equation}
 y \sim x_1 + x_2 + x_4 + x_5 + x_6 + \mathbf{x_8}  + x_9 + x_{10} (\#eq:2)
\end{equation}

\begin{equation}
 y \sim x_1 + x_2 + x_4 + x_5 + x_6 \mathbf{+ x_7 + x_8} + x_9 + x_{10} (\#eq:3)
\end{equation}

[^1]: The representation of the different models with the feature variables connected via '+' shall, in this context, not be read as a (linear) regression model where all coefficients are equal to 1, but rather as a combination of applicable feature variables to explain $y$. The (non-)linear effect of each variable is modelled individually, depending on the observed values and the learner. 

While model \@ref(eq:1) and \@ref(eq:2) only take one of the two substituting variables into account, \@ref(eq:3) considers both 'temp' and 'atemp' in one and the same model. Figures \@ref(fig:Figure04), \@ref(fig:Figure05) and \@ref(fig:Figure06) compare the associated PDPs for the different learning algorithms. Note that 'season' ($x_1$) and 'mnth' ($x_3$) are not taken into account in combination with $x_7$ and/or $x_8$, since there are meaningful associations between those variables, too, as we will show in chapter 3.??. The exclusion from the models at this stage in order to illustrate the isolated effect of the dependence between two numerical variables.  

```{r Figure04, echo=FALSE, out.width='80%', fig.cap="PDPs based on Linear Regression learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_4_Correlated_numerical_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure05, echo=FALSE, out.width='80%', fig.cap="PDPs based on Support Vector Machines learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_5_Correlated_numerical_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure06, echo=FALSE, out.width='80%', fig.cap="PDPs based on Random Forest learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_6_Correlated_numerical_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

In all cases, we can see that the features' effect on the prediction is basically the same for $x_7$ and $x_8$, if only one of the dependent variables is used for modelling (see PDPs in top left and top right corners). If both 'temp' and 'atemp' are relevant for the prediction of $y$, each feature's impact is smoothened and neither the PDP for $x_7$ nor the one for $x_8$ seems to properly reflect the true effect of the temperature on the count of bike rentals. 

### Dependency between Categorical Features
In order to measure the association between two categorical features, we calculate the corrected contingency coefficient, which is based on the $\chi^2$-statistic. Other than the Pearson correlation coefficient, the corrected contingency coefficient is a measure of association $\in [0,1]$ which can only indicate the strength but not the direction of the variables' relationship.\citep{fahrmeir2016statistik} For the categorical features in the Bike-Sharing dataset, we observe the values stated in figure \@ref(fig:Figure07).

```{r Figure07, echo=FALSE, out.width='75%', fig.cap="Matrix of corrected contingency coefficients between all categorical variables extracted from the bike-sharing dataset.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_7_Cat_Correlation_Matrix.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

The only combination of categorical features with an exceptionally high corrected contingency coefficient, is 'season' ($x_1$) and 'mnth' ($x_3$). Also from a content-related point of view, this finding is no surprise, since both variables measure the time of the year. For the computation of the respective PDPs, we use the following models: 

\begin{equation} 
y \sim \mathbf{x_1} + x_2 + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:4)
\end{equation}
\begin{equation}
y \sim x_2 +\mathbf{x_3} + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:5)
\end{equation}
\begin{equation}
y \sim \mathbf{x_1} + x_2 + \mathbf{x_3} + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:6)
\end{equation}

The approach is equivalent to the numeric case, with model \@ref(eq:4) containing only 'season' and \@ref(eq:5) only 'mnth', while both dependent features are part of model \@ref(eq:6). The impact on the PDPs for categorical features are shown in figures \@ref(fig:Figure08), \@ref(fig:Figure09) and \@ref(fig:Figure10).  

```{r Figure08, echo=FALSE, out.width='80%', fig.cap="PDPs based on Linear Regression learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_8_Correlated_categorical_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure09, echo=FALSE, out.width='80%', fig.cap="PDPs based on Support Vector Machines learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_9_Correlated_categorical_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure10, echo=FALSE, out.width='80%', fig.cap="PDPs based on Random Forest learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_10_Correlated_categorical_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Again, in all PDPs based on the different learning algorithms, the results between models with and without dependent features are diverging. The predicted number of bike rentals between the seasons/months shows a stronger variation when modelled without feature dependencies.

### Dependency between Numerical and Categorical Features
Our third dependency scenario seeks to provide an example for a strong correlation between a numerical and a categorical feature. For this constellation, neither the Pearson correlation nor the contingency coefficient are applicable as such, since both methods are limited to their respective classes of variables.  

We can, however, fit a linear model to explain the numeric variable through the categorical feature. By doing so, we produce another numerical variable, the fitted values. In a next step, we can calculate the Pearson correlation coefficient between the observed and the fitted values of the numerical feature. The resulting measure of association lies within the interval $[0,1]$ and is equivalent to the square root of the linear model's variance explained ($R^2$).  QUELLE einfÃ¼gen !!

When applying this procedure to the categorical feature 'season' ($x_1$) and the numerical feature 'temp' ($x_7$), we find that with a value of 0.83, there seems to be a reasonable association between the two features. The PDPs are derived through the following models:  

\begin{equation}
y \sim \mathbf{x_1} + x_2 + x_4 + x_5 + x_6 + x_9 + x_{10} (\#eq:7)
\end{equation}
\begin{equation}
y \sim x_1 + x_2 + x_4 + x_5 + x_6 + \mathbf{x_7}+ x_9 + x_{10} (\#eq:8)
\end{equation}
\begin{equation}
y \sim \mathbf{x_1} + x_2  + x_4 + x_5 + x_6 +\mathbf{x_7}+ x_9 + x_{10} (\#eq:9)
\end{equation}

Figure \@ref(fig:Figure11), \@ref(fig:Figure12) and \@ref(fig:Figure13) present the Partial Dependence Plots for the three underlying machine learning algorithms (LM, SVM and RF) defined for the purpose of our analysis.  

```{r Figure11, echo=FALSE, out.width='80%', fig.cap="PDPs based on Linear Regression learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_11_Correlated_cat_num_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure12, echo=FALSE, out.width='80%', fig.cap="PDPs based on Support Vector Machines learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_12_Correlated_cat_num_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

```{r Figure13, echo=FALSE, out.width='80%', fig.cap="PDPs based on Random Forest learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_13_Correlated_cat_num_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Compared to the first two scenarios, we observe a more moderate difference between the PDPs when comparing model \@ref(eq:7) and \@ref(eq:8) with just one of the dependent features to the full model \@ref(eq:9). The weaker association between the two variables, in contrast to scenario 1 and 2, could be an explanation for this observation. It is, however, evident that the dependency structure between two feature variables, irrespective of their class, does impact the Partial Dependence Plot.  

```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(ggpubr)
library(ggcorrplot)
library(mlr)
library(iml)
library(e1071) #svm
library(mgcv) #gam

bike_day <- read.csv("data/day.csv")
bike_day$season <- as.factor(bike_day$season)
bike_day$yr <- as.factor(bike_day$yr)
bike_day$mnth <- as.factor(bike_day$mnth)
bike_day$holiday <- as.factor(bike_day$holiday)
bike_day$workingday <- as.factor(bike_day$workingday)
bike_day$weathersit <- as.factor(bike_day$weathersit)

# Selection of variables for Prediction
bike <- bike_day[, c(4,6,8,9,10,11,12,13,16)]   # w/o season & mnth
bike2 <- bike_day[, c(3,4,5,6,8,9,12,13,16)]    # w/o temp & atemp
bike3 <- bike_day[, c(3,4,6,8,9,10,12,13,16)]   # w/o mnth & atemp


#### CORRELATED NUMERICAL FEATURES ####
# Correlation Matrix
cor_data <- cor(bike[, 5:9])
cor_mat <- 
  ggcorrplot(cor_data,lab = TRUE, 
             method = "square", 
             outline.col = "white", 
             tl.cex = 12, 
             show.legend=F, 
             tl.col="black") + 
  theme_bw(base_size=12) + 
  theme(legend.position="none", 
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5, face="bold", size=15), 
        axis.text = element_text(colour = "black", face="bold", size=12),
        axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(title = "Correlation Matrix of numerical features")

cor_mat

# ML models for numerical features
numerical <- function(learner){
  ## 1.) Both temp & atemp ## 
  task <- makeRegrTask(data = bike, target = "cnt")
  set.seed(1)
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike)
  eff <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "atemp", method = "pdp", grid.size = 50)
  
  p11 <- plot(eff)+ylim(2500,6000)+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with both temp & atemp")+theme_bw()
  p12 <- plot(eff2)+ylim(2500,6000)+labs(title = " ", subtitle = " ")+theme_bw()
  
  ## 2.) Only temp ## 
  task <- makeRegrTask(data = bike[, -6], target = "cnt")
  set.seed(1)
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike[, -6])
  eff <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "hum", method = "pdp", grid.size = 50)
  
  p2 <- plot(eff)+ylim(2500,6000)+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with temp only")+theme_bw()
  
  ## 3.) Only atemp ## 
  task <- makeRegrTask(data = bike[, -5], target = "cnt")
  set.seed(1)
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike[, -5])
  eff <- FeatureEffect$new(mod, feature = "atemp", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "hum", method = "pdp", grid.size = 50)
  
  p3 <- plot(eff)+ylim(2500,6000)+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with atemp only")+theme_bw()
  
  ggarrange(p2, p3, p11, p12, ncol=2, nrow=2)
}

set.seed(123)
numerical(learner = makeLearner("regr.lm"))
numerical(learner = makeLearner("regr.svm"))
numerical(learner = makeLearner("regr.randomForest"))

#### CORRELATED CATEGORICAL FEATURES ####
# Corrected Contingency Coefficient
cont_coeff <- function(data, feature1, feature2){
  a <- as.factor(feature1)
  b <- as.factor(feature2)
  
  chisq <- chisq.test(a, b, correct = F)
  K <- sqrt(chisq$statistic/(chisq$statistic+nrow(data)))
  M <- min(length(levels(a)), length(levels(b)))
  K_korr <- K/sqrt((M-1)/M)
  K_korr
}

cont_mat <- data.frame()
for (i in 1:6){ 
  for (j in 1:6){
    cont_mat[i,j] <- round(cont_coeff(bike2, bike2[,i], bike2[,j]),2)
    colnames(cont_mat)[i] <- names(bike2)[i]
    rownames(cont_mat)[i] <- names(bike2)[i]
  }
  }

plot_cont_mat <- ggcorrplot(as.matrix(cont_mat),lab = TRUE, 
           method = "square", 
           outline.col = "white", 
           tl.cex = 12, 
           show.legend=F, 
           tl.col="black") + 
  labs(title = "Corrected Contingency Coefficients of Categorical Features")+
  theme_bw(base_size=12) + 
  theme(legend.position="none", 
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5, face="bold",size=15), 
        axis.text = element_text(colour = "black", face="bold", size=12),
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_cont_mat

# ML models for categorical features
categorical <- function(learner){
  ## 1.) Both season & mnth ## 
  task <- makeRegrTask(data = bike2, target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike2)
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "mnth", method = "pdp", grid.size = 50)
  
  p11 <- plot(eff)+ylim(c(0,6000))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with both season & mnth")+theme_bw()
  p12 <- plot(eff2)+ylim(c(0,6000))+labs(title = " ", subtitle = " ")+theme_bw()
  
  ## 2.) Only season ## 
  task <- makeRegrTask(data = bike2[,-3], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike2[, -3])
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  
  p2 <- plot(eff)+ylim(c(0,6000))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with season only")+theme_bw()
  
  ## 3.) Only mnth ## 
  task <- makeRegrTask(data = bike2[,-1], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike2[,-1])
  eff <- FeatureEffect$new(mod, feature = "mnth", method = "pdp", grid.size = 50)
  
  p3 <- plot(eff)+ylim(c(0,6000))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with mnth only")+theme_bw()
  
  ggarrange(p2, p3, p11, p12, ncol=2, nrow=2)
}

set.seed(123)
categorical(learner = makeLearner("regr.lm"))
categorical(learner = makeLearner("regr.svm"))
categorical(learner = makeLearner("regr.randomForest"))

#### CORRELATED CATEGORICAL & NUMERICAL FEATURES ####
mod <- lm(temp~season, data=bike3)
cor(bike3$temp, mod$fitted.values)


### ML models ###
catnum <- function(learner){
  ## 1.) Both season & temp ## 
  task <- makeRegrTask(data = bike3, target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike3)
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  
  p11 <- plot(eff)+ylim(c(0,6500))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with both season & temp")+theme_bw()
  p12 <- plot(eff2)+ylim(c(0,6500))+labs(title = " ", subtitle = " ")+theme_bw()
  
  ## 2.) Only season ## 
  task <- makeRegrTask(data = bike3[,-6], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike3[, -6])
  eff <- FeatureEffect$new(mod, feature = "season", method = "pdp", grid.size = 50)
  
  p2 <- plot(eff)+ylim(c(0,6500))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with season only")+theme_bw()
  
  ## 3.) Only temp ## 
  task <- makeRegrTask(data = bike3[,-1], target = "cnt")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = bike3[,-1])
  eff <- FeatureEffect$new(mod, feature = "temp", method = "pdp", grid.size = 50)
  
  p3 <- plot(eff)+ylim(c(0,6500))+labs(subtitle = paste0("Learner: ", learner$name), title = "Model with temp only")+theme_bw()
  
  ggarrange(p2, p3, p11, p12, ncol=2, nrow=2)
}

set.seed(123)
catnum(learner = makeLearner("regr.lm"))
catnum(learner = makeLearner("regr.svm"))
catnum(learner = makeLearner("regr.randomForest"))
```


## Dependent Features: Simulated Data
A major disadvantage of the analysis of PDPs on the basis of real data examples is, that we cannot exclude other factors to play a role. As an example, underlying interactions could have an impact on the PDP and hide the true effect of a feature on the predicted target variable.\citep{molnar2019} In order to illustrate the isolated impact of dependent variables in the feature space, we have simulated data in different settings, which we will discuss in this chapter.  

### Simulation Settings: Numerical Features
For a start, the different settings of simulations used for our investigation shall be introduced. Just like in chapter 2, we are seperately looking at different classes of variables and different machine learning algorithms (LM, RF and SVM). PDPs for independent, correlated and dependent numerical features are computed for each of the following data generating processes (DGP), which describe the true impact of the features on $y$:  

* Setting 1: Linear Dependence:
\begin{equation}
y = x_1 + x_2 + x_3 + \varepsilon (\#eq:10)
\end{equation}

* Setting 2: Nonlinear Dependence in $x_1$:
\begin{equation}
y = \sin{( \, 3*x_1 ) \,} + x_2 + x_3 + \varepsilon (\#eq:11)
\end{equation}

* Setting 3: Missing informative feature $x_3$ 
\begin{equation}
y = x_1 + x_2 + x_3 + \varepsilon (\#eq:12)
\end{equation} with $x_3$ relevant for the DGP but unconsidered in the machine learning model.  


In the independent case, the feature variables $x_1$, $x_2$ and $x_3$ have been drawn from a gaussian distribution with $\mu = 0$, $\sigma^2 = 1$ and a correlation coefficient of $\rho_{ij} = 0$ $\forall$ $i \ne j$, $i,j \in \{1,2,3\}$.  
The correlated case is based on the same parameters for $\mu$ and  $\sigma^2$, but a correlation coefficient of $\rho_{12} = \rho_{21} = 0.90$, i.e. a relatively strong linear association between $x_1$ and $x_2$, and $\rho_{ij} = 0$ otherwise.  
The dependent case describes the event of perfect multicollinearity, where $x_2$ is a duplicate of $x_1$, based on the data generated in the independent case.  
The target variable $y$ results from the respective DGP with an error term $\varepsilon \sim N(0, \sigma^2_\varepsilon)$ and $\sigma^2_\varepsilon$ depending on the feature values.  

One source of variation in the PDPs is the simulation of the data itself. For this reason, the process has been repeated 20 times for each analysis and the resulting PDP curves are shown as gray lines in the plots below. The thicker, black line represents the average partial dependence curve over these 20 simulations and the error bars indicate their variation. Additionally, a red line represents the true effect of the feature for which the PDP is computed. In all cases, the simulations are based on a number of 500 observations and grid size 50.  

Since in the dependent case, $x_2$ is simply a duplicate of $x_1$, the DGP could also be written as $y = 2*x_1 + x_3 + \varepsilon$ in setting \@ref(eq:10) and \@ref(eq:12) and $y = \sin{( \, 3*x_1 ) \,} + x_1 + x_3 + \varepsilon$ in setting \@ref(eq:11). For the purpose of this analysis, we are looking at each of the three features' PDPs separately. However, in order to illustrate the aforementioned, the common effect of $x_1$ and $x_2$ on the prediction is added to the plots as dashed blue line.  

### Simulation of Setting 1: Linear Dependence
#### PDPs based on Linear Model
The results of our simulations based on the Linear Model are shown in figure \@ref(fig:Figure14):  

```{r Figure14, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 1, based on multiple simulations with Linear Model as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_14_Set1_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Across all simulations, there is hardly any variation between the PDPs based on the Linear Model. In the independent case, the PDPs for each feature adequately reflect the linear dependency structure. The effect is equivalent in each PDP, since all features have the same impact and are independent from each other. From the PDPs in the second row of figure \@ref(fig:Figure14) we see that even with a relatively strong correlation of features $x_1$ and $x_2$, the PDPs adequately reflect the linear dependency structure when predictions are computed from the Linear Model. In the event of perfect multicollinearity, the PDP for one of the dependent features ($x_2$) fails, while the corresponding PDP for the other feature ($x_1$) reflects the common effect of both. The PDP for feature $x_3$ adequately reveals the linear dependency structure.  

#### PDPs based on Random Forest
The results of our simulations based on Random Forest are shown in figure \@ref(fig:Figure15):  

```{r Figure15, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 1, based on multiple simulations with Random Forest as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_15_Set1_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Compared to the LM, there is a little more variance between the individual PDP curves produced from RF as learner. Furthermore, the Partial Dependence Plots cannot adequately reflect the linear dependency structure, particularly at the margins of the feature's distribution. Again, there is no visual differentiation between the different features in the first row of figure \@ref(fig:Figure15) due to their independence. Besides, the computation of PDPs based on Random Forest does not produce significantly worse results when two features are correlated and the relationship between all variables and $y$ is linear.  
When comparing the PDPs subject to perfect multicollinearity with those in the correlated case, a slightly increased variation in the individual PDP curves is observed. Other than in the Linear Model, the learner is not able to reveal the true common effect of $x_1$ and $x_2$.  

#### PDPs based on Support Vector Machines
The results of our simulations based on Support Vector Machines are shown in figure \@ref(fig:Figure16):  

```{r Figure16, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 1, based on multiple simulations with Support Vector Machines as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_16_Set1_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

Support Vector Machines as learning algorithms are able to reproduce the respective feature's linear effect on the prediction fairly adequate in case of independence. The accuracy decreases in the margins of the feature's distribution. With two correlated features, the interval of predicted values of both correlated features becomes smaller, while the learner produces the same 'shape' of its effect, both for $x_1$ and $x_2$. The same observation is made in the event of two identical features (dependent case), but even more evident with PDP curves increasingly deviating from the true effect. Other than in the LM, none of the PDPs for the dependent features reveals the true common effect of $x_1$ and $x_2$.  

```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mvtnorm)
library(matrixcalc)
library(tidyverse)
library(stringr)
library(mlr)
library(iml)
library(ggplot2)
library(ggpubr)

## Setting 1: Linear Dependence of all Variables ## 
#### PDP - 1 Feature #### 
pdp_function <- function(no_sim, obs, lrn = makeLearner("regr.randomForest"), grid_size=30){
  
  simulate_once <- function(obs){
    
    #### Simulate data once from normal distribution: ####
    data_once <- as.data.frame(rmvnorm(n = obs, 
                                       mean = rep(0, times = 3), 
                                       sigma = diag(1, nrow = 3))) # variance=1 -> cov(X,Y)=corr(X,Y)
    colnames(data_once) <- c("X1", "X2", "X3")
    
    ### Dataset independent features ###
    data_once_ind <- data_once
    std <- abs(mean(data_once_ind$X1 + data_once_ind$X2 + data_once_ind$X3))*0.1
    data_once_ind$Y <- data_once_ind$X1 + data_once_ind$X2 + data_once_ind$X3 + rnorm(n = obs, mean = 0, sd = std)

    ### Dataset dependent features ###
    data_once_dep <- data_once
    data_once_dep$X2 <- data_once_dep$X1
    std <- abs(mean(data_once_dep$X1 + data_once_dep$X2 + data_once_dep$X3))*0.1
    data_once_dep$Y <- data_once_dep$X1 + data_once_dep$X2 + data_once_dep$X3 + rnorm(n = obs, mean = 0, sd = std)

    ### Dataset correlated features ###
    sigma <- diag(1, nrow = 3)
    sigma[1,2] <- sigma[2,1] <- 0.9
    if(is.positive.semi.definite(sigma) == FALSE) 
      warning("Covariance Matrix is not positive semidefinite!")
    data_once_cor <- as.data.frame(rmvnorm(n = obs, 
                                       mean = rep(0, times = 3), 
                                       sigma = sigma))
    colnames(data_once_cor) <- c("X1", "X2", "X3")
    std <- abs(mean(data_once_cor$X1 + data_once_cor$X2 + data_once_cor$X3))*0.1
    data_once_cor$Y <- data_once_cor$X1 + data_once_cor$X2 + data_once_cor$X3 + rnorm(n = obs, mean = 0, sd = std)

    list(data_once_ind, data_once_dep, data_once_cor)
  }
  
  simulate_repeated <- function(no_sim, obs){

    replicate_list <- replicate(n = no_sim, simulate_once(obs=obs))
    
    # Dataset independent features - repeated simulation
    data_replicated_ind <- data.frame(c(rep(0, obs)))
    l1 <- seq(from = 1, by=3, length = no_sim)
    for (j in l1){
      for (i in 1:4){
        data_replicated_ind[,ncol(data_replicated_ind)+1] <- c(replicate_list[[j]][i])}}
    data_replicated_ind <- data_replicated_ind[, -1]
    colnames(data_replicated_ind) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    # Dataset dependent features - repeated simulation
    data_replicated_dep <- data.frame(c(rep(0, obs)))
    l2 <- seq(from = 2, by=3, length = no_sim)
    for (k in l2){
      for (l in 1:4){
        data_replicated_dep[,ncol(data_replicated_dep)+1] <- c(replicate_list[[k]][l])}}
    data_replicated_dep <- data_replicated_dep[, -1]
    colnames(data_replicated_dep) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    # Dataset correlated features - repeated simulation
    data_replicated_cor <- data.frame(c(rep(0, obs)))
    l3 <- seq(from = 3, by=3, length = no_sim)
    for (k in l3){
      for (l in 1:4){
        data_replicated_cor[,ncol(data_replicated_cor)+1] <- c(replicate_list[[k]][l])}}
    data_replicated_cor <- data_replicated_cor[, -1]
    colnames(data_replicated_cor) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    list(data_replicated_ind, data_replicated_dep, data_replicated_cor)
  }
  
  independent <- simulate_repeated(no_sim, obs)[[1]]
  dependent <- simulate_repeated(no_sim, obs)[[2]]
  correlated <- simulate_repeated(no_sim, obs)[[3]] 
  
  #### Function to produce predictions based on repeated simulations ####
  prediction <- function(dataset){
    
    # Convert simulated datasets to long-format
    data <- dataset %>% 
      gather("key", "Value") %>% 
      mutate(Variable = str_sub(key, 1, 2),
             Simulation = str_sub(key, -2, -1)) %>% 
      select(Value:Simulation)
    data$Variable = sub("_$", "", data$Variable)
    data$Simulation = sub("_", "", data$Simulation)
    
    # Prepare Data for Prediction
    set <- data.frame()
    set[1:obs, 1:4] <- data[data$Simulation == 1,] %>% 
      mutate(Index = rep(1:obs, 4)) %>% 
      spread(key = Variable, value = Value) %>% 
      select(X1:Y)
    for (i in 1:no_sim){
      set[((i-1)*obs+1):(i*obs), 1:4] <- data[data$Simulation == i,] %>% 
        mutate(Index = rep(1:obs, 4)) %>% 
        spread(key = Variable, value = Value) %>% 
        select(X1:Y)}
    
    ### Derive prediction for each simulated sample ###
    multi_sim_X1 <- data.frame()
    multi_sim_X2 <- data.frame()
    multi_sim_X3 <- data.frame()
    

    for (j in 1:no_sim){
      task <- makeRegrTask(data=set[((j-1)*obs+1):(j*obs), ], target="Y")
      trained <- train(learner = lrn, task = task)
      pred <- Predictor$new(trained, data = set[((j-1)*obs+1):(j*obs), ])
      mod1 <- FeatureEffect$new(pred, feature = "X1", method = "pdp", grid.size = grid_size)
      multi_sim_X1[((j-1)*grid_size+1):(j*grid_size),1] <- mod1$results[,1]
      multi_sim_X1[((j-1)*grid_size+1):(j*grid_size),2] <- mod1$results[,2]
      multi_sim_X1$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j 

      mod2 <- FeatureEffect$new(pred, feature = "X2", method = "pdp", grid.size = grid_size)
      multi_sim_X2[((j-1)*grid_size+1):(j*grid_size),1] <- mod2$results[,1]
      multi_sim_X2[((j-1)*grid_size+1):(j*grid_size),2] <- mod2$results[,2]
      multi_sim_X2$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j 
      
      mod3 <- FeatureEffect$new(pred, feature = "X3", method = "pdp", grid.size = grid_size)
      multi_sim_X3[((j-1)*grid_size+1):(j*grid_size),1] <- mod3$results[,1]
      multi_sim_X3[((j-1)*grid_size+1):(j*grid_size),2] <- mod3$results[,2]
      multi_sim_X3$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j }
      
      colnames(multi_sim_X1) <- c("X1", "Y_hat", "Simulation")
      colnames(multi_sim_X2) <- c("X2", "Y_hat", "Simulation")
      colnames(multi_sim_X3) <- c("X3", "Y_hat", "Simulation")
      multi_sim_X1$Simulation <- factor(multi_sim_X1$Simulation)
      multi_sim_X2$Simulation <- factor(multi_sim_X2$Simulation)
      multi_sim_X3$Simulation <- factor(multi_sim_X3$Simulation)
      
    list(multi_sim_X1, multi_sim_X2, multi_sim_X3)
  }

  ind <- prediction(independent)
  dep <- prediction(dependent)
  cor <- prediction(correlated)

  #### Function to add an "overall grid" to produce mean PDP and error bars #### 
  add_avg_grid <- function(input, element){
    pdp <- as.data.frame(input[[element]])
    pdp$Grid <- rep(0, nrow(pdp))
    for (j in 1:grid_size){
      for (i in 1:nrow(pdp)){
        abst <- (max(pdp[,1])-min(pdp[,1]))/grid_size
        if (min(pdp[,1])+(j-1)*abst <= pdp[,1][i] & pdp[,1][i] < min(pdp[,1])+j*abst) {
          pdp$Grid[i] <- min(pdp[,1])+(j-0.5)*abst
          pdp$Grid[pdp[,1]==max(pdp[,1])] <- max(pdp[,1])-0.5*abst
        }
      }
    }
    pdp
  }
  
  pdp_ind_X1 <- add_avg_grid(input=ind, element=1)
  pdp_ind_X2 <- add_avg_grid(input=ind, element=2)
  pdp_ind_X3 <- add_avg_grid(input=ind, element=3)
  
  pdp_cor_X1 <- add_avg_grid(input=cor, element=1)
  pdp_cor_X2 <- add_avg_grid(input=cor, element=2)
  pdp_cor_X3 <- add_avg_grid(input=cor, element=3)
  
  pdp_dep_X1 <- add_avg_grid(input=dep, element=1)
  pdp_dep_X2 <- add_avg_grid(input=dep, element=2)
  pdp_dep_X3 <- add_avg_grid(input=dep, element=3)
  
  #### Function to derive mean and error bars per grid interval ####
  add_mean_sd <- function(data){
    average <- data %>% 
      group_by(Grid) %>% 
      summarise(mean = mean(Y_hat), 
                sd = sd(Y_hat))
    average
  }
  
  avg_ind_X1 <- add_mean_sd(pdp_ind_X1)
  avg_ind_X2 <- add_mean_sd(pdp_ind_X2)
  avg_ind_X3 <- add_mean_sd(pdp_ind_X3)
  
  avg_cor_X1 <- add_mean_sd(pdp_cor_X1)
  avg_cor_X2 <- add_mean_sd(pdp_cor_X2)
  avg_cor_X3 <- add_mean_sd(pdp_cor_X3)
  
  avg_dep_X1 <- add_mean_sd(pdp_dep_X1)
  avg_dep_X2 <- add_mean_sd(pdp_dep_X2)
  avg_dep_X3 <- add_mean_sd(pdp_dep_X3)
  
  
  #### Function to produce PDP ####
  pdp_plot <- function(pdp_data, avg_data, feature){
    ggplot(data.frame(x = c(-3, 3)), aes(x = x))+
    geom_line(data = pdp_data, aes(x = pdp_data[,1], y = Y_hat, group = Simulation), alpha = 0.3)+
    geom_line(data = avg_data, aes(x = Grid, y = mean), col="black", size = 1.2)+
    geom_errorbar(data = avg_data, aes(x=Grid, ymin = mean-sd, ymax=mean+sd), col = "black")+
    theme_bw()+
    theme(plot.title = element_text(size=16, hjust = 0))+
    geom_abline(slope = 1, col = "red")+
    xlab(feature)+
    labs(title = "")+
    ylab("")+
    xlim(c(-3,3))+
    ylim(c(-3,3))
  }
  
  p11 <- pdp_plot(pdp_ind_X1, avg_ind_X1, feature = "X1")
  p12 <- pdp_plot(pdp_ind_X2, avg_ind_X2, feature = "X2")
  p13 <- pdp_plot(pdp_ind_X3, avg_ind_X3, feature = "X3")
  p21 <- pdp_plot(pdp_cor_X1, avg_cor_X1, feature = "X1")
  p22 <- pdp_plot(pdp_cor_X2, avg_cor_X2, feature = "X2")
  p23 <- pdp_plot(pdp_cor_X3, avg_cor_X3, feature = "X3")
  p31 <- pdp_plot(pdp_dep_X1, avg_dep_X1, feature = "X1") + 
    stat_function(fun = function(x) 2*x, geom = "line", col="blue", size=1.2, linetype = "dashed", alpha = 0.5)
  p32 <- pdp_plot(pdp_dep_X2, avg_dep_X2, feature = "X2") + 
    stat_function(fun = function(x) 2*x, geom = "line", col="blue", size=1.2, linetype = "dashed", alpha = 0.5)
  p33 <- pdp_plot(pdp_dep_X3, avg_dep_X3, feature = "X3")
  
  ga_ind <- ggarrange(p11+labs(title = "PDPs for independent Features"), 
                      p12, 
                      p13,
                      nrow = 1, ncol = 3)
  
  ga_cor <- ggarrange(p21+labs(title = "PDPs for correlated Features"), 
                      p22, 
                      p23, 
                      nrow = 1, ncol = 3)

  ga_dep <- ggarrange(p31+labs(title = "PDPs for dependent Features"), 
                      p32, 
                      p33, 
                      nrow = 1, ncol = 3)
  
  list(ga_ind, ga_cor, ga_dep)
}

set.seed(123)
out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.lm"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)
out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.randomForest"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)
out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.svm"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)

```


### Simulation of Setting 2: Nonlinear Dependence
In simulation setting 2 we are looking at a DGP with a nonlinear relationship of $x_1$ and $y$ and a linear impact of $x_2$ and $x_3$. Due to the nonlinearity in one of the features, it is clear that the LM would not deliver accurate results. For this reason, in this chapter we will restrict our analysis to RF and SVM.

#### PDPs based on Random Forest
The results of our simulations based on Random Forest are shown in figure \@ref(fig:Figure17):  

```{r Figure17, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 2, based on multiple simulations with Random Forest as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_17_Set2_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

From the PDP of feature $x_1$ in the first row of figure \@ref(fig:Figure17) it is evident that Random Forest as a learner can retrace the nonlinear effect of the variable quite well, except for the margin areas of the feature distribution. The PDPs for feature $x_2$ and $x_3$ are equivalent to those in simulation setting \@ref(eq:10).  
With a simulated correlation between features $x_1$ and $x_2$ and a nonlinear relationship of $x_1$ and $y$, the ability of the respective PDPs to illustrate the feature's effect degrades with RF as learner. Both the nonlinear effect of $x_1$ and the linear effect of $x_2$ are distorted in the PDPs.  
In the event of perfect multicollinearity, the PDPs for the involved feature variables fail even more. In contrast to the correlated case, we can observe that both curves take on a similar shape, which very roughly approximates the common effect.  

#### PDPs based on Support Vector Machines
The results of our simulations based on SVM are shown in figure \@ref(fig:Figure18):  

```{r Figure18, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 2, based on multiple simulations with SVM as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_18_Set2_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

The findings derived from PDPs based on Random Forest are equivalently applicable to Support Vector Machines as machine learning algorithm. In the event of independent features, the PDPs can fairly well reveal the true feature effects, despite in the margins of the feature distrubutions. With strongly correlated or even dependent features, this ability vanishes and the PDPs of the affected features transform towards the variables' common effect. 


```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mvtnorm)
library(matrixcalc)
library(tidyverse)
library(stringr)
library(mlr)
library(iml)
library(ggplot2)
library(ggpubr)

## Setting 2: Nonlinear Dependence of X1 ## 

#### PDP - 1 Feature #### 
pdp_function <- function(no_sim, obs, lrn = makeLearner("regr.randomForest"), grid_size=30){
  
  simulate_once <- function(obs){
    
    #### Simulate data once from normal distribution: ####
    data_once <- as.data.frame(rmvnorm(n = obs, 
                                       mean = rep(0, times = 3), 
                                       sigma = diag(1, nrow = 3))) # variance=1 -> cov(X,Y)=corr(X,Y)
    colnames(data_once) <- c("X1", "X2", "X3")
    
    ### Dataset independent features ###
    data_once_ind <- data_once
    std <- abs(mean(sin(3*data_once_ind$X1) + data_once_ind$X2 + data_once_ind$X3))*0.1
    data_once_ind$Y <- sin(3*data_once_ind$X1) + data_once_ind$X2 + data_once_ind$X3 + rnorm(n = obs, mean = 0, sd = std)

    ### Dataset dependent features ###
    data_once_dep <- data_once
    data_once_dep$X2 <- data_once_dep$X1 # X2 as duplicate of X1
    std <- abs(mean(sin(3*data_once_dep$X1) + data_once_dep$X2 + data_once_dep$X3))*0.1
    data_once_dep$Y <- sin(3*data_once_dep$X1) + data_once_dep$X2 + data_once_dep$X3 + rnorm(n = obs, mean = 0, sd = std)

    ### Dataset correlated features ###
    sigma <- diag(1, nrow = 3)
    sigma[1,2] <- sigma[2,1] <- 0.9
    if(is.positive.semi.definite(sigma) == FALSE) 
      warning("Covariance Matrix is not positive semidefinite!")
    data_once_cor <- as.data.frame(rmvnorm(n = obs, 
                                       mean = rep(0, times = 3), 
                                       sigma = sigma))
    colnames(data_once_cor) <- c("X1", "X2", "X3")
    std <- abs(mean(sin(3*data_once_cor$X1) + data_once_cor$X2 + data_once_cor$X3))*0.1
    data_once_cor$Y <- sin(3*data_once_cor$X1) + data_once_cor$X2 + data_once_cor$X3 + rnorm(n = obs, mean = 0, sd = std)

    list(data_once_ind, data_once_dep, data_once_cor)
  }
  
  simulate_repeated <- function(no_sim, obs){

    replicate_list <- replicate(n = no_sim, simulate_once(obs=obs))
    
    # Dataset independent features - repeated simulation
    data_replicated_ind <- data.frame(c(rep(0, obs)))
    l1 <- seq(from = 1, by=3, length = no_sim)
    for (j in l1){
      for (i in 1:4){
        data_replicated_ind[,ncol(data_replicated_ind)+1] <- c(replicate_list[[j]][i])}}
    data_replicated_ind <- data_replicated_ind[, -1]
    colnames(data_replicated_ind) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    # Dataset dependent features - repeated simulation
    data_replicated_dep <- data.frame(c(rep(0, obs)))
    l2 <- seq(from = 2, by=3, length = no_sim)
    for (k in l2){
      for (l in 1:4){
        data_replicated_dep[,ncol(data_replicated_dep)+1] <- c(replicate_list[[k]][l])}}
    data_replicated_dep <- data_replicated_dep[, -1]
    colnames(data_replicated_dep) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    # Dataset correlated features - repeated simulation
    data_replicated_cor <- data.frame(c(rep(0, obs)))
    l3 <- seq(from = 3, by=3, length = no_sim)
    for (k in l3){
      for (l in 1:4){
        data_replicated_cor[,ncol(data_replicated_cor)+1] <- c(replicate_list[[k]][l])}}
    data_replicated_cor <- data_replicated_cor[, -1]
    colnames(data_replicated_cor) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    list(data_replicated_ind, data_replicated_dep, data_replicated_cor)
  }
  
  independent <- simulate_repeated(no_sim, obs)[[1]]
  dependent <- simulate_repeated(no_sim, obs)[[2]]
  correlated <- simulate_repeated(no_sim, obs)[[3]] 
  
  #### Function to produce predictions based on repeated simulations ####
  prediction <- function(dataset){
    
    # Convert simulated datasets to long-format
    data <- dataset %>% 
      gather("key", "Value") %>% 
      mutate(Variable = str_sub(key, 1, 2),
             Simulation = str_sub(key, -2, -1)) %>% 
      select(Value:Simulation)
    data$Variable = sub("_$", "", data$Variable)
    data$Simulation = sub("_", "", data$Simulation)
    
    # Prepare Data for Prediction
    set <- data.frame()
    set[1:obs, 1:4] <- data[data$Simulation == 1,] %>% 
      mutate(Index = rep(1:obs, 4)) %>% 
      spread(key = Variable, value = Value) %>% 
      select(X1:Y)
    for (i in 1:no_sim){
      set[((i-1)*obs+1):(i*obs), 1:4] <- data[data$Simulation == i,] %>% 
        mutate(Index = rep(1:obs, 4)) %>% 
        spread(key = Variable, value = Value) %>% 
        select(X1:Y)}
    
    ### Derive prediction for each simulated sample ###
    multi_sim_X1 <- data.frame()
    multi_sim_X2 <- data.frame()
    multi_sim_X3 <- data.frame()
    

    for (j in 1:no_sim){
      task <- makeRegrTask(data=set[((j-1)*obs+1):(j*obs), ], target="Y")
      trained <- train(learner = lrn, task = task)
      pred <- Predictor$new(trained, data = set[((j-1)*obs+1):(j*obs), ])
      mod1 <- FeatureEffect$new(pred, feature = "X1", method = "pdp", grid.size = grid_size)
      multi_sim_X1[((j-1)*grid_size+1):(j*grid_size),1] <- mod1$results[,1]
      multi_sim_X1[((j-1)*grid_size+1):(j*grid_size),2] <- mod1$results[,2]
      multi_sim_X1$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j 

      mod2 <- FeatureEffect$new(pred, feature = "X2", method = "pdp", grid.size = grid_size)
      multi_sim_X2[((j-1)*grid_size+1):(j*grid_size),1] <- mod2$results[,1]
      multi_sim_X2[((j-1)*grid_size+1):(j*grid_size),2] <- mod2$results[,2]
      multi_sim_X2$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j 
      
      mod3 <- FeatureEffect$new(pred, feature = "X3", method = "pdp", grid.size = grid_size)
      multi_sim_X3[((j-1)*grid_size+1):(j*grid_size),1] <- mod3$results[,1]
      multi_sim_X3[((j-1)*grid_size+1):(j*grid_size),2] <- mod3$results[,2]
      multi_sim_X3$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j }
      
      colnames(multi_sim_X1) <- c("X1", "Y_hat", "Simulation")
      colnames(multi_sim_X2) <- c("X2", "Y_hat", "Simulation")
      colnames(multi_sim_X3) <- c("X3", "Y_hat", "Simulation")
      multi_sim_X1$Simulation <- factor(multi_sim_X1$Simulation)
      multi_sim_X2$Simulation <- factor(multi_sim_X2$Simulation)
      multi_sim_X3$Simulation <- factor(multi_sim_X3$Simulation)
      
    list(multi_sim_X1, multi_sim_X2, multi_sim_X3)
  }

  ind <- prediction(independent)
  dep <- prediction(dependent)
  cor <- prediction(correlated)

  #### Function to add an "overall grid" to produce mean PDP and error bars #### 
  add_avg_grid <- function(input, element){
    pdp <- as.data.frame(input[[element]])
    pdp$Grid <- rep(0, nrow(pdp))
    for (j in 1:grid_size){
      for (i in 1:nrow(pdp)){
        abst <- (max(pdp[,1])-min(pdp[,1]))/grid_size
        if (min(pdp[,1])+(j-1)*abst <= pdp[,1][i] & pdp[,1][i] < min(pdp[,1])+j*abst) {
          pdp$Grid[i] <- min(pdp[,1])+(j-0.5)*abst
          pdp$Grid[pdp[,1]==max(pdp[,1])] <- max(pdp[,1])-0.5*abst
        }
      }
    }
    pdp
  }
  
  pdp_ind_X1 <- add_avg_grid(input=ind, element=1)
  pdp_ind_X2 <- add_avg_grid(input=ind, element=2)
  pdp_ind_X3 <- add_avg_grid(input=ind, element=3)
  
  pdp_cor_X1 <- add_avg_grid(input=cor, element=1)
  pdp_cor_X2 <- add_avg_grid(input=cor, element=2)
  pdp_cor_X3 <- add_avg_grid(input=cor, element=3)
  
  pdp_dep_X1 <- add_avg_grid(input=dep, element=1)
  pdp_dep_X2 <- add_avg_grid(input=dep, element=2)
  pdp_dep_X3 <- add_avg_grid(input=dep, element=3)
  
  #### Function to derive mean and error bars per grid interval ####
  add_mean_sd <- function(data){
    average <- data %>% 
      group_by(Grid) %>% 
      summarise(mean = mean(Y_hat), 
                sd = sd(Y_hat))
    average
  }
  
  avg_ind_X1 <- add_mean_sd(pdp_ind_X1)
  avg_ind_X2 <- add_mean_sd(pdp_ind_X2)
  avg_ind_X3 <- add_mean_sd(pdp_ind_X3)
  
  avg_cor_X1 <- add_mean_sd(pdp_cor_X1)
  avg_cor_X2 <- add_mean_sd(pdp_cor_X2)
  avg_cor_X3 <- add_mean_sd(pdp_cor_X3)
  
  avg_dep_X1 <- add_mean_sd(pdp_dep_X1)
  avg_dep_X2 <- add_mean_sd(pdp_dep_X2)
  avg_dep_X3 <- add_mean_sd(pdp_dep_X3)
  
  
  #### Function to produce PDP ####
  pdp_plot <- function(pdp_data, avg_data, feature){
    ggplot(data.frame(x = c(-3, 3)), aes(x = x))+
    geom_line(data = pdp_data, aes(x = pdp_data[,1], y = Y_hat, group = Simulation), alpha = 0.3)+
    geom_line(data = avg_data, aes(x = Grid, y = mean), col="black", size = 1.2)+
    geom_errorbar(data = avg_data, aes(x=Grid, ymin = mean-sd, ymax=mean+sd), col = "black")+
    theme_bw()+
    theme(plot.title = element_text(size=16, hjust = 0))+
    xlab(feature)+
    labs(title = "")+
    ylab("")+
    xlim(c(-3,3))+
    ylim(c(-3,3))
  }
  
  # Plots Independent Case
  p11 <- pdp_plot(pdp_ind_X1, avg_ind_X1, feature = "X1") +
         stat_function(fun = function(x) sin(3*x), geom = "line", col="red", size=1)
  p12 <- pdp_plot(pdp_ind_X2, avg_ind_X2, feature = "X2") + geom_abline(slope = 1, col = "red")
  p13 <- pdp_plot(pdp_ind_X3, avg_ind_X3, feature = "X3") + geom_abline(slope = 1, col = "red")
  
  # Plots Correlated Case
  p21 <- pdp_plot(pdp_cor_X1, avg_cor_X1, feature = "X1") +
         stat_function(fun = function(x) sin(3*x), geom = "line", col="red", size=1)
  p22 <- pdp_plot(pdp_cor_X2, avg_cor_X2, feature = "X2") + geom_abline(slope = 1, col = "red")
  p23 <- pdp_plot(pdp_cor_X3, avg_cor_X3, feature = "X3") + geom_abline(slope = 1, col = "red")
  
  # Plots Dependent Case
  p31 <- pdp_plot(pdp_dep_X1, avg_dep_X1, feature = "X1") +
         stat_function(fun = function(x) sin(3*x), geom = "line", col="red", size=1) +
         stat_function(fun = function(x) x+sin(3*x), 
                         geom = "line", col="blue", size=1.2, linetype = "dashed", alpha = 0.5)
  p32 <- pdp_plot(pdp_dep_X2, avg_dep_X2, feature = "X2") + geom_abline(slope = 1, col = "red") +
         stat_function(fun = function(x) x+sin(3*x), 
                         geom = "line", col="blue", size=1.2, linetype = "dashed", alpha = 0.5)
  p33 <- pdp_plot(pdp_dep_X3, avg_dep_X3, feature = "X3") + geom_abline(slope = 1, col = "red")
  
  ga_ind <- ggarrange(p11+labs(title = "PDPs for independent Features"), 
                      p12, 
                      p13,
                      nrow = 1, ncol = 3)
  
  ga_cor <- ggarrange(p21+labs(title = "PDPs for correlated Features"), 
                      p22, 
                      p23, 
                      nrow = 1, ncol = 3)

  ga_dep <- ggarrange(p31+labs(title = "PDPs for dependent Features"), 
                      p32, 
                      p33, 
                      nrow = 1, ncol = 3)
  
  list(ga_ind, ga_cor, ga_dep)
}

set.seed(123)
out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.randomForest"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)

out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.svm"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)
```

### Simulation of Setting 3: Missing informative feature $x_3$
In simulation setting 3, we assume that there are three variables with an impact on the data generating process of $y$. In the training process of the machine learning model, only two of those are considered. Consequently, when looking at the PDPs, we only compare the independent, the correlated and the dependent case for $x_1$ and $x_2$ respectively.  

#### PDPs based on Linear Model
```{r Figure19, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$ and $x_2$ (left to right) in Setting 3, based on multiple simulations with LM as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_19_Set3_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

Compared the PDPs of independent features the Linear Model produced in setting \@ref(eq:10), the variation in individual PDPs is slightly higher with missing information from $x_3$. Overall, the learner can adequately reflect the linear feature effects of $x_1$ and $x_2$.  
The increase in variablility between the individual PDPs is even more evident in the correlated case. On average, we still obtain the true linear effect of the correlated features, but there are some individual curves which do indicate a steeper or more moderate slope.  
The PDPs drawn on basis of the Linear Model and dependent features indicate that for both individual features, the PDP consistently provides false effects on the predicted outcome. While both effects are actually linear with a slope of 1, the PDP for $x_1$ shows a steeper increase and $x_2$ fails completely. Nonetheless, the PDP for $x_1$ does reflect the common effect of both variables together.

#### PDPs based on Random Forest
```{r Figure20, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$ and $x_2$ (left to right) in Setting 3, based on multiple simulations with RF as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_20_Set3_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

Compared to setting \@ref(eq:10), where all relevant feature variables were taken into account for the trainig of the model, the variation in PDP curves in setting \@ref(eq:12) is larger. Between features $x_1$ and $x_2$, which are independent, there is no systematic difference traceable from the PDPs.  
Other than an increased variability between the individual PDP curves and a slightly tighter prediction interval, with correlated features and Random Forest as learner, there is no apparent deviation to the PDPs of independent features.  
In accordance with the observations made in setting \@ref(eq:10), the interval of predicted values for dependent features become even smaller while the PDP curves further deviate from the true effect. Neither the individual effect of each feature, nor their common effect are illustrated adequately.  

#### PDPs for Support Vector Machines
```{r Figure21, echo=FALSE, out.width='100%', fig.cap="PDPs for features $x_1$ and $x_2$ (left to right) in Setting 3, based on multiple simulations with SVM as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_21_Set3_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

Similar to learning based on Random Forest, SVM with missing feature variable $x_3$ produces a higher variability between the simulated PDP curves. The margin areas, where the PDPs cannot adequately reflect the linear dependence, are broader than in setting \@ref(eq:10).  
In the event of the two remaining features $x_1$ and $x_2$ being strongly correlated, the issue of larger variability between the individual simulations aggravates and the ability to reveal the linear effect ceases.  
With a perfect multicollinearity of $x_1$ and $x_2$, the variablity of the individual PDP curves becomes smaller, but at the same time the models' ability to uncover the true linear effect vanishes. The interval of predicted values is remarkably smaller than in the independent case.  

```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mvtnorm)
library(matrixcalc)
library(tidyverse)
library(stringr)
library(mlr)
library(iml)
library(ggplot2)
library(ggpubr)

## Setting 3: ML model with missing informative variable X3 ##

#### PDP - 1 Feature #### 
pdp_function <- function(no_sim, obs, lrn = makeLearner("regr.randomForest"), grid_size=30){
  
  simulate_once <- function(obs){
    
    #### Simulate data once from normal distribution: ####
    data_once <- as.data.frame(rmvnorm(n = obs, 
                                       mean = rep(0, times = 3), 
                                       sigma = diag(1, nrow = 3)))
    colnames(data_once) <- c("X1", "X2", "X3")
    
    ### Dataset independent features ###
    data_once_ind <- data_once
    std <- abs(mean(data_once_ind$X1 + data_once_ind$X2 + data_once_ind$X3))*0.1
    data_once_ind$Y <- data_once_ind$X1 + data_once_ind$X2 + data_once_ind$X3 + rnorm(n = obs, mean = 0, sd = std)

    ### Dataset dependent features ###
    data_once_dep <- data_once
    data_once_dep$X2 <- data_once_dep$X1
    std <- abs(mean(data_once_dep$X1 + data_once_dep$X2 + data_once_dep$X3))*0.1
    data_once_dep$Y <- data_once_dep$X1 + data_once_dep$X2 + data_once_dep$X3 + rnorm(n = obs, mean = 0, sd = std)

    ### Dataset correlated features ###
    sigma <- diag(1, nrow = 3)
    sigma[1,2] <- sigma[2,1] <- 0.9
    if(is.positive.semi.definite(sigma) == FALSE) 
      warning("Covariance Matrix is not positive semidefinite!")
    data_once_cor <- as.data.frame(rmvnorm(n = obs, 
                                       mean = rep(0, times = 3), 
                                       sigma = sigma))
    colnames(data_once_cor) <- c("X1", "X2", "X3")
    std <- abs(mean(data_once_cor$X1 + data_once_cor$X2 + data_once_cor$X3))*0.1
    data_once_cor$Y <- data_once_cor$X1 + data_once_cor$X2 + data_once_cor$X3 + rnorm(n = obs, mean = 0, sd = std)

    list(data_once_ind, data_once_dep, data_once_cor)
  }
  
  simulate_repeated <- function(no_sim, obs){

    replicate_list <- replicate(n = no_sim, simulate_once(obs=obs))
    
    # Dataset independent features - repeated simulation
    data_replicated_ind <- data.frame(c(rep(0, obs)))
    l1 <- seq(from = 1, by=3, length = no_sim)
    for (j in l1){
      for (i in 1:4){
        data_replicated_ind[,ncol(data_replicated_ind)+1] <- c(replicate_list[[j]][i])}}
    data_replicated_ind <- data_replicated_ind[, -1]
    colnames(data_replicated_ind) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    # Dataset dependent features - repeated simulation
    data_replicated_dep <- data.frame(c(rep(0, obs)))
    l2 <- seq(from = 2, by=3, length = no_sim)
    for (k in l2){
      for (l in 1:4){
        data_replicated_dep[,ncol(data_replicated_dep)+1] <- c(replicate_list[[k]][l])}}
    data_replicated_dep <- data_replicated_dep[, -1]
    colnames(data_replicated_dep) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    # Dataset correlated features - repeated simulation
    data_replicated_cor <- data.frame(c(rep(0, obs)))
    l3 <- seq(from = 3, by=3, length = no_sim)
    for (k in l3){
      for (l in 1:4){
        data_replicated_cor[,ncol(data_replicated_cor)+1] <- c(replicate_list[[k]][l])}}
    data_replicated_cor <- data_replicated_cor[, -1]
    colnames(data_replicated_cor) <- c(paste0(rep(c("X1", "X2", "X3", "Y"),times=no_sim), "_", rep(c(1:no_sim), each = 4)))
    
    list(data_replicated_ind, data_replicated_dep, data_replicated_cor)
  }
  
  independent <- simulate_repeated(no_sim, obs)[[1]]
  dependent <- simulate_repeated(no_sim, obs)[[2]]
  correlated <- simulate_repeated(no_sim, obs)[[3]] 
  
  #### Function to produce predictions based on repeated simulations ####
  prediction <- function(dataset){
    
    # Convert simulated datasets to long-format
    data <- dataset %>% 
      gather("key", "Value") %>% 
      mutate(Variable = str_sub(key, 1, 2),
             Simulation = str_sub(key, -2, -1)) %>% 
      select(Value:Simulation)
    data$Variable = sub("_$", "", data$Variable)
    data$Simulation = sub("_", "", data$Simulation)
    
    
    # Prepare Data for Prediction
    set <- data.frame()
    set[1:obs, 1:4] <- data[data$Simulation == 1,] %>% 
      mutate(Index = rep(1:obs, 4)) %>% 
      spread(key = Variable, value = Value) %>% 
      select(X1:Y)
    for (i in 1:no_sim){
      set[((i-1)*obs+1):(i*obs), 1:4] <- data[data$Simulation == i,] %>% 
        mutate(Index = rep(1:obs, 4)) %>% 
        spread(key = Variable, value = Value) %>% 
        select(X1:Y)}
    set <- set[, -3]
    
    ### Derive prediction for each simulated sample ###
    multi_sim_X1 <- data.frame()
    multi_sim_X2 <- data.frame()

    for (j in 1:no_sim){
      task <- makeRegrTask(data=set[((j-1)*obs+1):(j*obs), ], target="Y")
      trained <- train(learner = lrn, task = task)
      pred <- Predictor$new(trained, data = set[((j-1)*obs+1):(j*obs), ])
      mod1 <- FeatureEffect$new(pred, feature = "X1", method = "pdp", grid.size = grid_size)
      multi_sim_X1[((j-1)*grid_size+1):(j*grid_size),1] <- mod1$results[,1]
      multi_sim_X1[((j-1)*grid_size+1):(j*grid_size),2] <- mod1$results[,2]
      multi_sim_X1$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j 

      mod2 <- FeatureEffect$new(pred, feature = "X2", method = "pdp", grid.size = grid_size)
      multi_sim_X2[((j-1)*grid_size+1):(j*grid_size),1] <- mod2$results[,1]
      multi_sim_X2[((j-1)*grid_size+1):(j*grid_size),2] <- mod2$results[,2]
      multi_sim_X2$Simulation[((j-1)*grid_size+1):(j*grid_size)] <- j 
      }
      
      colnames(multi_sim_X1) <- c("X1", "Y_hat", "Simulation")
      colnames(multi_sim_X2) <- c("X2", "Y_hat", "Simulation")
      multi_sim_X1$Simulation <- factor(multi_sim_X1$Simulation)
      multi_sim_X2$Simulation <- factor(multi_sim_X2$Simulation)

    list(multi_sim_X1, multi_sim_X2)
    }
  
  ind <- prediction(independent)
  dep <- prediction(dependent)
  cor <- prediction(correlated)

  #### Function to add an "overall grid" to produce mean PDP and error bars #### 
  add_avg_grid <- function(input, element){
    pdp <- as.data.frame(input[[element]])
    pdp$Grid <- rep(0, nrow(pdp))
    for (j in 1:grid_size){
      for (i in 1:nrow(pdp)){
        abst <- (max(pdp[,1])-min(pdp[,1]))/grid_size
        if (min(pdp[,1])+(j-1)*abst <= pdp[,1][i] & pdp[,1][i] < min(pdp[,1])+j*abst) {
          pdp$Grid[i] <- min(pdp[,1])+(j-0.5)*abst
          pdp$Grid[pdp[,1]==max(pdp[,1])] <- max(pdp[,1])-0.5*abst
        }
      }
    }
    pdp
  }
  
  pdp_ind_X1 <- add_avg_grid(input=ind, element=1)
  pdp_ind_X2 <- add_avg_grid(input=ind, element=2)

  pdp_cor_X1 <- add_avg_grid(input=cor, element=1)
  pdp_cor_X2 <- add_avg_grid(input=cor, element=2)

  pdp_dep_X1 <- add_avg_grid(input=dep, element=1)
  pdp_dep_X2 <- add_avg_grid(input=dep, element=2)

  #### Function to derive mean and error bars per grid interval ####
  add_mean_sd <- function(data){
    average <- data %>% 
      group_by(Grid) %>% 
      summarise(mean = mean(Y_hat), 
                sd = sd(Y_hat))
    average
  }
  
  avg_ind_X1 <- add_mean_sd(pdp_ind_X1)
  avg_ind_X2 <- add_mean_sd(pdp_ind_X2)

  avg_cor_X1 <- add_mean_sd(pdp_cor_X1)
  avg_cor_X2 <- add_mean_sd(pdp_cor_X2)

  avg_dep_X1 <- add_mean_sd(pdp_dep_X1)
  avg_dep_X2 <- add_mean_sd(pdp_dep_X2)

  #### Function to produce PDP ####
  pdp_plot <- function(pdp_data, avg_data, feature){
    ggplot(data.frame(x = c(-3, 3)), aes(x = x))+
    geom_line(data = pdp_data, aes(x = pdp_data[,1], y = Y_hat, group = Simulation), alpha = 0.3)+
    geom_line(data = avg_data, aes(x = Grid, y = mean), col="black", size = 1.2)+
    geom_errorbar(data = avg_data, aes(x=Grid, ymin = mean-sd, ymax=mean+sd), col = "black")+
    theme_bw()+
    theme(plot.title = element_text(size=16, hjust = 0))+
    geom_abline(slope = 1, col = "red")+
    xlab(feature)+
    labs(title = "")+
    ylab("")+
    xlim(c(-3,3))+
    ylim(c(-3,3))
  }
  
  p11 <- pdp_plot(pdp_ind_X1, avg_ind_X1, feature = "X1")
  p12 <- pdp_plot(pdp_ind_X2, avg_ind_X2, feature = "X2")
  p21 <- pdp_plot(pdp_cor_X1, avg_cor_X1, feature = "X1")
  p22 <- pdp_plot(pdp_cor_X2, avg_cor_X2, feature = "X2")
  p31 <- pdp_plot(pdp_dep_X1, avg_dep_X1, feature = "X1") + 
    stat_function(fun = function(x) 2*x, geom = "line", col="blue", size=1.2, linetype = "dashed", alpha = 0.5)
  p32 <- pdp_plot(pdp_dep_X2, avg_dep_X2, feature = "X2") + 
    stat_function(fun = function(x) 2*x, geom = "line", col="blue", size=1.2, linetype = "dashed", alpha = 0.5)

  ga_ind <- ggarrange(p11+labs(title = "PDPs for independent Features"), 
                      p12, 
                      nrow = 1, ncol = 2)
  
  ga_cor <- ggarrange(p21+labs(title = "PDPs for correlated Features"), 
                      p22, 
                      nrow = 1, ncol = 2)

  ga_dep <- ggarrange(p31+labs(title = "PDPs for dependent Features"), 
                      p32, 
                      nrow = 1, ncol = 2)
  
  list(ga_ind, ga_cor, ga_dep)
}

set.seed(123)
out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.lm"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)

out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.randomForest"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)

out <- pdp_function(no_sim=20, obs=500, lrn=makeLearner("regr.svm"), grid_size=50) 
ggarrange(out[[1]], # Plot Independent
          out[[2]], # Plot Correlated
          out[[3]], # Plot Dependent
          nrow=3)
```


### Simulation Settings: Categorical Features
In this chapter we want to investigate the impact of dependencies between two categorical and between a categorical and a numerical feature. For this purpose, we simulate data with a number of 1000 randomly drawn observations and three feature variables, where:  

* $x_1$ categorical variable $\in \{0,1\}$,
* $x_2$ categorical variable $\in \{A,B,C\}$,
* $x_3$ numerical variable with $x_3 \sim N(\mu, \sigma^2)$.

All features are characterized by their linear relationship with the target variable: $y=x_1+x_2+x_3+\varepsilon$.  

Again, in order to isolate the individual effects of two dependent features on their respective PDPs, we define three different simulation settings:

1. Independent Case: In this setting, the feature variables are drawn independently from each other, i.e. the observations are randomly sampled with the following parameters:

* $x_1: P(x_1=1)=P(x_1=0)=0.5$
* $x_2: P(x_2=A)=0.475, P(x_2=B)=0.175, P(x_2=C)=0.35)$
* $x_3: P(x_3 \sim N(1,1))=0.5, P(x_3 \sim N(20,2))=0.5$

The association between $x_1$ and $x_2$ can be measured by the corrected contingency coefficient, which is rather low with a value of 0.10. In accordance with the approach in chapter 2.?, we calculate the association between $x_1$ and $x_3$ by means of the variance-explained measure. With a value of 0.01 we take the independence assumption as confirmed.

2. Dependence between two categorical features: In this setting, $x_1$ and $x_2$ are depending on each each other, i.e. the observations are randomly sampled with the following parameters:

* $x_1: P(x_1=1)=P(x_1=0)=0.5$
* $x_2: \begin{cases} P(x_2=A)=0.90, P(x_2=B)=0.10, P(x_2=C)=0), \text{if } x_1=0, \\ P(x_2=A)=0.05, P(x_2=B)=0.25, P(x_2=C)=0.70), \text{if } x_1=1 \end{cases}$
* $x_3: P(x_3 \sim N(1,1))=0.5, P(x_3 \sim N(20,2))=0.5$

The corrected contingency coefficient of 0.94 comfirms a strong dependence between features $x_1$ and $x_2$. 

3. Dependence between categorical and numerical features: In this setting, $x_1$ and $x_3$ are depending on each each other, i.e. the observations are randomly sampled with the following parameters:

* $x_1: P(x_1=1)=P(x_1=0)=0.5$
* $x_2: P(x_2=A)=0.475, P(x_2=B)=0.175, P(x_2=C)=0.35)$
* $x_3: \begin{cases} x_3 \sim N(1,1), \text{if }  x_1=0 \\ x_3 \sim N(20,2), \text{if } x_1=1 \end{cases}$

With a value of 0.986, the variance-explained measure indicates a substantial degree of dependence between $x_1$ and $x_3$. 

#### PDPs based on Linear Model
Figure \@ref(fig:Figure22) shows the PDPs for all feature variables and all simulation settings based on the Linear Model. 

```{r Figure22, echo=FALSE, out.width='100%', fig.cap="PDPs for categorical features $x_1$, $x_2$ and numerical feature $x_3$ (left to right), based on simulated data and LM as learning algorithm. Top row shows independent case, second row the case of two dependent categorical features and the bottom row the case of a numerical feature depending on a categorical feature.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_22_Set4_LM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

Apparently, the Linear Model is robust against our simulated dependencies, since the PDPs of the correlated and dependent features do not differ significantly from those of independent features. 

#### PDPs based on Random Forest
FIGURE 23  
Figure \@ref(fig:Figure23) shows the PDPs for all feature variables and all simulation settings based on Random Forest. 

```{r Figure23, echo=FALSE, out.width='100%', fig.cap="PDPs for categorical features $x_1$, $x_2$ and numerical feature $x_3$ (left to right), based on simulated data and RF as learning algorithm. Top row shows independent case, second row the case of two dependent categorical features and the bottom row the case of a numerical feature depending on a categorical feature.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_23_Set4_RF.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

Based on Random Forest, the partial dependence function seems to be impacted much stronger by our simulated dependencies, since the PDPs for dependent variables indicate feature effects which differ from those in the independent case. This is particularly true for a strong association between a categorical an a numerical variable (bottom row of figure \@ref(fig:Figure23)).

#### PDPs based on Support Vector Machines
Figure \@ref(fig:Figure24) shows the PDPs for all feature variables and all simulation settings based on Support Vector Machines. 

```{r Figure24, echo=FALSE, out.width='100%', fig.cap="PDPs for categorical features $x_1$, $x_2$ and numerical feature $x_3$ (left to right), based on simulated data and RF as learning algorithm. Top row shows independent case, second row the case of two dependent categorical features and the bottom row the case of a numerical feature depending on a categorical feature.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_24_Set4_SVM.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

In accordancy with our findings based on the Linear Model, the predicted effects based on SVM seem to be robust against our simulated dependencies, since the PDPs for the individual settings do not differ significantly from the independent case. 

```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mlr)
library(iml)
library(ggplot2)
library(ggpubr)

## Association between two Categorical Features: Corrected Contingency Coefficient ##
cont_coeff <- function(data, feature1, feature2){
  a <- as.factor(feature1)
  b <- as.factor(feature2)
  
  chisq <- chisq.test(a, b, correct = F)
  K <- sqrt(chisq$statistic/(chisq$statistic+nrow(data)))
  M <- min(length(levels(a)), length(levels(b)))
  K_korr <- K/sqrt((M-1)/M)
  K_korr
}

#### Independent Case ####
set.seed(123)
n <- 1000
ind <- data.frame(X1 = rep(0,n))

# Categorical Feature X1
ind$X1 <- sample(x = c(0,1), size = n, prob = c(0.5, 0.5), replace = TRUE)
ind$X1 <- as.factor(ind$X1)

# Categorical Feature X2 (Wahrscheinlichkeiten als Mittel der bed. Wkt. von oben):
ind$X2 <- sample(x=c("A", "B", "C"), size = n, prob = c(0.475, 0.175, 0.35), replace = TRUE)
ind$X2 <- as.factor(ind$X2)
cont_coeff(ind, ind$X1, ind$X2)

# Numerical Feature X3
for (i in 1:n){
  ind$X3[i] <- sample(x = c(rnorm(n = 1, mean = 20, sd = 2), rnorm(n = 1, mean = 1, sd = 1)), size=1, prob = c(0.5, 0.5), replace = TRUE)
}
limo <- lm(X3~X1, data=ind)
cor(ind$X3, limo$fitted.values)

#### X1 and X2 Correlated ####
set.seed(123)
n <- 1000
df <- data.frame(X1 = rep(0,n))

# Categorical Feature X1
df$X1 <- sample(x = c(0,1), size = n, prob = c(0.5, 0.5), replace = TRUE)
df$X1 <- as.factor(df$X1)

# Categorical Feature X2 (dependent)
for (i in 1:n){
  df$X2[i] <- ifelse(df$X1[i] == 0, 
                     sample(x=c("A", "B", "C"), size = n, prob = c(0.9, 0.1, 0), replace = TRUE), 
                     sample(x=c("A", "B", "C"), size = n, prob = c(0.05, 0.25, 0.7), replace = TRUE))
}
df$X2 <- as.factor(df$X2)
cont_coeff(df, df$X1, df$X2)

# Numerical Feature X3 (independent)
for (i in 1:n){
  df$X3[i] <- sample(x = c(rnorm(n = 1, mean = 20, sd = 2), rnorm(n = 1, mean = 1, sd = 1)), size=1, prob = c(0.5, 0.5), replace = TRUE)
}
limo <- lm(X3~X1, data=df)
cor(df$X3, limo$fitted.values)

#### X1 and X3 Correlated ####
set.seed(123)
n <- 1000
df2 <- data.frame(X1 = rep(0,n))

# Categorical Feature X1
df2$X1 <- sample(x = c(0,1), size = n, prob = c(0.5, 0.5), replace = TRUE)
df2$X1 <- as.factor(df2$X1)

# Categorical Feature X2 (independent):
df2$X2 <- sample(x=c("A", "B", "C"), size = n, prob = c(0.475, 0.175, 0.35), replace = TRUE)
df2$X2 <- as.factor(df2$X2)
cont_coeff(df2, df2$X1, df2$X2)

# Numerical Feature X3 (dependent)
for (i in 1:n){
  df2$X3[i] <- ifelse(df2$X1[i] == 0, 
                     rnorm(n = n, mean = 20, sd = 2), 
                     rnorm(n = n, mean = 1, sd = 1))
}
limo <- lm(X3~X1, data=df2)
cor(df2$X3, limo$fitted.values)

#### PDPs ####
pdp <- function(data, learner = makeLearner("regr.randomForest"), title = "PDPs for ?"){
  
  # Target Variable Y
  for (i in 1:n){data$X1_1[i] <- ifelse(data$X1[i] == "1", 1, 0)}
  for (i in 1:n){data$X2_A[i] <- ifelse(data$X2[i] == "A", 1, 0)}
  for (i in 1:n){data$X2_B[i] <- ifelse(data$X2[i] == "B", 1, 0)}
  std <- abs(mean(data$X1_1 + data$X2_A + data$X2_B + data$X3))*0.1
  data$Y <- data$X1_1 + data$X2_A + data$X2_B + data$X3 + rnorm(n = n, mean = 0, sd = std)
  
  task <- makeRegrTask(data = data[, c(1,2,3,7)], target = "Y")
  train <- train(learner = learner, task = task)
  mod <- Predictor$new(train, data = data[, c(1,2,3,7)])
  eff1 <- FeatureEffect$new(mod, feature = "X1", method = "pdp", grid.size = 50)
  eff2 <- FeatureEffect$new(mod, feature = "X2", method = "pdp", grid.size = 50)
  eff3 <- FeatureEffect$new(mod, feature = "X3", method = "pdp", grid.size = 50)
  
  p1 <- plot(eff1)+theme_bw()+ylim(c(0,17))+labs(title = title)
  p2 <- plot(eff2)+theme_bw()+ylim(c(0,17))+labs(title = "")
  p3 <- plot(eff3)+theme_bw()+ylim(c(5,20))+labs(title = "")
  
  ggarrange(p1, p2, p3, nrow=1, ncol=3)
}

set.seed(123)
a <- pdp(data=ind, learner = makeLearner("regr.lm"), title = "PDPs for independent features")
b <- pdp(data=df, learner = makeLearner("regr.lm"), title = "PDPs for X1 and X2 dependent")
c <- pdp(data=df2, learner = makeLearner("regr.lm"), title = "PDPs for X1 and X3 dependent")
ggarrange(a,b,c, nrow = 3)

a <- pdp(data=ind, learner = makeLearner("regr.randomForest"), title = "PDPs for independent features")
b <- pdp(data=df, learner = makeLearner("regr.randomForest"), title = "PDPs for X1 and X2 dependent")
c <- pdp(data=df2, learner = makeLearner("regr.randomForest"), title = "PDPs for X1 and X3 dependent")
ggarrange(a,b,c, nrow = 3)

a <- pdp(data=ind, learner = makeLearner("regr.svm"), title = "PDPs for independent features")
b <- pdp(data=df, learner = makeLearner("regr.svm"), title = "PDPs for X1 and X2 dependent")
c <- pdp(data=df2, learner = makeLearner("regr.svm"), title = "PDPs for X1 and X3 dependent")
ggarrange(a,b,c, nrow = 3)
```

## Extrapolation Problem: Simulation
### Simulation based on established learners
In the problem description of this chapter we announced that, in addition to the issue with dependent features, we want to investigate the extrapolation problem and its implications for the computation of Partial Dependence Plots. For this purpose, we use the dataset introduced in chapter 3.1, which was simulated once with $x_1$ and $x_2$ independent, and once with both features strongly correlated. Remember that in a next step, the observed data was manipulated by cutting out all observations with $x_1, x_2 \in [0,1.5]$, and thus artificially producing an area with no observations.  

Now we are looking at the PDPs resulting from these modifications. Figure \@ref(fig:Figure25) compares the PDP curves derived for both features based on the complete, uncorrelated dataset to its manipulated version with missing values.   

```{r Figure25, echo=FALSE, out.width='80%', fig.cap="PDPs for uncorrelated features $x_1$ (left) and $x_2$ (right) based on complete simulated dataset (top row) and based on manipulated dataset with missing observations (bottom row). The red curve represents the true effect of the feature for which the PDP is drawn, while the PDPs derived from the machine learning models are represented by curves drawn in black (Random Forest) and blue (SVM).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_25_Extrapol_uncor.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

The first row of PDPs in figure \@ref(fig:Figure25), computed on basis of the complete dataset of uncorrelated features, adequately reflects the true effects of $x_1$ and $x_2$ (red curve). In the presence of an extrapolation problem, the adequacy of the predicted effects decreases. Especially with the more complex, nonlinear effect of $x_1$, extrapolation causes a clearly visible deviation between the partial dependence curves and the true feature effect, irrespective of the learner.

In figure \@ref(fig:Figure26) we do the same comparison, but this time based on the dataset with strongly correlated features.  

```{r Figure26, echo=FALSE, out.width='80%', fig.cap="PDPs for correlated features $x_1$ (left) and $x_2$ (right) based on complete simulated dataset (top row) and based on manipulated dataset with missing observations (bottom row). The red curve represents the true effect of the feature for which the PDP is drawn, while the PDPs derived from the machine learning models are represented by curves drawn in black (Random Forest) and blue (SVM).", fig.align='center'}
knitr::include_graphics('images/VK_PDP_26_Extrapol_cor.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

From the first row of PDPs in figure \@ref(fig:Figure26), we again discover the difficulty to obtain reliable PDPs when features are dependent. The results in the bottom row of figure \@ref(fig:Figure26) are even more striking: with a combination of dependent features and extrapolation, the PDPs come up with estimated effects which are far from the true effects on the prediction. Those deviations seem to occur irrespective of the learning algorithm. 

```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mvtnorm)
library(ggplot2) 
library(ggpubr) 
library(cowplot) 
library(mlr)
library(iml)

################################# CORRELATION & EXTRAPOLATION PROBLEM ################################# 
simulate_data <- function(obs, correlation){
  sigma <- diag(1, nrow = 2)
  sigma[1,2] <- sigma[2,1] <- correlation
  data <- as.data.frame(rmvnorm(n = obs, 
                                mean = rep(0, times = 2), 
                                sigma = sigma))
  colnames(data) <- c("X1", "X2")
  std <- abs(mean(sin(data$X1) + data$X2))*0.1
  data$Y <- sin(data$X1) + data$X2 + rnorm(n = obs, mean = 0, sd = std)
  invisible(data)
}

set.seed(123)
uncorrelated <- simulate_data(obs=1000, correlation = 0)
correlated <- simulate_data(obs=1000, correlation = 0.99)

# Data Manipulation: Produce area with no observations
uncorrelated_gap <- uncorrelated
for (i in 1:nrow(uncorrelated_gap)){
  uncorrelated_gap$X1[i] <- ifelse(uncorrelated_gap$X2[i] > 0 & uncorrelated_gap$X2[i] <= 1.5 & uncorrelated_gap$X1[i] > 0 & uncorrelated_gap$X1[i] <= 1.5, NA, uncorrelated_gap$X1[i])
}
for (i in 1:nrow(uncorrelated_gap)){uncorrelated_gap$X2[i] <- ifelse(is.na(uncorrelated_gap$X1[i]), NA, uncorrelated_gap$X2[i])}
for (i in 1:nrow(uncorrelated_gap)){uncorrelated_gap$Y[i] <- ifelse(is.na(uncorrelated_gap$X1[i]), NA, uncorrelated_gap$Y[i])} 

correlated_gap <- correlated
for (i in 1:nrow(correlated_gap)){
  correlated_gap$X1[i] <- ifelse(correlated_gap$X2[i] > 0 & correlated_gap$X2[i] <= 1.5 & correlated_gap$X1[i] > 0 & correlated_gap$X1[i] <= 1.5, NA, correlated_gap$X1[i])
}
for (i in 1:nrow(correlated_gap)){correlated_gap$X2[i] <- ifelse(is.na(correlated_gap$X1[i]), NA, correlated_gap$X2[i])}
for (i in 1:nrow(correlated_gap)){correlated_gap$Y[i] <- ifelse(is.na(correlated_gap$X1[i]), NA, correlated_gap$Y[i])} 


# Visualization of extrapolation problem in machine learning models / PDPs
extr_prob <- function(data, title){
  learner1 <- makeLearner("regr.randomForest")
  learner2 <- makeLearner("regr.svm")
  task <- makeRegrTask(data = na.omit(data), target = "Y")
  train1 <- train(learner = learner1, task = task)
  train2 <- train(learner = learner2, task = task)
  
  mod1 <- Predictor$new(train1, data = na.omit(data))
  eff11 <- FeatureEffect$new(mod1, feature = "X1", method = "pdp", grid.size = 30)
  eff12 <- FeatureEffect$new(mod1, feature = "X2", method = "pdp", grid.size = 30)
  
  mod2 <- Predictor$new(train2, data = na.omit(data))
  eff21 <- FeatureEffect$new(mod2, feature = "X1", method = "pdp", grid.size = 30)
  eff22 <- FeatureEffect$new(mod2, feature = "X2", method = "pdp", grid.size = 30)
  
  ggarrange(ggplot(data.frame(x = c(-3, 3)), aes(x = x))+
              geom_line(data= eff11$results, aes(x=eff11$results[,1], y=eff11$results[,2]), col="black")+
              geom_line(data= eff21$results, aes(x=eff21$results[,1], y=eff21$results[,2]), col="blue")+
              stat_function(fun = function(x) sin(x), geom = "line", col="red")+
              labs(title = title)+
              xlab(names(eff11$results[1]))+
              ylab("Predicted Y")+
              theme(plot.title = element_text(hjust = 0)), 
            ggplot(data.frame(x = c(-3, 3)), aes(x = x))+
              geom_line(data= eff12$results, aes(x=eff12$results[,1], y=eff12$results[,2]), col="black")+
              geom_line(data= eff22$results, aes(x=eff22$results[,1], y=eff22$results[,2]), col="blue")+
              geom_abline(slope=1, col="red")+
              labs(title = "")+
              xlab(names(eff12$results[1]))+
              ylab("")+
              theme(plot.title = element_text(hjust = 0)))
}

ggarrange(extr_prob(data = uncorrelated, title = "PDPs for X1 and X2 uncorrelated"),
          extr_prob(data = uncorrelated_gap, title = "PDPs for X1 and X2 uncorrelated with extrapolation problem"), nrow=2)

ggarrange(extr_prob(data = correlated, title = "PDPs for X1 and X2 correlated"), extr_prob(data = correlated_gap, title = "PDPs for X1 and X2 correlated with extrapolation problem"), nrow=2)
```

### Simulation based on own prediction function
So far, all our analyses were based on the established learning algorithms LM, RF and SVM. We have seen that the choice of the learner does have an impact on the suitability of PDP curves. Obviously there is a countless number of other possibilities to come up with prediction functions other than the ones we have seen. PDPs are prone to fail when this prediction function is doing 'weird' stuff in areas outside the feature distribution. This can happen due to the fact that the learner minimizes the loss based on training data while there is no penalization for extrapolation.\citep{molnar2019} 

Let's illustrate the issue with an example. Assume that we want to predict the size of a potato ($\hat{y}$) by means of the share of maximum amount of soil ($x_1$) and the share of maximum amount of water ($x_2$) available during the process of growing the plant. The feature variables are dependent in the sense that when using a larger amount of soil, the farmer would also use a larger amount of water, i.e. $x_1$ and $x_2$ are positively correlated. Typically, the more ressources the farmer invests, the larger the crops. The corresponding model is basically a simple linear regression which adds up the two components.  

In the event of improper planting, meaning the usage of a too large amount of water in proportion to the soil (and vice versa), the plant would die and the result would be a potato of size 0. This is exactly what our self-constructed prediction function predicts. Luckily, all farmers in our dataset know how to grow potatoes, therefore there are no such zero cases in the underlying observations. Figure \@ref(fig:Figure27) illustrates our observations as points and the prediction function as shaded background colour.  

```{r Figure27, echo=FALSE, out.width='80%', fig.cap="Visualization of the observed data points (n=100) and the self-contructed prediction function. Dark blue background colour indicates a predicted potato size of zero which increases with the brightness of the yellow shaded background colour.", fig.align='center'}
knitr::include_graphics('images/VK_PDP_27_Prediction_Fct.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

In view of the observed data, one would expect to uncover a linear effect of both feature variables when looking at the corresponding PDPs. As we can see in figure \@ref(fig:Figure28), this is not necessarily the case. While the two-dimensional PDP perfectly depicts the prediction function, the individual PDPs for feature $x_1$ and $x_2$ fail in the areas where the prediction function does 'weird' stuff compared to what has been observed. 

```{r Figure28, echo=FALSE, out.width='100%', fig.cap="The first plot shows the two-dimensional PDP for features $x_1$ and $x_2$. The darker the background colour, the smaller the predicted values. The other plots are the PDPs derived for feature $x_1$ and $x_2$ resepctively. Up to a value of approximately 0.5 both partial dependence curves are mostly linear and bend at larger $x_1$- / $x_2$-values.", fig.align='left'}
knitr::include_graphics('images/VK_PDP_28_Prediction_Fct_Fail.png', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

```{r echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
library(mlr)
library(iml)
library(ggplot2)
library(viridis)
library(ggpubr)

set.seed(123)
n <- 100
X1 <- runif(n, min = 0, max = 1)
X2 <- X1 + rnorm(n, sd = 0.1)
df <- data.frame(X1, X2)
df$y = df$X1 + df$X2

pred.fun = function(model, newdata) {
  pred = predict(model, newdata)
  pred[newdata$X1 > 0.5 & newdata$X2 < (newdata$X1-0.5)] = 0
  pred[newdata$X2 > 0.5 & newdata$X1 < (newdata$X2-0.5)] = 0
  pred
}

mod <- lm(y ~ X1 + X2, data = df)

grid.dat = expand.grid(X1 = seq(from = 0, to = 1, length.out = 100), X2 = seq(from = 0, to = 1, length.out = 100))
grid.dat$predicted = pred.fun(mod, grid.dat)

ggplot(df) + 
  geom_tile(data = grid.dat, aes(x = X1, y = X2, fill = predicted)) + 
  geom_point(aes(x = X1, y = X2), size = 3) + 
  scale_fill_viridis(option = "E")+
  labs(title = "Visualization of Prediction Function and Features X1, X2")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0, size = 18))+
  xlim(c(0,1))+ylim(c(0,1))

pred <- Predictor$new(mod, data = df, predict.fun = pred.fun)

pdp <- FeatureEffect$new(pred, feature = c("X1","X2"), method = "pdp", grid.size = 50)
p1 <- plot(pdp)+
      ggtitle("2D-PDP for X1, X2")+
      theme_bw()+
      theme(plot.title = element_text(hjust = 0, size = 18))+
      xlim(c(0,1))+
      ylim(c(0,1))

pdp <- FeatureEffect$new(pred, feature = "X1", method = "pdp", grid.size = 50)
p2 <- plot(pdp)+
      ggtitle("PDP for X1")+
      theme_bw()+
      theme(plot.title = element_text(hjust = 0, size = 18))+
      xlim(c(0,1))+
      ylim(c(0,1.1))

pdp <- FeatureEffect$new(pred, feature = "X2", method = "pdp", grid.size = 50)
p3 <- plot(pdp)+
      ggtitle("PDP for X2")+
      theme_bw()+
      theme(plot.title = element_text(hjust = 0, size = 18))+
      xlim(c(0,1))+
      ylim(c(0,1.1))

ggarrange(p1, p2, p3, ncol=3)
```

## Summary
Our analysis of Partial Dependence Plots in the context of dependent features and missing values has revealed that both a violation of the underlying independence assumption and the presence of areas with no observations can have a significant impact on the marginalized feature effects. As a consequence, there is a risk of misinterpretation of the effect of features in $x_S$. However, we have also seen cases where the PDP (or the underlying machine learning algorithm) proved to be relatively robust against the dependence of features. Besides the independence assumtion, there are also other parameters playing a role for the accuracy of PDPs, like the grid size, the number of observations, the learning algorithm, variance in the data, complexity of the data generating process, etc.  
In practical applications it is recommended to analyse the variables used in the model, both by means of correlation and/or association measures and from a content-based perspective. Furthermore, data scientists can apply further methods, such as ALE plots, which will be discussed in chapter ??. 

