# Introduction (work in progress)

## Statistical Modeling: The Two Approaches (first draft, citations to be inserted)
* Data Modeling Approach
* Algorithmic Modeling Approach
* Historical Development
* Machine Learning
* Information Criterion

In statistics there are two approaches to reach conclusions from Data. First, the data modeling approach, where one assumes that the data are generated by a given stochastic data model. More specifically, a proposed model associates the input variables, random noise and parameters with the response variables. Typical models are for instance the linear and logistic regression model. These models allow to predict what the responses are going to be to future input variables and give information on how the response variables and input variables are associated, i.e. they are interpretable.  
Second, the algorithmic modeling approach that uses algorithmic models and treats the underlying data mechanism as unknown. More precisely, the goal is to find an algorithm that operates on the input variables to predict the response variables. Algorithms that are used are for instance random forests and neural nets. These algorithms allow to predict what the responses are going to be to future input variables, but do not give information on how the response variables and input variables are associated. Put differently, these algorithms produce black box models, because they do not provide any direct explanation for their predictions, i.e. they are not interpretable.

Within the statistics community the data modeling approach was prevalently dominant for a long time. However, especially in the last decade the increasing availability of enormous amounts of complex and unstructured data as well as the increase in processing power of computers served as a breeding ground for a strong shift to the algorithmic modeling approach, primarily for two reasons.  
First, the data modeling approach is not applicable to exciting problems like text, speech and image recognition. Second, for complex prediction problems new algorithms such as random forests and neural nets outperform classical models in prediction accuracy as they can model complex relationships in the data. 
For these reasons, more and more researchers switched from the data modeling approach to the algorithmic modeling approach that is much more common under the name machine learning.  

But what about the interpretability? As we learned in the first paragraph machine learning algorithms are black box models that do not provide any direct explanation for their predictions. Hence, the questions arises whether we need to know why an algorithm makes a certain prediction? To get a better feeling for this question it's helpful to understand how algorithms learn to make predictions and for which tasks machine learning is used.


## Importance of Interpretability (first draft, citations to be inserted)
* How do algorithms learn
* Use cases in academia
* Uses cases in industry
* Summary
 
Algorithms learn to make predictions from training data. Thus, algorithms also pick up biases of the training data and hence may not be robust under certain circumstances, e.g. they perform well on a test set, but not in the real-world. Such behavior can lead to undesired outcomes.  
For instance, consider a simple husky versus wolf classifier that misclassifies some huskies as wolves. Since the machine learning model does not give any information on how the response and input variables are associated, we do not know why it classified a husky as a wolf. However, interpretability might be useful to debug the algorithm and see if this problem is persistent or not. Using methods that make machine learning algorithms interpretable and that we will discuss later in the book, we would find that the misclassification was due to the snow on the image. The algorithm learned to use snow as a feature for classifying images as wolf. This might make sense in the training dataset, but not in the real-world. Thus, in this example interpretability helps us to understand how the algorithm gets to the result and hence, we know in which cases the robustness of the algorithm is not given.  
In the following we want to derive the importance of interpretability by focusing on academic and industrial settings.

Broadly speaking in academia machine learning is used to draw conclusions from data. However, off the shelf machine learning algorithms only give predictions without explanations. Thus, they answer only the "what", but not the "why" of a certain question and hence, do not allow for actual scientific findings. Especially, in areas such as life sciences and social sciences, that aim to identify causal relationships between input and response variables interpretability is key to scientific discoveries.  
For example, in a medical study researchers applying machine learning found that patients with pneumonia who have a history of asthma have a lower risk of dying from pneumonia than the general population. This is of course counterintuitive. However it was a true pattern in the training data: pneumonia patients with a history of asthma were usually admitted not only to the hospital but also directly to the Intensive Care Unit. The aggressive care received by asthmatic pneumonia patients was so effective that it lowered their risk of dying from pneumonia compared to the general population. However, since the prognosis for these patients was above average, models trained on this data erroneously find that asthma reduces the risk, while asthmatics actually have a much higher risk if they are not hospitalized. Hence, in this example blind trust in the machine learning algorithm would yield misleading results. 
Thus, interpretability is necessary in research to help identify causal relationships and increase the reliability and robustness of machine learning algorithms. Especially in areas outside of statistics the adoption of machine learning would be facilitated by making these models interpretable by adding explanations to their predictions.  

In industry, machine learning is a standard component of almost any digital product offered by the big tech companies. From Amazons Alexa, or Netflixs movie recommendation system to the search algorithm from Google and Facebooks feed. These companies use machine learning to improve their products and business models. However, their machine learning algorithms are also built on training data collected from their users.
Thus, in the age af data leaks à la Cambridge Analytica people want to understand for what purposes their data is collected and how the algorithms work that keep them on the streaming platforms or urge them to buy additional products and spend more time on social media.  
Thus, in the digital world interpretability of machine learning models would yield to a broader understanding of machine learning in the society and make the technology more trustworthy and fair.
Switching to the analog world we see a far slower adoption of machine learning sytsems at scale. This is due to the fact that decions made by machine learning systems in the real world can have far more severe consequences than in the digital world.  
For instance, if the wrong movie is suggested to us it does not really matter, but if a machine learning system that is deployed to a self driving car does not recognize a cyclist it might take the wrong decision with real lifes at stake. We need to be sure that the machine learning system is flawless, because driving over a cyclist is pretty bad. For example, an explanation might show that the most important feature is to recognize the two wheels of a bicycle, and this explanation helps you to think about certain edge cases, such as bicycles with side pockets that partially cover the wheels. Self-driving cars are just one example where machines are taking over decisions in the real world that were previously taken by humans and can involve severe and sometimes irreversible consequences. Interpretability helps to ensure the reliability and robusntess of these systems and thus makes them safer. 

To conclude, adding interpretability to machine learning algorithms is necessary in both academic and industrial applications. While we distinguished between academia and industry settings the general points, causality, robustness & reliability, trust and fairness are of course valid in both worlds.  
However, for academia interpretability is especially key to identify causal relationships and increase the reliability and robustness of scientific discoveries made by the help of machine learning algorithms.  
In industrial settings establishing trust in and fairness of machine learning systems matters most in low-risk environments whereas robustness and reliability is key to high-risk environments where machines take over decisions that have far reaching consequences.  

Now that we established the importance of interpretability, how do we put this into practice? A restriction to machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models has the drawback that better performing models are excluded a priori from model selection. Hence, do we trade of prediction versus information and go back to more simple models? - No!  
We seperate the explanations from the machine learning model and apply interpretable methods that analyze the model after the model is trained.


## Interpretable Machine Learning (first draft)
* Definition of post-hoc, model-agnostic interpretation methods
* Overview of methods
* Examples

As discussed in the previous chapter, most machine learning algorithms produce black box models, because they do not provide any direct explanation for their predictions. However, we do not want to restrict ourselves to models that are considered interpretable due to their simple structure and thus trade prediction accuracy for interpretability.  
Instead, we make machine learning models interpretable by applying methods that analyze the model after the model is trained, i.e. we establish post-hoc interpretability. Moreover, we are separating the explanations from the machine learning model, i.e we focus on so called model-agnostic interpretation methods.   
Post-hoc, model-agnostic explanation systems have several advantages \citep{ribeiro2016model}. First, since we seperate the underlying machien learning model and its interpretation developers can work with any model as the interpretation method is independent of the model. Thus we establish model flexibility. Second, since the interpretation is independent of the of the underlying machine learning model, the form of the interpretation also becomes independent. For instance in some cases it might be useful to have a linear formula, in other cases a graphic with feature importances is more appropriate. Thus, we establish explanation flexibility.  
To conclude, post-hoc and model-agnostic explanation systems establish model and explanation flexibility.

So what do these explanation systems do? - As discussed before interpretation methods for machine learning algorithms ensure causal relationships, robustness & reliability and establish trust and fairness. More specifically, they do so by shedding light on the following issues \citep{molnar2019}:

* Algorithm Transparency - How does the algorithm create the model? 
* Global, Holistic Model Interpretability - How does the trained model make predictions?
* Global Model Interpretability on a Modular Level - How do parts of the model affect predictions?
* Local Interpretability for a Single Prediction - Why did the model make a certain prediction for an instance?
* Local Interpretability for a Group of Predictions - Why did the model make specific predictions for a group of instances?

This booklet introduces and investigates the limitations of current post-hoc and model agnostic approaches in interpretable machine learning, such as partial dependence plots (PDP), Accumulated Local Effects (ALE), permutation feature importance, leave-one-covariate out (LOCO) and local interpretable model-agnostic explanations (LIME). All of these methods can be used to explain the behavior and predictions of trained machine learning models. 


## Outline of the booklet (work in progress)
* Methods (Model agnostic)
  * Partial Dependence Plots
  * Accumulative local effects
  * Feature importance
  * Lime
* Limitations 
  * if a model models interactions (e.g. when a random forest is used)
  * if features strongly correlate with each other
  * if the model does not correctly model causal relationships
  * if parameters of the interpretation method are not set correctly

While model-agnostic post-hoc interpretation methods can – in general – be used regardless of model complexity, their reliability and compactness deteriorates when models use a high number of features, have strong feature interactions and complex feature main effects. In the following the respective are introduced and possible limitations summarized.

This booklet investigates the limitations of current approaches (model agnostic and post-hoc) in interpretable machine learning, such as partial dependence plots (PDP, Accumulated Local Effects (ALE), permutation feature importance, leave-one-covariate out (LOCO) and local interpretable model-agnostic explanations (LIME). All of these methods can be used to explain the behavior and predictions of trained machine learning models. The interpretation methods might not work well in the following cases:


PDP: The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 200127). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. For example, when applied to a linear regression model, partial dependence plots always show a linear relationship.
If features of a machine learning model are correlated, the partial dependence plot cannot be trusted. The computation of a partial dependence plot for a feature that is strongly correlated with other features involves averaging predictions of artificial data instances that are unlikely in reality. This can greatly bias the estimated feature effect. 
The Partial Dependence Plot (PDP) is a rather intuitive and easy-to-understand visualization of the features' impact on the predicted outcome. If the assumptions for the PDP are met, it can show the way a feature impacts an outcome variable. More precisely, mapping the marginal effect of the selected variable(s) uncovers the linear, monotonic or nonlinear relationship between the predicted response and the individual feature variable(s).\citep{molnar2019}
Drawing a PDP with one or two feature variables allows a straight-forward interpretation of the marginal effects. This holds true as long as the features are not correlated. Should this independence assumption be violated, the partial dependence function will produce unrealistic data points. For instance, correlation between height and weight leading to a data point for someone taller than 2 meters weighing less than 50 kilos. Furthermore, opposite effects of heterogeneous subgroups might remain hidden through averaging the marginal effects, which could lead to wrong conclusions.\cite{molnar2019}


ICE 

As seen in section 2 PDPs don't work well as soon as two or more features are correlated. This gives rise to the definition of ALEs. Although their definition makes sense for high dimensional feature spaces including categorical features, within this section we only treat a space with two continous features. 
ALE: Accumulated local effects31 describe how features influence the prediction of a machine learning model on average. ALE plots are a faster and unbiased alternative to partial dependence plots (PDPs). I recommend reading the chapter on partial dependence plots first, as they are easier to understand and both methods share the same goal: Both describe how a feature affects the prediction on average. In the following section, I want to convince you that partial dependence plots have a serious problem when the features are correlated.

Feature Importance: The concept is really straightforward: We measure the importance of a feature by calculating the increase in the model’s prediction error after permuting the feature. A feature is “important” if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction. A feature is “unimportant” if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction. The permutation feature importance measurement was introduced by Breiman (2001)35 for random forests. Based on this idea, Fisher, Rudin, and Dominici (2018)36 proposed a model-agnostic version of the feature importance and called it model reliance. They also introduced more advanced ideas about feature importance, for example a (model-specific) version that takes into account that many prediction models may predict the data well. Their paper is worth reading.
As in previous chapters already discussed, there exist a variety of methods that enable a better understanding of the relationship between features and the outcome variables, especially for complex machine learning models. For instance, Partial Dependence (PD) plots visualize the feature effects on a global, aggregated level, whereas Individual Conditional Expectation (ICE) plots unravel the average feature effect by analyzing individual observations. The latter allows to detect, if existing, any heterogeneous relationship. Yet, these methods do not provide any insights to what extent a feature contributes to the predictive power of a model - in the following defined as Feature Importance. This perspective becomes interesting when recalling that black-box machine learning models aim for predictive accuracy rather than for inference. Hence, it is persuasive to also establish agnostic-methods that focus on the performance dimension. In the following, the two most common approaches, Permutation Feature Importance (PFI) by @breiman2001random and Leave-One-Covariate-Out (LOCO) by @lei2018distribution, for calculating and visualizing a Feature Importance metric, are introduced. At this point, it is worth to clarify that the concepts of feature effects and Feature Importance can by no means be ranked. Instead, they should be considered as mutual complements that enable interpretability from different angles. After introducing the concepts of PFI and LOCO, a brief discussion of their interpretability but also its non-negligible limitations will follow.

LIME: Local surrogate models are interpretable models that are used to explain individual predictions of black box machine learning models. Local interpretable model-agnostic explanations (LIME)37 is a paper in which the authors propose a concrete implementation of local surrogate models. Surrogate models are trained to approximate the predictions of the underlying black box model. Instead of training a global surrogate model, LIME focuses on training local surrogate models to explain individual predictions.
When doing machine learning we always build models.
Models are simplifications of reality.
Even if the predictive power of a model may be very strong, it will still only be a model.
However, models with high predictive capacity do most of the time not seem simple to a human as seen throughout this book.
In order to simplify a complex model we could use another model.
These simplifying models are referred to as surrogate models.
They imitate the black box prediction behaviour of a machine learning model subject to a specific and important constraint: 
surrogate models are interpretable.
For example, we may use a neural network to solve a classification task.
While a neural network is anything but interpretable, we may find that some of the decision boundaries are explained reasonably well by a logistic regression which in fact yields interpretable coefficients.

In general, there are two kinds of surrogate models: global and local surrogate models.
In this chapter, we will focus on the latter ones.

The concept of local surrogate models is heavily tied to @ribeiro2016should, who propose local interpretable model-agnostic explanations (LIME). 
Different from global surrogate models, local ones aim to rather explain single predictions by interpretable models than the whole black box model at once.
These surrogate models, also referred to as explainers, need to be easily interpretable (like linear regression or decision trees) and thus may of course not have the adaptability and flexibility of the original black box model which they aim to explain.
However, we actually don't care about a __global__ fit in this case.
We only want to have a very __local__ fit of the surrogate model in the proximity of the instance whose prediction is explained. 








The interpretation methods might not work well in the following cases:


In this booklet we want show different methods for interpretation and especially see if there are cases where those models break.

In more detail, we consider (xxxxx)...



Not Model-specific but model-agnostic?

Local or global?



