# Introduction

*Author: Emanuel Renkl*

*Supervisor: Christoph Molnar*

## Statistical Modeling: The Two Approaches
<!-- * Data Modeling Approach
* Algorithmic Modeling Approach
* Historical Development
* Machine Learning
* Information Criterion --> 

In statistics there are two approaches to reach conclusions from data (see @breiman2001). First, the data modeling approach, where one assumes that the data are generated by a given stochastic data model. More specifically, a proposed model associates the input variables, random noise and parameters with the response variables. Typical models are for instance the linear and logistic regression model. These models allow to predict what the responses are going to be to future input variables and give information on how the response variables and input variables are associated, i.e. they are interpretable.  Second, the algorithmic modeling approach that uses algorithmic models and treats the underlying data mechanism as unknown. More precisely, the goal is to find an algorithm that operates on the input variables to predict the response variables. Algorithms that are used are for instance random forests and neural nets. These algorithms allow to predict what the responses are going to be to future input variables, but do not give information on how the response variables and input variables are associated. Put differently, these algorithms produce black box models, because they do not provide any direct explanation for their predictions, i.e. they are not interpretable.

Within the statistics community the data modeling approach was prevalently dominant for a long time (@breiman2001). However, especially in the last decade the increasing availability of enormous amounts of complex and unstructured data as well as the increase in processing power of computers served as a breeding ground for a strong shift to the algorithmic modeling approach, primarily for two reasons.  
First, the data modeling approach is not applicable to exciting problems like text, speech and image recognition (@breiman2001). Second, for complex prediction problems new algorithms such as random forests and neural nets outperform classical models in prediction accuracy as they can model complex relationships in the data (@breiman2001). 
For these reasons, more and more researchers switched from the data modeling approach to the algorithmic modeling approach that is much more common under the name machine learning.  

But what about interpretability? As we learned in the first paragraph machine learning algorithms are black box models that do not provide any direct explanation for their predictions. Hence, the questions arises whether we need to know why an algorithm makes a certain prediction? To get a better feeling for this question it's helpful to understand how algorithms learn to make predictions and for which tasks machine learning is used.


## Importance of Interpretability
<!-- * How do algorithms learn
* Use cases in academia
* Uses cases in industry
* Summary --> 

Algorithms learn to make predictions from training data. Thus, algorithms also pick up biases of the training data and hence may not be robust under certain circumstances, e.g. they perform well on a test set, but not in the real-world. Such behavior can lead to undesired outcomes.  
For instance, consider a simple husky versus wolf classifier that misclassifies some huskies as wolves (see @ribeiro2016should). Since the machine learning model does not give any information on how the response and input variables are associated, we do not know why it classified a husky as a wolf. However, interpretability might be useful to debug the algorithm and see if this problem is persistent or not. Using methods that make machine learning algorithms interpretable and that we will discuss later in the book, we would find that the misclassification was due to the snow on the image. The algorithm learned to use snow as a feature for classifying images as wolf. This might make sense in the training dataset, but not in the real-world. Thus, in this example interpretability helps us to understand how the algorithm gets to the result and hence, we know in which cases the robustness of the algorithm is not given. In the following we want to derive the importance of interpretability by focusing on academic and industrial settings.

Broadly speaking in academia machine learning is used to draw conclusions from data. However, off the shelf machine learning algorithms only give predictions without explanations. Thus, they answer only the "what", but not the "why" of a certain question and hence, do not allow for actual scientific findings. Especially, in areas such as life sciences and social sciences, that aim to identify causal relationships between input and response variables interpretability is key to scientific discoveries.  
For example, in a medical study researchers applying machine learning found that patients with pneumonia who have a history of asthma have a lower risk of dying from pneumonia than the general population (@caruana2015intelligible). This is of course counterintuitive. However it was a true pattern in the training data: pneumonia patients with a history of asthma were usually admitted not only to the hospital but also directly to the Intensive Care Unit. The aggressive care received by asthmatic pneumonia patients was so effective that it lowered their risk of dying from pneumonia compared to the general population. However, since the prognosis for these patients was above average, models trained on this data erroneously find that asthma reduces the risk, while asthmatics actually have a much higher risk if they are not hospitalized. Hence, in this example blind trust in the machine learning algorithm would yield misleading results. 
Thus, interpretability is necessary in research to help identify causal relationships and increase the reliability and robustness of machine learning algorithms. Especially in areas outside of statistics the adoption of machine learning would be facilitated by making these models interpretable by adding explanations to their predictions.  

In industry, machine learning is a standard component of almost any digital product offered by the big tech companies. From Amazons Alexa, or Netflixs movie recommendation system to the search algorithm from Google and Facebooks feed. These companies use machine learning to improve their products and business models. However, their machine learning algorithms are also built on training data collected from their users.
Thus, in the age af data leaks à la Cambridge Analytica people want to understand for what purposes their data is collected and how the algorithms work that keep them on the streaming platforms or urge them to buy additional products and spend more time on social media.  
Thus, in the digital world interpretability of machine learning models would yield to a broader understanding of machine learning in the society and make the technology more trustworthy and fair.
Switching to the analog world we see a far slower adoption of machine learning sytsems at scale. This is due to the fact that decions made by machine learning systems in the real world can have far more severe consequences than in the digital world.  
For instance, if the wrong movie is suggested to us it does not really matter, but if a machine learning system that is deployed to a self driving car does not recognize a cyclist it might take the wrong decision with real lifes at stake (see @molnar2019). We need to be sure that the machine learning system is flawless, because driving over a cyclist is pretty bad. For example, an explanation might show that the most important feature is to recognize the two wheels of a bicycle, and this explanation helps you to think about certain edge cases, such as bicycles with side pockets that partially cover the wheels. Self-driving cars are just one example where machines are taking over decisions in the real world that were previously taken by humans and can involve severe and sometimes irreversible consequences. Interpretability helps to ensure the reliability and robusntess of these systems and thus makes them safer. 

To conclude, adding interpretability to machine learning algorithms is necessary in both academic and industrial applications. While we distinguished between academia and industry settings the general points, causality, robustness & reliability, trust and fairness are of course valid in both worlds.  
However, for academia interpretability is especially key to identify causal relationships and increase the reliability and robustness of scientific discoveries made by the help of machine learning algorithms.  
In industrial settings establishing trust in and fairness of machine learning systems matters most in low-risk environments whereas robustness and reliability is key to high-risk environments where machines take over decisions that have far reaching consequences.  

Now that we established the importance of interpretability, how do we put this into practice? A restriction to machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models has the drawback that better performing models are excluded a priori from model selection. Hence, do we trade of prediction versus information and go back to more simple models? - No!  
We seperate the explanations from the machine learning model and apply interpretable methods that analyze the model after the model is trained.


## Interpretable Machine Learning
<!-- * Definition of post-hoc, model-agnostic interpretation methods
* Overview of methods
* Examples --> 

As discussed in the previous chapter, most machine learning algorithms produce black box models, because they do not provide any direct explanation for their predictions. However, we do not want to restrict ourselves to models that are considered interpretable due to their simple structure and thus trade prediction accuracy for interpretability.
Instead, we make machine learning models interpretable by applying methods that analyze the model after the model is trained, i.e. we establish post-hoc interpretability. Moreover, we are separating the explanations from the machine learning model, i.e we focus on so called model-agnostic interpretation methods.   
Post-hoc, model-agnostic explanation systems have several advantages (@ribeiro2016model). First, since we seperate the underlying machine learning model and its interpretation developers can work with any model as the interpretation method is independent of the model. Thus we establish model flexibility. Second, since the interpretation is independent of the underlying machine learning model, the form of the interpretation also becomes independent. For instance in some cases it might be useful to have a linear formula, in other cases a graphic with feature importances is more appropriate. Thus, we establish explanation flexibility.

So what do these explanation systems do? - As discussed before, interpretation methods for machine learning algorithms ensure causal relationships, robustness & reliability and establish trust and fairness. More specifically, they do so by shedding light on the following issues (see @molnar2019):

* Algorithm transparency - How does the algorithm create the model? 
* Global, kolistic model interpretability - How does the trained model make predictions?
* Global model interpretability on a modular level - How do parts of the model affect predictions?
* Local interpretability for a single prediction - Why did the model make a certain prediction for an instance?
* Local interpretability for a group of predictions - Why did the model make specific predictions for a group of instances?


Now that we learned that post-hoc and model-agnostic methods ensure model as well as explanantion flexibility and in which ways explanation systems ensure causal relationships, robustness & reliability and establish trust and fairness, we can move on to the outline of the booklet and discuss specific interpretation methods and their limitations.


## Outline of the booklet
<!-- * Methods (Model agnostic)
  * Partial Dependence Plots (PDP)
  * Accumulated Local Effects (ALE)
  * Permutation Feature Importance (PMI)
  * Leave-One-Covariate Out (LOCO)
  * Local Interpretable Model-Agnostic Explanations (LIME)
* Limitations 
  * if a model models interactions (e.g. when a random forest is used)
  * if features strongly correlate with each other
  * if the model does not correctly model causal relationships
  * if parameters of the interpretation method are not set correctly --> 

This booklet introduces and investigates the limitations of current post-hoc and model agnostic approaches in interpretable machine learning, such as Partial Dependence Plots (PDP), Accumulated Local Effects (ALE), Permutation Feature Importance (PMI), Leave-One-Covariate Out (LOCO) and Local Interpretable Model-Agnostic Explanations (LIME). All of these methods can be used to explain the behavior and predictions of trained machine learning models. However, their reliability and compactness deteriorates when models use a high number of features, have strong feature interactions and complex feature main effects among others. In the following the methods mentioned are introduced and the outline of the booklet is given.

To start with, PDP and ALE are methods that enable a better understanding of the relationship between the outcome and feature variables of a machine learning model. 
Common to both methods is that they reduce the prediction function to a function that depends on only one or two features (@molnar2019). Both methods reduce the function by averaging the effects of the other features, but they differ in how the averages are calculated.  
PDP for example visualize whether the relationship between the outcome and a feature variable is for instance linear, monotonic or nonlinear and hence allow for a straightforward interpretation of the marginal effect of a certain feature on the predicted outcome of a model (@friedman2001greedy).
However, this holds only true as long as the feature in question is not correlated with any other features of the model. ALE plots are basically a faster and unbiased alternative to PDP, because they can interpret models containing correlated variables correctly (@molnar2019).
Chapter \@ref(pdp) of the booklet gives a short introduction to PDP. Next, PDP and its limitations when features are correlated is investigated in Chapter \@ref(pdp-correlated), respectively.
Chapter \@ref(pdp-causal) discusses if PDP allow for causal interpretations.
Chapter \@ref(ale) gives then a short introduction to ALE.
ALE and PDP are compared in detail in Chapter \@ref(ale-pdp).
The choice of intervals, problems with pice-wise constant models and categorial features in the context of ALE are investigated in Chapter \@ref(ale-misc).

Yet, PDP and ALE do not provide any insights to what extent a feature contributes to the predictive power of a model - in the following defined as feature importance.
PFI and LOCO are two methods that allow to compute and visualize the importance of a certain feature for a machine learning model.
PFI by @breiman2001random measures the importance of a feature by calculating the increase in the model’s prediction error after permuting the feature.
Leave-One-Covariate-Out (LOCO) by @lei2018distribution, requires to refit the model as the approach is based on leaving features out instead of permuting them (@casalicchio2018visualizing).
Chapter \@ref(pfi) gives a short introduction to PFI and LOCO and gives rise to its limitations.
Chapter \@ref(pfi-correlated) discusses both methods in the case of correlated features.
Then partial and individual PFI are discussed in Chapter \@ref(pfi-partial) and the issue whether feature importanceshould be computed on training or test data is discussed in Chapter \@ref(pfi-data). 

Finally, LIME is a method that explains individual predictions of a black box machine learning model by locally approximating the prediction using a less complex and interpretable model (@molnar2019).
These simplifying models are referred to as surrogate models. Consider for instance a neural network that is used for a classification task.
The neural network itself is of course not interpretable, but certain decision boundaries could for example be explained reasonably well by a logistic regression which in fact yields interpretable coefficients.
To refer to the first paragraph of the introduction, we use the data modeling approach to explain the algorithmic modeling approach in this example.
Chapter \@ref(lime) gives a short introduction to LIME.
Chapter \@ref(lime-neighbor) sheds light on the effect of the neighbourhood on LIME's explanantion for tabular data.
Chapter \@ref(lime-sample) deals with the sampling step in LIME and the resulting side effects in terms of feature weight stability of the surrogate model.

Now that we shortly introduced the different methods we can move to the respective chapters of the booklet which discuss the methods and their limitations in more detail and by the help of practical examples.
