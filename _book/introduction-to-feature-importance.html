<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Introduction to Feature Importance | Limitations of Interpretable Machine Learning Methods</title>
  <meta name="description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Introduction to Feature Importance | Limitations of Interpretable Machine Learning Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Introduction to Feature Importance | Limitations of Interpretable Machine Learning Methods" />
  
  <meta name="twitter:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  

<meta name="author" content="" />


<meta name="date" content="2019-08-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ale-intervals-piece-wise-constant-models-and-categorical-features.html">
<link rel="next" href="pfi-loco-and-correlated-features.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Limitations of ML Interpretability</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><i class="fa fa-check"></i><b>2</b> Introduction to Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#partial-dependence-plots-pdp"><i class="fa fa-check"></i><b>2.1</b> Partial Dependence Plots (PDP)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#advantages-and-limitations-of-partial-dependence-plots"><i class="fa fa-check"></i><b>2.1.1</b> Advantages and Limitations of Partial Dependence Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#individual-conditional-expectation-curves"><i class="fa fa-check"></i><b>2.2</b> Individual Conditional Expectation Curves</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#centered-ice-plot"><i class="fa fa-check"></i><b>2.2.1</b> Centered ICE Plot</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#derivative-ice-plot"><i class="fa fa-check"></i><b>2.2.2</b> Derivative ICE Plot</a></li>
<li class="chapter" data-level="2.2.3" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#advantages-and-limitations-of-ice-plots"><i class="fa fa-check"></i><b>2.2.3</b> Advantages and Limitations of ICE Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html"><i class="fa fa-check"></i><b>3</b> PDP and Correlated Features</a></li>
<li class="chapter" data-level="4" data-path="pdp-and-feature-interactions.html"><a href="pdp-and-feature-interactions.html"><i class="fa fa-check"></i><b>4</b> PDP and Feature Interactions</a></li>
<li class="chapter" data-level="5" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html"><i class="fa fa-check"></i><b>5</b> PDP and Causal Interpretation</a></li>
<li class="chapter" data-level="6" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html"><i class="fa fa-check"></i><b>6</b> Introduction to Accumulated Local Effects (ALE)</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#the-theoretical-formula"><i class="fa fa-check"></i><b>6.2</b> The Theoretical Formula</a><ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#centering"><i class="fa fa-check"></i><b>6.2.1</b> Centering</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#estimation-formula"><i class="fa fa-check"></i><b>6.3</b> Estimation Formula</a><ul>
<li class="chapter" data-level="6.3.1" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#implementation-formula"><i class="fa fa-check"></i><b>6.3.1</b> Implementation Formula</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#intuition-and-interpretation"><i class="fa fa-check"></i><b>6.4</b> Intuition and Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html"><i class="fa fa-check"></i><b>7</b> Comparison of ALE and PDP</a></li>
<li class="chapter" data-level="8" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><i class="fa fa-check"></i><b>8</b> ALE Intervals, Piece-Wise Constant Models and Categorical Features</a></li>
<li class="chapter" data-level="9" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html"><i class="fa fa-check"></i><b>9</b> Introduction to Feature Importance</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html#permutation-feature-importance-pfi"><i class="fa fa-check"></i><b>9.1</b> Permutation Feature Importance (PFI)</a></li>
<li class="chapter" data-level="9.2" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html#leave-one-covariate-out-loco"><i class="fa fa-check"></i><b>9.2</b> Leave-One-Covariate-Out (LOCO)</a></li>
<li class="chapter" data-level="9.3" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html#interpretability-of-feature-importance-and-its-limitations"><i class="fa fa-check"></i><b>9.3</b> Interpretability of Feature Importance and its Limitations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html"><i class="fa fa-check"></i><b>10</b> PFI, LOCO and Correlated Features</a></li>
<li class="chapter" data-level="11" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html"><i class="fa fa-check"></i><b>11</b> Partial and Individual Permutation Feature Importance</a></li>
<li class="chapter" data-level="12" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html"><i class="fa fa-check"></i><b>12</b> PFI: Training vs. Test Data</a></li>
<li class="chapter" data-level="13" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><i class="fa fa-check"></i><b>13</b> Introduction to Local Interpretable Model-Agnostic Explanations (LIME)</a></li>
<li class="chapter" data-level="14" data-path="lime-and-neighbourhood.html"><a href="lime-and-neighbourhood.html"><i class="fa fa-check"></i><b>14</b> LIME and Neighbourhood</a></li>
<li class="chapter" data-level="15" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html"><i class="fa fa-check"></i><b>15</b> LIME and Sampling</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Limitations of Interpretable Machine Learning Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-feature-importance" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Introduction to Feature Importance</h1>
<p>As in previous chapters already discussed, there exist a variety of methods that enable a better understanding of the relationship between features and the outcome variables, especially for complex machine learning models. For instance, Partial Dependence (PD) plots visualize the feature effects on a global, aggregated level, whereas Individual Conditional Expectation (ICE) plots unravel the average feature effect by analyzing individual observations. The latter allows to detect, if existing, any heterogeneous relationship. Yet, these methods do not provide any insights to what extent a feature contributes to the predictive power of a model - in the following defined as Feature Importance. This perspective becomes interesting when recalling that black-box machine learning models aim for predictive accuracy rather than for inference. Hence, it is persuasive to also establish agnostic-methods that focus on the performance dimension. In the following, the two most common approaches, Permutation Feature Importance (PFI) by <span class="citation">Breiman (<a href="#ref-breiman2001random" role="doc-biblioref">2001</a>)</span> and Leave-One-Covariate-Out (LOCO) by <span class="citation">Lei et al. (<a href="#ref-lei2018distribution" role="doc-biblioref">2018</a>)</span>, for calculating and visualizing a Feature Importance metric, are introduced. At this point, it is worth to clarify that the concepts of feature effects and Feature Importance can by no means be ranked. Instead, they should be considered as mutual complements that enable interpretability from different angles. After introducing the concepts of PFI and LOCO, a brief discussion of their interpretability but also its non-negligible limitations will follow.</p>
<div id="permutation-feature-importance-pfi" class="section level2">
<h2><span class="header-section-number">9.1</span> Permutation Feature Importance (PFI)</h2>
<p>The concept of Permutation Feature Importance was first introduced by <span class="citation">Breiman (<a href="#ref-breiman2001random" role="doc-biblioref">2001</a>)</span> and applied on a random forest model. The main principle is rather straightforward and easily implemented. The idea is as follows: When permuting the values of feature <span class="math inline">\(j\)</span>, its explanatory power mitigates, as it breaks the association to the outcome variable <span class="math inline">\(y\)</span>. Therefore, if the model relied on the feature <span class="math inline">\(j\)</span>, the prediction error <span class="math inline">\(e = L(y,f(X))\)</span> of the model <span class="math inline">\(f\)</span> should increase when predicting with the “permuted feature” dataset <span class="math inline">\(X_{perm}\)</span> instead of with the “initial feature” dataset <span class="math inline">\(X\)</span>. The importance of feature <span class="math inline">\(j\)</span> is then evaluated by the increase of the prediction error which can be either determined by taking the difference <span class="math inline">\(e_{perm} - e_{orig}\)</span> or taking the ratio <span class="math inline">\(e_{perm}/e_{orig}\)</span>. Note, taking the ratio can be favorable when comparing the result across different models. A feature is considered less important, if the increase in the prediction error was comparably small and the opposite if the increase was large. Thereby, it is important to note that when calculating the prediction error based on the permuted features there is no need to retrain the model <span class="math inline">\(f\)</span>. This property constitutes computational advantages, especially in case of complex models and large feature spaces. Below, a respective PFI algorithm based on <span class="citation">Fisher, Rudin, and Dominici (<a href="#ref-fisher2018model" role="doc-biblioref">2018</a>)</span> is outlined. Note however, that their original algorithm has a slightly different specification and was adjusted here for general purposes.</p>
<p><strong>The Permutation Feature Importance algorithm based on Fisher, Rudin, and Dominici (2018):</strong></p>
<p>Input: Trained model <span class="math inline">\(f\)</span>, feature matrix <span class="math inline">\(X\)</span>, target vector <span class="math inline">\(y\)</span>, error measure <span class="math inline">\(L(y,f(X))\)</span></p>
<ol style="list-style-type: decimal">
<li>Estimate the original model error <span class="math inline">\(e_{orig} = L(y,f(X))\)</span> (e.g. mean squared error)</li>
<li>For each feature <span class="math inline">\(j = 1,...,p\)</span> do:
<ul>
<li>Generate feature matrix <span class="math inline">\(X_{perm}\)</span> by permuting feature j in the data <span class="math inline">\(X\)</span></li>
<li>Estimate error <span class="math inline">\(e_{perm} = L(y,f(X_{perm}))\)</span> based on the predictions of the permuted data</li>
<li>Calculate permutation feature importance <span class="math inline">\(PFI_{j} = e_{perm}/e_{orig}\)</span>. Alternatively, the difference can be used: <span class="math inline">\(PFI_{j} = e_{perm} - e_{orig}\)</span></li>
</ul></li>
<li>Sort features by descending FI.</li>
</ol>
<p><br></p>
<p>In Figure <a href="introduction-to-feature-importance.html#fig:PFI">9.1</a> it is illustrated, by a fictional example, how the permutation algorithm alters the original dataset. For each of the <span class="math inline">\(p\)</span> features, the respectively permuted dataset is then used to first predict the outcomes and then calculate the prediction error.</p>
<br>
<div class="figure" style="text-align: center"><span id="fig:PFI"></span>
<img src="images/Permutation_All.jpeg" alt="Example for Permutation Feature Importance. The tables illustrate the second step of the algorithm of PFI, in particular the permutation of the features $x_{1}$ and $x_{p}$. As shown, the respective columns in dark grey are the ones which were shuffeled. This breaks the association between the feature of interest and the target value. Based on the formular underneath the tables, the PFI is calculated." width="65%" />
<p class="caption">
FIGURE 9.1: Example for Permutation Feature Importance. The tables illustrate the second step of the algorithm of PFI, in particular the permutation of the features <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{p}\)</span>. As shown, the respective columns in dark grey are the ones which were shuffeled. This breaks the association between the feature of interest and the target value. Based on the formular underneath the tables, the PFI is calculated.
</p>
</div>
<p><br></p>
<p>To show, how the PFI for all features of a model can be visualized and thereby more conveniently compared, the PFI algorithm with a random forest model is applied on the dataset “Boston” (see Figure <a href="introduction-to-feature-importance.html#fig:plot">9.2</a>), which is available in R via the <code>MASS</code> package. To predict the house price, seven variables are included, whereby as the results show, the PFI varies substantially across the variables. In this case, the features <code>Status of Population</code> and <code>Rooms</code> should be interpreted as the most important ones for the model, whereas <code>Blacks</code> is considered as less important.</p>
<p><br></p>
<center>
<div class="figure"><span id="fig:plot"></span>
<img src="book_files/figure-html/plot-1.svg" alt="Visualization of Permutation Feature Importance with a random forest applied on Boston dataset. The depicted points correspond to the median PFI over all shuffling iterations of one feature and the boundaries of the bands illustrate the 0.05- and 0.95-quantiles, respectively (see `iml` package).  " width="1152" />
<p class="caption">
FIGURE 9.2: Visualization of Permutation Feature Importance with a random forest applied on Boston dataset. The depicted points correspond to the median PFI over all shuffling iterations of one feature and the boundaries of the bands illustrate the 0.05- and 0.95-quantiles, respectively (see <code>iml</code> package).
</p>
</div>
</center>
</div>
<div id="leave-one-covariate-out-loco" class="section level2">
<h2><span class="header-section-number">9.2</span> Leave-One-Covariate-Out (LOCO)</h2>
<p>The concept of Leave-One-Covariate-Out (LOCO) follows the same objective as PFI, to gain insights on the importance of a specific feature for the prediction performance of a model. Although applications of LOCO exist, where comparable to PFI, the initial values of feature <span class="math inline">\(j\)</span> are replaced by its mean, median or zero <span class="citation">(see Hall, Phan, and Ambati <a href="#ref-hall2017ideas" role="doc-biblioref">2017</a>)</span>, and hence, circumvent the disadvantage of re-training the model <span class="math inline">\(f\)</span>, the common approach follows the idea to simply leave the respective feature out. The overall prediction error of the re-trained model <span class="math inline">\(f_{-j}\)</span> is then compared to the prediction error resulted from the baseline model. However, re-training the model results in higher computational costs, which becomes more severe with an increasing feature space. Typically, one is interested in assessing the Feature Importance within a fixed model <span class="math inline">\(f\)</span>. Applying LOCO might raise plausible concerns, as it compares the performance of a fixed model with the performance of a model <span class="math inline">\(f_{-j}\)</span> which is merely fitted with a subset of the data <span class="citation">(see Molnar <a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span>. The pseudo-code shown below, illustrates the algorithm for the common case where the feature is left out <span class="citation">(see Lei et al. <a href="#ref-lei2018distribution" role="doc-biblioref">2018</a>)</span>.</p>
<p><strong>The Leave-One-Covariate-Out algorithm based on Lei et al. (2018):</strong></p>
<p>Input: Trained model <span class="math inline">\(f\)</span>, feature matrix <span class="math inline">\(X\)</span>, target vector <span class="math inline">\(y\)</span>, error measure <span class="math inline">\(L(y,f(X))\)</span></p>
<ol style="list-style-type: decimal">
<li>Estimate the original model error <span class="math inline">\(e_{orig} = L(y,f(X))\)</span> (e.g. mean squared error)</li>
<li>For each feature <span class="math inline">\(j = 1,...,p\)</span> do:
<ul>
<li>Generate feature matrix <span class="math inline">\(X_{-j}\)</span> by removing feature j in the data <span class="math inline">\(X\)</span></li>
<li>Refit model <span class="math inline">\(f_{-j}\)</span> with data <span class="math inline">\(X_{-j}\)</span></li>
<li>Estimate error <span class="math inline">\(e_{-j} = L(y,f_{-j}(X_{-j}))\)</span> based on the predictions of the reduced data</li>
<li>Calculate LOCO Feature Importance <span class="math inline">\(FI_{j} = e_{-j}/e_{orig}\)</span>. Alternatively, the difference can be used: <span class="math inline">\(FI_{j} = e_{-j} - e_{orig}\)</span></li>
</ul></li>
<li>Sort features by descending FI.</li>
</ol>
<p><br></p>
<p>In Figure <a href="introduction-to-feature-importance.html#fig:LOCO">9.3</a> it is shown, how the LOCO algorithm alters the original dataset, whereby it always differs, depending on the respective feature that is left out. Note, that the qualitative and quantitative interpretations correspond to the ones from the PFI method. So do the visualization tools and therefore at this point it is refrained from providing the reader with an additional real data example.</p>
<p><br></p>
<div class="figure" style="text-align: center"><span id="fig:LOCO"></span>
<img src="images/LOCO_All.jpeg" alt="Example for Leave-One-Covariate-Out Feature Importance. The tables illustrate the second step of the algorithm of LOCO in particular the drop of $x_{1}$ and $x_{p}$. The dark grey columns of the original dataset mark the variables that will be dropped and therefore ignored when refitting the model. This breaks the relationship between the feature of interest and the target value. Based on the formular underneath the tables, the Feature Importance of LOCO is calculated." width="65%" />
<p class="caption">
FIGURE 9.3: Example for Leave-One-Covariate-Out Feature Importance. The tables illustrate the second step of the algorithm of LOCO in particular the drop of <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{p}\)</span>. The dark grey columns of the original dataset mark the variables that will be dropped and therefore ignored when refitting the model. This breaks the relationship between the feature of interest and the target value. Based on the formular underneath the tables, the Feature Importance of LOCO is calculated.
</p>
</div>
<p><br></p>
</div>
<div id="interpretability-of-feature-importance-and-its-limitations" class="section level2">
<h2><span class="header-section-number">9.3</span> Interpretability of Feature Importance and its Limitations</h2>
<p>After both methods are presented, it will be now questioned to what extent these agnostic-methods can contribute to a more comprehensive interpretability of machine learning models. Reflecting upon these limitations will consitute the main focus in the following chapters. Conveniently, both methods are highly adaptable on whether using classification or regression models, as they are non-rigid towards the prediction error metric (e.g. Accuracy, Precision, Recall, AUC, Average Log Loss, Mean Absolute Error, Mean Squared Error etc.). This allows to assess Feature Importance based on different performance measures. Besides, the interpretation can be conducted on a high-level, as both concepts do consider neither the shape of the relationship between the feature and outcome variable nor the direction of the feature effect. However, as illustrated in Figure <a href="introduction-to-feature-importance.html#fig:plot">9.2</a>, PFI and LOCO only return for each feature a single number and thereby neglect possible variations between subgroups in the data. Chapter XX will focus on how this limitation can be, at least for PFI, circumvented and introduces the concepts of Partial Importance (PI) and Individual Conditional Importance (ICI) which both avail themselves on the conceptual ideas of PD and ICE <span class="citation">(see Casalicchio, Molnar, and Bischl <a href="#ref-casalicchio2018visualizing" role="doc-biblioref">2018</a>)</span>. Besides, two general limitations appear when some features in the feature space are correlated. First, correlation makes an isolated analysis of the explanatory power of a feature complicated which results in an erroneous ranking in Feature Importance and hence, in incorrect conclusions. Second, if correlation exists and only in case of applying the PFI method, permuting a feature can result in unrealistic data instances so that the model performance is evaluated based on data which is never observed in reality. This makes comparisons of prediction errors complicated and therefore it should always be checked for this problem, if applying the PFI method. Chapter XX will focus on this limitations by comparing the performance of PFI and LOCO for different models and different levels of correlation in the data. Beyond these limitations, it is evident to also question whether these agnostic-methods should be computed on training or test data. As answering that depends highly on the research question and data, it is refrained from going into more detail at this point but will be examined and further discussed in chapter XX.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div id="ref-casalicchio2018visualizing">
<p>Casalicchio, Giuseppe, Christoph Molnar, and Bernd Bischl. 2018. “Visualizing the Feature Importance for Black Box Models.” In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 655–70. Springer.</p>
</div>
<div id="ref-fisher2018model">
<p>Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2018. “Model Class Reliance: Variable Importance Measures for Any Machine Learning Model Class, from the” Rashomon” Perspective.” <em>arXiv Preprint arXiv:1801.01489</em>.</p>
</div>
<div id="ref-hall2017ideas">
<p>Hall, Patrick, Wen Phan, and Sri Satish Ambati. 2017. “Ideas on Interpreting Machine Learning.” O’Reilly Ideas.</p>
</div>
<div id="ref-lei2018distribution">
<p>Lei, Jing, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. 2018. “Distribution-Free Predictive Inference for Regression.” <em>Journal of the American Statistical Association</em> 113 (523): 1094–1111.</p>
</div>
<div id="ref-molnar2019">
<p>Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pfi-loco-and-correlated-features.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/iml_methods_limitations/edit/master/03-00-feature-importance.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub", "book.mobi"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
